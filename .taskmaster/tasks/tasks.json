{
  "master": {
    "tasks": [
      {
        "id": 2,
        "title": "Implement Real Food Classification ML Model",
        "description": "Replace the mock food classifier in ml-service/app/services/food_analysis_service.py with a real CNN model (EfficientNet or ResNet) trained on food image datasets.",
        "details": "1. Create a new module `ml-service/app/ml_models/food_classifier.py` with:\n   - Load pre-trained EfficientNet-B0 or ResNet-50 from torchvision\n   - Fine-tune on Food-101 dataset or custom food dataset\n   - Implement proper image preprocessing pipeline matching ImageNet stats\n   - Support for GPU inference if available (auto-detect CUDA)\n\n2. Update `food_analysis_service.py`:\n   - Replace `NUTRITION_DATABASE` with a proper food database (USDA FoodData Central API integration)\n   - Update `_classify_food()` to use real model inference instead of random selection\n   - Add model loading with caching to avoid reloading on each request\n   - Implement top-5 predictions with confidence scores\n\n3. Add model versioning:\n   - Store model checkpoints in `ml-service/models/food_classifier/`\n   - Add `model_version` field to responses\n   - Implement A/B testing capability by loading multiple model versions\n\n4. Extend nutrition database:\n   - Expand from current 6 items to 100+ common foods\n   - Structure: JSON file or SQLite database with USDA data\n   - Include serving size variations (small, medium, large)\n\nPseudo-code for classifier:\n```python\nclass FoodClassifier:\n    def __init__(self, model_path: str = None):\n        self.model = self._load_model(model_path)\n        self.classes = self._load_class_labels()\n    \n    def _load_model(self, path):\n        model = torchvision.models.efficientnet_b0(pretrained=True)\n        model.classifier[-1] = nn.Linear(1280, num_food_classes)\n        if path:\n            model.load_state_dict(torch.load(path))\n        model.eval()\n        return model\n    \n    async def classify(self, image: np.ndarray) -> List[Tuple[str, float]]:\n        tensor = self._preprocess(image)\n        with torch.no_grad():\n            outputs = self.model(tensor)\n            probs = torch.softmax(outputs, dim=1)\n        return self._get_top_k(probs, k=5)\n```",
        "testStrategy": "1. Unit tests for model loading and inference\n2. Test classification accuracy on held-out test set (target >80% top-5)\n3. Integration test: POST /api/food/analyze with real food images\n4. Performance test: Inference time <3s per image\n5. Test model fallback when GPU not available",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Build Health Metrics Mobile UI Screens",
        "description": "Create mobile screens for viewing and manually entering health metrics (RHR, HRV, sleep, recovery). Backend API is complete at /api/health-metrics.",
        "details": "1. Create new screens in `app/` directory:\n   - `app/health/index.tsx` - Health metrics dashboard/list\n   - `app/health/[id].tsx` - Detail view for specific metric\n   - `app/health/add.tsx` - Manual entry form\n\n2. Health Dashboard (`app/health/index.tsx`):\n   - Display today's key metrics in cards (RHR, HRV, Sleep, Recovery)\n   - Time range selector: Today, Week, Month\n   - Pull-to-refresh functionality\n   - Navigate to detail view on tap\n\n3. Metric Detail View (`app/health/[id].tsx`):\n   - Line chart showing metric over time (use react-native-chart-kit or Victory Native)\n   - Statistics: avg, min, max, trend arrow\n   - Data source indicator (Apple Health, Fitbit, Manual)\n   - Date range filter\n\n4. Manual Entry Form (`app/health/add.tsx`):\n   - Metric type picker (dropdown with all HealthMetricType enum values)\n   - Value input with unit display (bpm, ms, hours, %)\n   - Date/time picker (defaults to now)\n   - Source set to 'MANUAL'\n   - Validation: min/max ranges per metric type\n\n5. Create API client in `lib/api/health-metrics.ts`:\n```typescript\nexport const healthMetricsApi = {\n  getAll: (params: { startDate?: string; endDate?: string; metricType?: string }) => \n    apiClient.get('/health-metrics', { params }),\n  getById: (id: string) => apiClient.get(`/health-metrics/${id}`),\n  create: (data: CreateHealthMetricInput) => apiClient.post('/health-metrics', data),\n  getDailySummary: (date: string) => apiClient.get(`/health-metrics/daily/${date}`),\n}\n```\n\n6. Add navigation:\n   - Add 'Health' tab to bottom navigation in `app/(tabs)/_layout.tsx`\n   - Use health heart icon from @expo/vector-icons\n<info added on 2025-12-05T01:32:13.931Z>\nNow I have a comprehensive understanding of the codebase's styling patterns, typography, colors, and testing conventions. Let me provide the update text:\n\n7. Styling and UX Consistency Requirements:\n\nAll Health Metrics screens must follow the established design system in `lib/theme/colors.ts`:\n- Use `colors.background.primary` (#0F1419) as main background\n- Use `colors.background.tertiary` (#1E2330) for cards and surfaces\n- Use `colors.primary.main` (#8B5CF6) for interactive elements\n- Apply `gradients.primary` (purple-pink) for CTAs and active states\n- Text: `colors.text.primary` for headings, `colors.text.tertiary` for labels\n- Use `spacing` constants (xs:4, sm:8, md:16, lg:24, xl:32)\n- Apply `borderRadius` constants (sm:8, md:12, lg:16)\n- Use `typography.fontSize` and `typography.fontWeight` for consistent text styling\n- Import theme tokens: `import { colors, gradients, shadows, spacing, borderRadius, typography } from '@/lib/theme/colors'`\n\nMatch existing UX patterns from `app/(tabs)/index.tsx` and `app/add-meal.tsx`:\n- Cards with `borderWidth: 1, borderColor: colors.border.secondary`\n- Section titles: `fontSize: typography.fontSize['2xl']` or `lg`, `fontWeight: bold/semibold`\n- Form inputs: Height 48px, `backgroundColor: colors.background.tertiary`, `borderRadius: borderRadius.md`\n- Pull-to-refresh using `RefreshControl` with `tintColor={colors.primary.main}`\n- Loading states with `ActivityIndicator` using `colors.primary.main`\n- FAB pattern: 56x56 with LinearGradient and `shadows.xl`\n- SafeAreaView container with ScrollView using `showsVerticalScrollIndicator={false}`\n\n8. Comprehensive Test Requirements:\n\nCreate mobile component tests in `__tests__/` directory using react-native-testing-library:\n\nTest file structure:\n- `__tests__/screens/health/HealthDashboard.test.tsx`\n- `__tests__/screens/health/HealthMetricDetail.test.tsx`\n- `__tests__/screens/health/AddHealthMetric.test.tsx`\n- `__tests__/api/health-metrics.test.ts`\n\nRequired test coverage per screen:\n\nHealthDashboard tests:\n- Renders loading state with ActivityIndicator\n- Renders metric cards for RHR, HRV, Sleep, Recovery when data exists\n- Renders empty state when no health data available\n- Time range selector changes displayed data (Today/Week/Month)\n- Pull-to-refresh triggers API reload\n- Tapping metric card navigates to detail view\n- Error state rendering when API fails\n\nHealthMetricDetail tests:\n- Renders line chart with historical data\n- Displays statistics (avg, min, max, trend)\n- Shows correct data source indicator (Apple Health, Fitbit, Manual)\n- Date range filter updates chart data\n- Handles empty data gracefully\n- Loading and error states\n\nAddHealthMetric (Manual Entry) tests:\n- Metric type picker renders all HealthMetricType enum values\n- Value input accepts numeric input with correct units per type\n- Date/time picker defaults to current time\n- Source automatically set to MANUAL\n- Validation errors for out-of-range values (per metric type)\n- Required field validation\n- Successful submission calls API and navigates back\n- Cancel button discards changes and navigates back\n- Form disabled during submission\n\nAPI client tests (`lib/api/health-metrics.ts`):\n- Follow existing patterns from `__tests__/unit/api/food-analysis.test.ts`\n- Mock axios using `jest.mock('axios')`\n- Test getAll with various query params (startDate, endDate, metricType)\n- Test getById returns single metric\n- Test create sends correct payload and returns created metric\n- Test getDailySummary formats date correctly\n- Test error handling with proper error messages\n\nUse test patterns from existing tests:\n- Arrange, Act, Assert pattern\n- Mock dependencies with `jest.mock()`\n- Use `waitFor` for async assertions\n- Test user interactions with `fireEvent`\n</info added on 2025-12-05T01:32:13.931Z>",
        "testStrategy": "1. Component tests for each screen using react-native-testing-library\n2. Test form validation for manual entry\n3. Test API integration with mock server\n4. Visual regression tests for chart rendering\n5. Test pull-to-refresh behavior\n6. Test empty state when no health data exists",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create TypeScript types for health metrics in lib/types/health-metrics.ts",
            "description": "Define TypeScript interfaces and types for health metrics to be used across the mobile app, matching the backend API contracts.",
            "dependencies": [],
            "details": "Create `lib/types/health-metrics.ts` with the following types:\n\n1. **HealthMetricType enum** - Match the 28 types from `server/src/validation/schemas.ts` healthMetricTypeSchema: RESTING_HEART_RATE, HEART_RATE_VARIABILITY_SDNN, HEART_RATE_VARIABILITY_RMSSD, BLOOD_PRESSURE_SYSTOLIC, BLOOD_PRESSURE_DIASTOLIC, RESPIRATORY_RATE, OXYGEN_SATURATION, VO2_MAX, SLEEP_DURATION, DEEP_SLEEP_DURATION, REM_SLEEP_DURATION, SLEEP_EFFICIENCY, SLEEP_SCORE, STEPS, ACTIVE_CALORIES, TOTAL_CALORIES, EXERCISE_MINUTES, STANDING_HOURS, RECOVERY_SCORE, STRAIN_SCORE, READINESS_SCORE, BODY_FAT_PERCENTAGE, MUSCLE_MASS, BONE_MASS, WATER_PERCENTAGE, SKIN_TEMPERATURE, BLOOD_GLUCOSE, STRESS_LEVEL\n\n2. **HealthMetricSource type** - Match `server/src/validation/schemas.ts` healthMetricSourceSchema: 'apple_health' | 'fitbit' | 'garmin' | 'oura' | 'whoop' | 'manual'\n\n3. **HealthMetric interface** - Based on backend response: id, userId, metricType, value, unit, recordedAt, source, sourceId?, metadata?, createdAt, updatedAt\n\n4. **CreateHealthMetricInput interface** - Match `server/src/validation/schemas.ts` createHealthMetricSchema: metricType, value, unit, recordedAt (ISO string), source, sourceId?, metadata?\n\n5. **HealthMetricStats interface** - For stats endpoint: average, min, max, count, trend ('up' | 'down' | 'stable'), percentChange\n\n6. **TimeSeriesDataPoint interface** - For charts: date (string), value, source?\n\n7. **METRIC_CONFIG constant** - Unit and display info per metric type: { unit: string, displayName: string, minValue?: number, maxValue?: number, icon?: string }. Example: RESTING_HEART_RATE: { unit: 'bpm', displayName: 'Resting Heart Rate', minValue: 30, maxValue: 220 }",
            "status": "done",
            "testStrategy": null,
            "updatedAt": "2025-12-05T01:39:21.568Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create health metrics API client in lib/api/health-metrics.ts",
            "description": "Implement API client functions to communicate with the backend health metrics endpoints, following the existing mealsApi pattern in lib/api/meals.ts.",
            "dependencies": [
              1
            ],
            "details": "Create `lib/api/health-metrics.ts` following the pattern from `lib/api/meals.ts`:\n\n```typescript\nimport api from './client';\nimport { HealthMetric, CreateHealthMetricInput, HealthMetricStats, TimeSeriesDataPoint, HealthMetricType, HealthMetricSource } from '../types/health-metrics';\n\nexport interface GetHealthMetricsParams {\n  metricType?: HealthMetricType;\n  startDate?: string;\n  endDate?: string;\n  source?: HealthMetricSource;\n  limit?: number;\n}\n\nexport const healthMetricsApi = {\n  // POST /health-metrics - Create single metric\n  async create(data: CreateHealthMetricInput): Promise<HealthMetric>,\n\n  // GET /health-metrics - Get all with optional filters\n  async getAll(params?: GetHealthMetricsParams): Promise<HealthMetric[]>,\n\n  // GET /health-metrics/:id - Get by ID\n  async getById(id: string): Promise<HealthMetric>,\n\n  // GET /health-metrics/latest/:metricType - Get latest value for a metric type\n  async getLatest(metricType: HealthMetricType): Promise<HealthMetric | null>,\n\n  // GET /health-metrics/timeseries/:metricType - Get time series data for charts\n  async getTimeSeries(metricType: HealthMetricType, startDate?: string, endDate?: string): Promise<TimeSeriesDataPoint[]>,\n\n  // GET /health-metrics/stats/:metricType - Get statistics (avg, min, max, trend)\n  async getStats(metricType: HealthMetricType, days?: number): Promise<HealthMetricStats>,\n\n  // GET /health-metrics/average/daily/:metricType - Get daily average\n  async getDailyAverage(metricType: HealthMetricType, date?: string): Promise<{ average: number; count: number }>,\n\n  // GET /health-metrics/average/weekly/:metricType - Get weekly average\n  async getWeeklyAverage(metricType: HealthMetricType): Promise<{ average: number; count: number }>,\n\n  // DELETE /health-metrics/:id - Delete metric\n  async delete(id: string): Promise<void>,\n};\n```\n\nUse the existing `api` client from `./client` which handles JWT auth token injection. Match the routes from `server/src/routes/healthMetricRoutes.ts`.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-05T01:39:52.642Z"
          },
          {
            "id": 3,
            "title": "Add Health tab to bottom navigation in app/(tabs)/_layout.tsx",
            "description": "Add a new 'Health' tab to the existing tab navigation using a heart icon, maintaining consistency with the current tab styling.",
            "dependencies": [],
            "details": "Modify `app/(tabs)/_layout.tsx` to add the Health tab:\n\n1. Add the 'heart.fill' SF Symbol to the MAPPING in `components/ui/IconSymbol.tsx`:\n```typescript\n'heart.fill': 'favorite',  // MaterialIcons mapping\n'person.fill': 'person',   // Already exists\n```\n\n2. Add new Tabs.Screen in `app/(tabs)/_layout.tsx` after the index tab and before profile:\n```typescript\n<Tabs.Screen\n  name=\"health\"\n  options={{\n    title: 'Health',\n    tabBarIcon: ({ color }) => <IconSymbol size={28} name=\"heart.fill\" color={color} />,\n  }}\n/>\n```\n\n3. Ensure the tab follows existing styling from tabBarStyle with:\n- `tabBarActiveTintColor: colors.primary.main` (purple #8B5CF6)\n- `tabBarInactiveTintColor: colors.text.disabled` (gray #6B7280)\n- Same height and padding as other tabs\n\n4. Export IconSymbolName type must include 'heart.fill' for TypeScript safety.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-05T01:40:18.178Z"
          },
          {
            "id": 4,
            "title": "Create Health Dashboard screen at app/(tabs)/health.tsx",
            "description": "Build the main Health Dashboard screen displaying today's key metrics (RHR, HRV, Sleep, Recovery) in cards with time range selector and pull-to-refresh.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Create `app/(tabs)/health.tsx` following patterns from `app/(tabs)/index.tsx`:\n\n**Imports:**\n- React hooks: useState, useEffect, useCallback\n- Components: View, Text, StyleSheet, ScrollView, TouchableOpacity, RefreshControl, ActivityIndicator\n- expo-router: useRouter\n- SafeAreaView from react-native-safe-area-context\n- LinearGradient from expo-linear-gradient\n- Theme: colors, gradients, shadows, spacing, borderRadius, typography from '@/lib/theme/colors'\n- API: healthMetricsApi from '@/lib/api/health-metrics'\n- Types: HealthMetric, HealthMetricStats, METRIC_CONFIG from '@/lib/types/health-metrics'\n\n**State:**\n- metrics: Record<HealthMetricType, { latest: HealthMetric | null; stats: HealthMetricStats | null }>\n- timeRange: 'today' | 'week' | 'month' (default 'today')\n- isLoading: boolean, refreshing: boolean\n\n**Layout Structure:**\n1. Header with title \"Health\" and date (matches index.tsx greeting style: fontSize: typography.fontSize['3xl'], fontWeight: bold)\n2. Time Range Selector (horizontal buttons like meal type selector in add-meal.tsx)\n3. Metric Cards Grid (2x2) for: RESTING_HEART_RATE, HEART_RATE_VARIABILITY_SDNN, SLEEP_DURATION, RECOVERY_SCORE\n\n**Each Metric Card (TouchableOpacity):**\n- backgroundColor: colors.background.tertiary\n- borderWidth: 1, borderColor: colors.border.secondary\n- borderRadius: borderRadius.lg\n- padding: spacing.md\n- Icon (use Ionicons: heart-outline, pulse-outline, moon-outline, fitness-outline)\n- Label (colors.text.tertiary, fontSize: typography.fontSize.sm)\n- Value (colors.text.primary, fontSize: typography.fontSize['2xl'], fontWeight: bold)\n- Unit (colors.text.tertiary)\n- Trend indicator arrow (green up, red down, gray stable)\n- onPress: router.push(`/health/${metricType}`)\n\n**Pull-to-refresh:** Use RefreshControl with tintColor={colors.primary.main}\n\n**Loading State:** ActivityIndicator centered with colors.primary.main\n\n**Empty State:** \"No health data yet. Add your first metric!\" with button to /health/add",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-05T01:41:25.915Z"
          },
          {
            "id": 5,
            "title": "Create Metric Detail screen at app/health/[metricType].tsx",
            "description": "Build the detail view for a specific health metric showing historical line chart, statistics (avg, min, max, trend), date range filter, and data source indicator.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create `app/health/[metricType].tsx` as a dynamic route screen:\n\n**Imports:**\n- useLocalSearchParams from expo-router to get metricType param\n- LineChart from react-native-chart-kit (install if needed) or VictoryLine from victory-native\n- All theme imports from lib/theme/colors\n- healthMetricsApi and types\n\n**State:**\n- timeSeries: TimeSeriesDataPoint[] for chart data\n- stats: HealthMetricStats | null\n- dateRange: '7d' | '30d' | '90d' (default '30d')\n- isLoading: boolean\n\n**Layout:**\n1. **Header** with back button (TouchableOpacity with Ionicons chevron-back) and metric display name from METRIC_CONFIG\n\n2. **Date Range Selector** - Horizontal buttons matching add-meal.tsx mealTypeContainer style:\n   - 7 Days, 30 Days, 90 Days\n   - Active: LinearGradient with gradients.primary\n   - Inactive: backgroundColor: colors.background.tertiary\n\n3. **Line Chart** (full width, height ~200):\n   - backgroundColor: colors.background.tertiary\n   - Line color: colors.primary.main (#8B5CF6)\n   - Grid lines: colors.border.secondary\n   - Labels: colors.text.tertiary\n   - Data points from timeSeries API response\n\n4. **Statistics Card** (similar to macrosContainer in index.tsx):\n   - Three columns: Average, Minimum, Maximum\n   - Each shows value with unit\n   - fontSize: typography.fontSize.xl for values\n   - backgroundColor: colors.background.tertiary\n   - borderRadius: borderRadius.md\n\n5. **Trend Section:**\n   - Arrow icon (trending-up/down/minus from Ionicons)\n   - Percentage change text\n   - Color: status.success (green) for up, status.error (red) for down\n\n6. **Data Source Badge:**\n   - Icon per source (Apple Health, Fitbit, Manual, etc.)\n   - Text showing source name\n   - colors.text.tertiary styling\n\n**Chart Config (react-native-chart-kit):**\n```typescript\nchartConfig: {\n  backgroundColor: colors.background.tertiary,\n  backgroundGradientFrom: colors.background.tertiary,\n  backgroundGradientTo: colors.background.tertiary,\n  color: (opacity = 1) => `rgba(139, 92, 246, ${opacity})`, // primary.main\n  labelColor: (opacity = 1) => `rgba(156, 163, 175, ${opacity})`, // text.tertiary\n  strokeWidth: 2,\n}\n```",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-05T01:43:39.003Z"
          },
          {
            "id": 6,
            "title": "Create Manual Entry form at app/health/add.tsx",
            "description": "Build the form for manually entering health metrics with metric type picker, value input with dynamic units, date/time picker, and validation for min/max ranges per metric type.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create `app/health/add.tsx` following the pattern from `app/add-meal.tsx`:\n\n**Imports:**\n- useState, useEffect from react\n- All RN components: View, Text, TextInput, TouchableOpacity, StyleSheet, ScrollView, KeyboardAvoidingView, Platform, ActivityIndicator\n- useRouter from expo-router\n- SafeAreaView, LinearGradient, Ionicons\n- DateTimePicker from @react-native-community/datetimepicker (or expo-date-time-picker)\n- Picker from @react-native-picker/picker\n- healthMetricsApi, CreateHealthMetricInput, HealthMetricType, METRIC_CONFIG\n- colors, gradients, spacing, borderRadius, typography from theme\n- showAlert from '@/lib/utils/alert'\n- getErrorMessage from '@/lib/utils/errorHandling'\n\n**State:**\n- metricType: HealthMetricType (default 'RESTING_HEART_RATE')\n- value: string (for TextInput)\n- recordedAt: Date (default new Date())\n- isLoading: boolean\n- showDatePicker: boolean\n- errors: { value?: string }\n\n**Layout (match add-meal.tsx structure):**\n1. **Header** - Same as add-meal: Cancel (left), \"Add Health Metric\" (center), Save (right)\n   - Cancel: text style, color: colors.text.secondary\n   - Save: text style, color: colors.primary.main, disabled when isLoading\n\n2. **Metric Type Picker Section:**\n   - Label: \"Metric Type\" (styles.sectionTitle)\n   - Dropdown picker with all 28 HealthMetricType values\n   - Group by category: Cardiovascular, Sleep, Activity, Recovery, Body Composition\n   - Display friendly names from METRIC_CONFIG.displayName\n\n3. **Value Input Section:**\n   - Label: \"Value ({unit})\" - dynamically show unit from METRIC_CONFIG[metricType].unit\n   - TextInput with keyboardType=\"decimal-pad\"\n   - Height 48px, backgroundColor: colors.background.tertiary\n   - Validation message below if out of range (colors.status.error)\n\n4. **Date/Time Picker Section:**\n   - Label: \"Recorded At\"\n   - TouchableOpacity showing formatted date/time\n   - Opens DateTimePicker modal\n   - Default to current time\n\n5. **Source Badge** (non-editable):\n   - Shows \"Manual\" with checkmark icon\n   - Subtle styling: colors.special.highlight background\n\n**Validation:**\n- Value required and numeric\n- Check against METRIC_CONFIG[metricType].minValue and maxValue\n- Show inline error: \"Value must be between {min} and {max} {unit}\"\n\n**handleSave:**\n```typescript\nconst data: CreateHealthMetricInput = {\n  metricType,\n  value: parseFloat(value),\n  unit: METRIC_CONFIG[metricType].unit,\n  recordedAt: recordedAt.toISOString(),\n  source: 'manual',\n};\nawait healthMetricsApi.create(data);\nshowAlert('Success', 'Health metric added!', [{ text: 'OK', onPress: () => router.back() }]);\n```",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-05T01:45:06.446Z"
          },
          {
            "id": 7,
            "title": "Write unit tests for health-metrics API client",
            "description": "Create comprehensive unit tests for lib/api/health-metrics.ts following the existing test patterns from __tests__/unit/api/food-analysis.test.ts.",
            "dependencies": [
              2
            ],
            "details": "Create `__tests__/unit/api/health-metrics.test.ts`:\n\n**Setup:**\n```typescript\nimport axios from 'axios';\nimport { healthMetricsApi } from '@/lib/api/health-metrics';\nimport { HealthMetric, HealthMetricType, HealthMetricStats, TimeSeriesDataPoint } from '@/lib/types/health-metrics';\n\njest.mock('axios');\nconst mockedAxios = axios as jest.Mocked<typeof axios>;\n```\n\n**Test Fixtures:**\n```typescript\nconst mockHealthMetric: HealthMetric = {\n  id: 'metric-1',\n  userId: 'user-1',\n  metricType: 'RESTING_HEART_RATE',\n  value: 62,\n  unit: 'bpm',\n  recordedAt: '2024-01-15T08:00:00Z',\n  source: 'manual',\n  createdAt: '2024-01-15T08:00:00Z',\n  updatedAt: '2024-01-15T08:00:00Z',\n};\n\nconst mockStats: HealthMetricStats = {\n  average: 65,\n  min: 58,\n  max: 72,\n  count: 30,\n  trend: 'down',\n  percentChange: -3.5,\n};\n```\n\n**Test Cases:**\n\n1. **create():**\n   - Should successfully create a health metric\n   - Should send correct payload format to POST /health-metrics\n   - Should handle validation errors (400)\n   - Should handle auth errors (401)\n\n2. **getAll():**\n   - Should return array of metrics\n   - Should pass query params (metricType, startDate, endDate, source, limit)\n   - Should handle empty results\n   - Should handle network errors\n\n3. **getById():**\n   - Should return single metric by ID\n   - Should handle 404 not found\n\n4. **getLatest():**\n   - Should return latest metric for type\n   - Should return null when no metrics exist\n\n5. **getTimeSeries():**\n   - Should return array of data points for chart\n   - Should pass date range params\n   - Should handle empty data gracefully\n\n6. **getStats():**\n   - Should return stats with average, min, max, trend\n   - Should pass days param\n   - Should handle 404 when no data\n\n7. **getDailyAverage() and getWeeklyAverage():**\n   - Should return average and count\n   - Should handle date param for daily\n\n8. **delete():**\n   - Should successfully delete metric\n   - Should handle 404 errors\n\n**Pattern:** Use Arrange, Act, Assert. Mock `api.get`, `api.post`, `api.delete` from the client module.",
            "status": "done",
            "testStrategy": "Run with `npm test __tests__/unit/api/health-metrics.test.ts`. Verify all API methods are tested with success and error cases. Check coverage with `npm run test:coverage`.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T01:46:30.103Z"
          },
          {
            "id": 8,
            "title": "Write component tests for Health screens",
            "description": "Create comprehensive component tests for HealthDashboard, HealthMetricDetail, and AddHealthMetric screens using react-native-testing-library.",
            "dependencies": [
              4,
              5,
              6
            ],
            "details": "Create test files in `__tests__/screens/health/` directory:\n\n**1. __tests__/screens/health/HealthDashboard.test.tsx:**\n```typescript\nimport React from 'react';\nimport { render, fireEvent, waitFor } from '@testing-library/react-native';\nimport HealthDashboard from '@/app/(tabs)/health';\nimport { healthMetricsApi } from '@/lib/api/health-metrics';\n\njest.mock('@/lib/api/health-metrics');\njest.mock('expo-router', () => ({ useRouter: () => ({ push: jest.fn() }) }));\njest.mock('@/lib/context/AuthContext', () => ({ useAuth: () => ({ user: { id: '1', name: 'Test' } }) }));\n```\n\nTest cases:\n- Renders loading state with ActivityIndicator initially\n- Renders metric cards when data loads successfully\n- Renders empty state when no health data available\n- Time range selector updates displayed data (Today/Week/Month)\n- Pull-to-refresh triggers API reload (test RefreshControl onRefresh)\n- Tapping metric card calls router.push with correct metric type\n- Error state renders when API fails\n\n**2. __tests__/screens/health/HealthMetricDetail.test.tsx:**\n\nTest cases:\n- Renders line chart with historical data points\n- Displays statistics (avg, min, max, trend arrow, percentage)\n- Shows correct data source indicator icon/text\n- Date range filter buttons update chart data (7d/30d/90d)\n- Handles empty data gracefully with message\n- Loading state shows ActivityIndicator\n- Error state with retry button\n\n**3. __tests__/screens/health/AddHealthMetric.test.tsx:**\n\nTest cases:\n- Metric type picker renders all HealthMetricType enum values\n- Value input accepts numeric input and shows correct unit dynamically\n- Date/time picker defaults to current time and opens modal\n- Source automatically displays \"Manual\"\n- Shows validation error for out-of-range values (test per metric type bounds)\n- Shows required field validation when value empty\n- Successful submission calls API and navigates back\n- Cancel button navigates back without saving\n- Form inputs disabled during submission (isLoading state)\n- Shows loading indicator on Save button during submission\n\n**Common Mocks:**\n- Mock @react-native-community/datetimepicker\n- Mock @react-native-picker/picker\n- Mock react-native-chart-kit or victory-native\n- Mock expo-linear-gradient",
            "status": "done",
            "testStrategy": "Run with `npm test __tests__/screens/health/`. Verify all user interactions are tested. Use `fireEvent` for button presses and text input. Use `waitFor` for async operations. Target 80%+ coverage for all three screens.",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T01:46:30.166Z"
          }
        ],
        "updatedAt": "2025-12-05T01:59:24.616Z"
      },
      {
        "id": 4,
        "title": "Build Activity Tracking Mobile UI Screens",
        "description": "Create mobile screens for viewing and manually logging activities. Backend API is complete at /api/activities.",
        "details": "1. Create new screens in `app/` directory:\n   - `app/activity/index.tsx` - Activity list/history\n   - `app/activity/[id].tsx` - Activity detail view\n   - `app/activity/add.tsx` - Manual activity entry form\n\n2. Activity List (`app/activity/index.tsx`):\n   - Weekly summary card: total minutes, calories, workout count\n   - Filter by activity type (All, Cardio, Strength, Flexibility)\n   - List of recent activities with icon, duration, calories\n   - Floating action button to add new activity\n   - Pull-to-refresh\n\n3. Activity Detail View (`app/activity/[id].tsx`):\n   - Display all activity fields: type, duration, intensity, calories\n   - Heart rate data if available (avg, max)\n   - Distance and steps for applicable activities\n   - Notes field\n   - Edit/Delete buttons\n\n4. Manual Entry Form (`app/activity/add.tsx`):\n   - Activity type picker (21 types from ActivityType enum)\n   - Intensity picker (Low, Moderate, High, Maximum)\n   - Duration input (hours:minutes picker)\n   - Date/time pickers for start time\n   - Optional fields: calories, heart rate, distance, notes\n   - Validation: duration > 0, end time > start time\n\n5. Create API client in `lib/api/activities.ts`:\n```typescript\nexport const activitiesApi = {\n  getAll: (params?: { activityType?: string; startDate?: string }) =>\n    apiClient.get('/activities', { params }),\n  getById: (id: string) => apiClient.get(`/activities/${id}`),\n  create: (data: CreateActivityInput) => apiClient.post('/activities', data),\n  update: (id: string, data: Partial<CreateActivityInput>) =>\n    apiClient.put(`/activities/${id}`, data),\n  delete: (id: string) => apiClient.delete(`/activities/${id}`),\n  getWeeklySummary: () => apiClient.get('/activities/weekly-summary'),\n}\n```\n\n6. Add activity icons mapping for different activity types",
        "testStrategy": "1. Component tests for each screen\n2. Test form validation (duration, time constraints)\n3. Test activity type filtering\n4. Test CRUD operations with mock API\n5. Test weekly summary calculation display\n6. Test edit/delete flows with confirmation dialogs",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Implement Apple HealthKit Integration",
        "description": "Enable automatic sync of comprehensive health data from Apple HealthKit including cardiovascular metrics (RHR, Heart Rate, HRV SDNN/RMSSD), sleep quality metrics (duration, deep sleep, REM, efficiency, score), respiratory data (respiratory rate, oxygen saturation), and VO2Max.",
        "status": "done",
        "dependencies": [
          "3",
          "4"
        ],
        "priority": "medium",
        "details": "1. Install and configure react-native-health (recommended for Expo managed workflow):\n   - Add `react-native-health` to package.json dependencies\n   - Configure `app.json` with NSHealthShareUsageDescription and NSHealthUpdateUsageDescription\n   - Add HealthKit entitlement to iOS build configuration\n   - Request HealthKit permissions on iOS for all metric types\n\n2. **Schema Consideration**: The existing Prisma HealthMetricType enum in `server/prisma/schema.prisma` already supports:\n   - Cardiovascular: `RESTING_HEART_RATE`, `HEART_RATE_VARIABILITY_SDNN`, `HEART_RATE_VARIABILITY_RMSSD`\n   - Respiratory: `RESPIRATORY_RATE`, `OXYGEN_SATURATION`, `VO2_MAX`\n   - Sleep: `SLEEP_DURATION`, `DEEP_SLEEP_DURATION`, `REM_SLEEP_DURATION`, `SLEEP_EFFICIENCY`, `SLEEP_SCORE`\n   - NOTE: Consider adding `HEART_RATE` (instantaneous/average HR, separate from resting) if needed for workout HR data\n\n3. Create health sync service in `lib/services/healthkit.ts`:\n```typescript\nexport interface HealthKitConfig {\n  permissions: {\n    read: HealthKitPermission[];\n    write?: HealthKitPermission[];\n  };\n}\n\nexport const healthKitService = {\n  // Initialize and request permissions\n  requestPermissions: () => Promise<boolean>,\n  isAvailable: () => Promise<boolean>,\n  \n  // Sync cardiovascular data\n  syncCardiovascularMetrics: (startDate: Date, endDate: Date) => Promise<HealthMetric[]>,\n  // - HKQuantityTypeIdentifierRestingHeartRate → RESTING_HEART_RATE\n  // - HKQuantityTypeIdentifierHeartRate → instantaneous HR samples\n  // - HKQuantityTypeIdentifierHeartRateVariabilitySDNN → HEART_RATE_VARIABILITY_SDNN\n  // - HKQuantityTypeIdentifier.heartRateVariabilityRMSSD (if available)\n  \n  // Sync respiratory data\n  syncRespiratoryMetrics: (startDate: Date, endDate: Date) => Promise<HealthMetric[]>,\n  // - HKQuantityTypeIdentifierRespiratoryRate → RESPIRATORY_RATE\n  // - HKQuantityTypeIdentifierOxygenSaturation → OXYGEN_SATURATION\n  // - HKQuantityTypeIdentifierVO2Max → VO2_MAX\n  \n  // Sync sleep data\n  syncSleepMetrics: (startDate: Date, endDate: Date) => Promise<HealthMetric[]>,\n  // - HKCategoryTypeIdentifierSleepAnalysis → parse into:\n  //   - SLEEP_DURATION (total sleep time)\n  //   - DEEP_SLEEP_DURATION (deep/core sleep stages)\n  //   - REM_SLEEP_DURATION (REM stages)\n  //   - SLEEP_EFFICIENCY (time asleep / time in bed)\n  //   - SLEEP_SCORE (if available from source)\n  \n  // Sync activity data\n  syncActivityMetrics: (startDate: Date, endDate: Date) => Promise<HealthMetric[]>,\n  // - HKQuantityTypeIdentifierStepCount → STEPS\n  // - HKQuantityTypeIdentifierActiveEnergyBurned → ACTIVE_CALORIES\n  // - HKWorkoutType → Activity model with exercise HR data\n  \n  // Background sync\n  setupBackgroundSync: () => void,\n}\n```\n\n4. HealthKit type identifiers mapping:\n```typescript\nconst HEALTHKIT_TYPE_MAP = {\n  // Cardiovascular\n  HKQuantityTypeIdentifierRestingHeartRate: 'RESTING_HEART_RATE',\n  HKQuantityTypeIdentifierHeartRateVariabilitySDNN: 'HEART_RATE_VARIABILITY_SDNN',\n  // Note: RMSSD might need manual calculation from RR intervals\n  \n  // Respiratory\n  HKQuantityTypeIdentifierRespiratoryRate: 'RESPIRATORY_RATE',\n  HKQuantityTypeIdentifierOxygenSaturation: 'OXYGEN_SATURATION',\n  HKQuantityTypeIdentifierVO2Max: 'VO2_MAX',\n  \n  // Sleep (requires parsing HKCategoryTypeIdentifierSleepAnalysis)\n  // Sleep stages: inBed, asleepUnspecified, awake, asleepCore, asleepDeep, asleepREM\n  \n  // Activity\n  HKQuantityTypeIdentifierStepCount: 'STEPS',\n  HKQuantityTypeIdentifierActiveEnergyBurned: 'ACTIVE_CALORIES',\n};\n```\n\n5. Implement data transformation layer:\n   - Convert HealthKit units to API units (bpm, ms, %, steps, kcal, etc.)\n   - Match existing Zod validation schemas in `server/src/validation/schemas.ts`\n   - Use source: 'apple_health' to match healthMetricSourceSchema\n   - Include device metadata: {device: \"Apple Watch\", quality: \"high\"}\n\n6. Create sync flow:\n   - Initial sync: Fetch last 30 days of data on first connect\n   - Incremental sync: Fetch data since last sync timestamp\n   - Store lastSyncTimestamp in Expo SecureStore (per metric type for efficiency)\n   - Batch API calls using bulkCreateHealthMetricsSchema (50 items per request)\n   - Handle timezone conversions (HealthKit returns local time, API expects UTC)\n\n7. Handle data deduplication:\n   - Use existing (userId, metricType, recordedAt, source) unique constraint in schema\n   - Server handles conflicts via upsert\n   - Store sourceId from HealthKit sample UUID for traceability\n\n8. Add sync UI in profile settings (app/profile.tsx or new app/settings/health.tsx):\n   - Connect/Disconnect Apple Health button\n   - Permission status for each metric category\n   - Last sync timestamp display (per category)\n   - Manual sync button with progress indicator\n   - Sync status indicator (syncing, synced, error)\n   - Data preview showing recently synced metrics\n\n9. Handle background sync (future enhancement):\n   - Configure iOS background fetch capability\n   - Sync when app becomes active via AppState listener\n   - Respect battery and data usage settings\n   - Consider using HealthKit's HKObserverQuery for real-time updates",
        "testStrategy": "1. Unit tests for HealthKit service:\n   - Mock react-native-health module for simulator testing\n   - Test permission request flow and error handling\n   - Test data transformation for each metric type (HK format → API format)\n   - Test unit conversions (HK units → standard units)\n\n2. Test cardiovascular data sync:\n   - Mock RHR samples → verify RESTING_HEART_RATE records\n   - Mock HRV samples → verify HEART_RATE_VARIABILITY_SDNN records\n   - Test edge cases: missing data, invalid values\n\n3. Test respiratory data sync:\n   - Mock respiratory rate → verify RESPIRATORY_RATE records\n   - Mock SpO2 → verify OXYGEN_SATURATION records\n   - Mock VO2Max → verify VO2_MAX records\n\n4. Test sleep data parsing:\n   - Mock sleep analysis categories → verify correct stage classification\n   - Test sleep efficiency calculation (time asleep / time in bed)\n   - Test sleep duration aggregation across fragmented sleep\n\n5. Test sync error handling and retry logic:\n   - Network errors during bulk upload\n   - Partial failures in batch operations\n   - Permission denied scenarios\n\n6. Test deduplication with existing data:\n   - Re-sync same data → verify no duplicates\n   - Test sourceId matching\n\n7. Test UI state updates during sync:\n   - Loading states\n   - Progress indication\n   - Error display\n\n8. Manual testing on physical device:\n   - Test with real Apple Watch data\n   - Verify data accuracy against Health app\n   - Test permission prompts\n   - Test background sync behavior",
        "subtasks": [
          {
            "id": 1,
            "title": "Install and configure react-native-health package",
            "description": "Add react-native-health dependency and configure iOS build settings with proper entitlements and Info.plist descriptions",
            "dependencies": [],
            "details": "Install react-native-health via npm/yarn. Update app.json with NSHealthShareUsageDescription explaining why the app needs read access to health data. Add HealthKit entitlement. Configure iOS build to include HealthKit framework. Test that the package builds correctly on iOS simulator/device.",
            "status": "done",
            "testStrategy": "Verify package installs without errors. Confirm iOS build succeeds. Check Info.plist contains correct health usage descriptions.",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T20:30:33.391Z"
          },
          {
            "id": 2,
            "title": "Create HealthKit permission management system",
            "description": "Implement permission request flow for all required HealthKit data types including cardiovascular, respiratory, and sleep metrics",
            "dependencies": [
              1
            ],
            "details": "Create lib/services/healthkit/permissions.ts. Define permission sets for each metric category. Implement isAvailable() check for HealthKit. Implement requestPermissions() with granular permission requests. Handle partial permission grants gracefully. Store permission status in context/state.",
            "status": "done",
            "testStrategy": "Mock HealthKit permissions. Test all permission states (granted, denied, not determined). Test partial permission scenarios.",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T20:30:33.786Z"
          },
          {
            "id": 3,
            "title": "Implement cardiovascular metrics sync (RHR, HR, HRV)",
            "description": "Create sync functions for Resting Heart Rate, Heart Rate samples, and Heart Rate Variability (SDNN and RMSSD)",
            "dependencies": [
              2
            ],
            "details": "Implement syncCardiovascularMetrics() in healthkit.ts. Query HKQuantityTypeIdentifierRestingHeartRate for RHR. Query HKQuantityTypeIdentifierHeartRateVariabilitySDNN for HRV. Transform HealthKit samples to match HealthMetric API format. Handle unit conversions (HK returns bpm/ms). Include device metadata from sample source.",
            "status": "done",
            "testStrategy": "Mock HK cardiovascular queries. Verify correct transformation to RESTING_HEART_RATE and HEART_RATE_VARIABILITY_SDNN types. Test unit handling.",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T20:30:33.979Z"
          },
          {
            "id": 4,
            "title": "Implement respiratory metrics sync (respiratory rate, SpO2, VO2Max)",
            "description": "Create sync functions for respiratory rate, oxygen saturation, and VO2Max data from HealthKit",
            "dependencies": [
              2
            ],
            "details": "Implement syncRespiratoryMetrics() in healthkit.ts. Query HKQuantityTypeIdentifierRespiratoryRate, HKQuantityTypeIdentifierOxygenSaturation, and HKQuantityTypeIdentifierVO2Max. Transform to RESPIRATORY_RATE, OXYGEN_SATURATION, and VO2_MAX metric types. Handle different sample frequencies (VO2Max is less frequent).",
            "status": "done",
            "testStrategy": "Mock HK respiratory queries. Verify correct transformation to API format. Test handling of sparse VO2Max data.",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T20:30:34.209Z"
          },
          {
            "id": 5,
            "title": "Implement sleep metrics sync with stage classification",
            "description": "Create sync function for sleep analysis including duration, deep sleep, REM, and efficiency calculations",
            "dependencies": [
              2
            ],
            "details": "Implement syncSleepMetrics() in healthkit.ts. Query HKCategoryTypeIdentifierSleepAnalysis. Parse sleep stages (asleepCore→DEEP_SLEEP, asleepDeep→DEEP_SLEEP, asleepREM→REM_SLEEP). Calculate total SLEEP_DURATION. Calculate SLEEP_EFFICIENCY (asleep time / in bed time). Handle fragmented sleep sessions.",
            "status": "done",
            "testStrategy": "Mock HK sleep analysis with various stage combinations. Verify correct duration calculations. Test efficiency calculation accuracy. Test fragmented sleep handling.",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T20:30:34.351Z"
          },
          {
            "id": 6,
            "title": "Implement activity metrics sync (steps, active calories)",
            "description": "Create sync function for daily activity data including step count and active energy burned",
            "dependencies": [
              2
            ],
            "details": "Implement syncActivityMetrics() in healthkit.ts. Query HKQuantityTypeIdentifierStepCount and HKQuantityTypeIdentifierActiveEnergyBurned. Aggregate daily totals. Transform to STEPS and ACTIVE_CALORIES metric types. Handle timezone boundaries for daily aggregation.",
            "status": "done",
            "testStrategy": "Mock HK activity queries. Verify daily aggregation logic. Test timezone handling for day boundaries.",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T20:30:34.528Z"
          },
          {
            "id": 7,
            "title": "Create batch sync orchestration with API integration",
            "description": "Implement the main sync orchestration that coordinates all metric syncs and uploads to the backend API",
            "dependencies": [
              3,
              4,
              5,
              6
            ],
            "details": "Create syncAllHealthData() coordinator function. Implement incremental sync using stored lastSyncTimestamp from SecureStore. Batch API uploads using bulkCreateHealthMetricsSchema (50 items per request). Handle partial failures and retry logic. Update lastSyncTimestamp per metric category on success.",
            "status": "done",
            "testStrategy": "Test full sync orchestration flow. Verify batch chunking at 50 items. Test incremental sync with stored timestamps. Test error handling and retry.",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T20:30:34.685Z"
          },
          {
            "id": 8,
            "title": "Build HealthKit settings UI with sync controls",
            "description": "Create the user interface for managing HealthKit connection, viewing sync status, and triggering manual syncs",
            "dependencies": [
              7
            ],
            "details": "Create app/settings/health.tsx screen or add section to app/profile.tsx. Display Connect/Disconnect Apple Health button. Show permission status per metric category with toggle indicators. Display last sync timestamp and synced data counts. Add manual Sync Now button with progress indicator. Show sync status (syncing/synced/error) with appropriate feedback.",
            "status": "done",
            "testStrategy": "Test UI component rendering. Test connect/disconnect flow. Test sync button interaction and loading states. Test error state display.",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T20:30:34.823Z"
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 7,
        "expansionPrompt": "Break down the Apple HealthKit integration into implementation phases:\n\n1. **Library Setup & Configuration**: Install react-native-health or expo-health-connect, configure iOS entitlements in app.json (NSHealthShareUsageDescription), and set up the Expo plugin. Research which library works best with Expo SDK 52.\n\n2. **HealthKit Service Core**: Create `lib/services/healthkit.ts` with typed interfaces matching the existing TypeScript patterns in `lib/types/index.ts`. Implement the base service structure with permission request methods.\n\n3. **Data Fetching Implementation**: Implement data fetching for each metric type mapping HealthKit identifiers to the existing HealthMetricType enum (RESTING_HEART_RATE, HEART_RATE_VARIABILITY_SDNN, SLEEP_DURATION, STEPS, ACTIVE_CALORIES). Note: Add ACTIVE_HEART_RATE to schema.prisma for instantaneous HR.\n\n4. **Activity Sync Implementation**: Map HKWorkoutType to the existing ActivityType enum (RUNNING, CYCLING, SWIMMING, etc.) in schema.prisma. Handle activity data transformation.\n\n5. **Sync Logic & State Management**: Implement initial sync (30 days), incremental sync with lastSyncTimestamp stored in SecureStore (pattern from lib/api/client.ts), batch API calls (50 items/request), and leverage the existing bulk endpoint at `/api/health-metrics/bulk`.\n\n6. **Profile UI Integration**: Extend the existing `app/(tabs)/profile.tsx` with a Health Integration section. Add Connect/Disconnect Apple Health button, sync status indicators, last sync timestamp display, and manual sync button following the existing design patterns.\n\n7. **Testing & Error Handling**: Mock HealthKit data for simulator, test permission flows, data transformation, sync error handling/retry logic, and UI state updates. Follow existing error handling patterns in lib/utils/errorHandling.ts.",
        "updatedAt": "2025-12-11T20:30:34.823Z"
      },
      {
        "id": 6,
        "title": "Train and Deploy LSTM Models for Health Predictions",
        "description": "Train LSTM models for RHR and HRV prediction using the existing model architecture in ml-service/app/ml_models/lstm.py and make them production-ready.",
        "details": "1. Create training pipeline in `ml-service/app/services/model_training.py`:\n   - Already has TrainModelRequest/Response schemas\n   - Implement data loading from database\n   - Create training/validation split (80/20)\n   - Add early stopping with patience=10\n   - Save model checkpoints and metadata\n\n2. Training data preparation:\n   - Use FeatureEngineeringService to generate features\n   - Create sliding window sequences (30-day windows)\n   - Normalize features using StandardScaler (save scaler with model)\n   - Handle missing data: forward-fill then drop incomplete sequences\n\n3. Training configuration:\n   - RHR model: hidden_dim=128, num_layers=2, dropout=0.2\n   - HRV model: hidden_dim=128, num_layers=2, dropout=0.2\n   - Batch size: 32, learning rate: 0.001\n   - Use Adam optimizer, MSE loss\n   - Train for max 100 epochs with early stopping\n\n4. Model evaluation metrics:\n   - MAE (Mean Absolute Error)\n   - RMSE (Root Mean Square Error)\n   - R² score (>0.5 for production)\n   - MAPE (Mean Absolute Percentage Error, <15% for production)\n\n5. Update PredictionService in `ml-service/app/services/prediction.py`:\n   - Load trained model from disk\n   - Load corresponding scaler\n   - Prepare input sequence from recent features\n   - Run inference and denormalize output\n   - Calculate confidence intervals\n\n6. Model storage structure:\n```\nml-service/models/\n  {user_id}_{metric}_{timestamp}/\n    model.pt              # PyTorch model weights\n    scaler.pkl           # Feature scaler\n    metadata.pkl         # Training config and metrics\n```\n\n7. Add minimum data requirements:\n   - At least 30 days of health data\n   - At least 21 days of nutrition data\n   - Check requirements before training",
        "testStrategy": "1. Unit tests for data preparation pipeline\n2. Test training with synthetic data (verify loss decreases)\n3. Test model save/load roundtrip\n4. Test prediction accuracy on held-out test set\n5. Integration test: full train -> predict flow\n6. Test minimum data requirement validation\n7. Test early stopping triggers correctly",
        "priority": "high",
        "dependencies": [
          "3",
          "5"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Create Predictions Visualization Mobile UI",
        "description": "Build mobile screens to display ML predictions (RHR, HRV forecasts) with confidence intervals and historical context.",
        "details": "1. Create new screens:\n   - `app/predictions/index.tsx` - Predictions dashboard\n   - `app/predictions/[metric].tsx` - Detailed prediction view\n\n2. Predictions Dashboard (`app/predictions/index.tsx`):\n   - Card for each predictable metric (RHR, HRV)\n   - Display: predicted value, confidence score, direction indicator\n   - Comparison to 30-day average\n   - 'No prediction available' state if model not trained\n   - Pull-to-refresh to get latest predictions\n\n3. Detailed Prediction View (`app/predictions/[metric].tsx`):\n   - Chart showing:\n     - Historical values (last 30 days)\n     - Predicted value for tomorrow\n     - Confidence interval as shaded region\n   - Interpretation text (AI-generated explanation)\n   - Recommendation based on prediction\n   - Feature importance breakdown (what drove this prediction)\n\n4. Create API client in `lib/api/predictions.ts`:\n```typescript\nexport const predictionsApi = {\n  predict: (metric: string, targetDate: string) =>\n    apiClient.post('/api/predictions/predict', { metric, target_date: targetDate }),\n  batchPredict: (metrics: string[], targetDate: string) =>\n    apiClient.post('/api/predictions/batch-predict', { metrics, target_date: targetDate }),\n  listModels: () => apiClient.get('/api/predictions/models'),\n}\n```\n\n5. Chart implementation:\n   - Use Victory Native or react-native-chart-kit\n   - Line chart for historical + predicted\n   - Shaded area for confidence interval\n   - Animate prediction point\n\n6. Handle states:\n   - Loading: Show skeleton\n   - No model trained: Show CTA to collect more data\n   - Prediction available: Show full UI\n   - Error: Show error message with retry",
        "testStrategy": "1. Component tests for dashboard and detail screens\n2. Test chart rendering with mock data\n3. Test loading/error/empty states\n4. Test confidence interval visualization\n5. Test API integration with mock responses\n6. Snapshot tests for consistent UI",
        "priority": "medium",
        "dependencies": [
          "6"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Implement ML Insights Engine and Recommendations",
        "description": "Build the insights generation system that analyzes correlations and generates personalized nutrition recommendations stored in MLInsight model.",
        "details": "1. Create insights service in `ml-service/app/services/insights_engine.py`:\n```python\nclass InsightsEngine:\n    async def generate_insights(self, user_id: str) -> List[MLInsight]:\n        correlations = await self._get_significant_correlations(user_id)\n        predictions = await self._get_recent_predictions(user_id)\n        anomalies = await self._detect_anomalies(user_id)\n        \n        insights = []\n        insights.extend(self._correlation_insights(correlations))\n        insights.extend(self._prediction_insights(predictions))\n        insights.extend(self._anomaly_insights(anomalies))\n        insights.extend(self._goal_progress_insights(user_id))\n        \n        return self._prioritize_and_limit(insights, max_insights=5)\n```\n\n2. Insight types to implement:\n   - CORRELATION: 'Your protein intake correlates with better HRV (+0.65)'\n   - PREDICTION: 'Tomorrow's RHR is predicted higher than average'\n   - ANOMALY: 'Your sleep duration last night was unusually low'\n   - RECOMMENDATION: 'Try eating dinner earlier to improve sleep quality'\n   - GOAL_PROGRESS: 'You're 80% of the way to your protein goal this week'\n   - PATTERN_DETECTED: 'You tend to eat more carbs on weekends'\n\n3. Correlation-based recommendations:\n   - Use CorrelationEngineService to find significant correlations\n   - Filter by correlation strength (|r| > 0.5)\n   - Generate natural language recommendations\n   - Example: If protein ↔ HRV has r=0.7, recommend 'Increasing protein may improve your HRV'\n\n4. Anomaly detection:\n   - Z-score based detection (>2 std from 30-day mean)\n   - Detect unusual: meal timing, calorie intake, sleep duration\n   - Generate alerts for negative anomalies\n\n5. Create API endpoints in `ml-service/app/api/insights.py`:\n   - GET /api/insights - List user's active insights\n   - POST /api/insights/generate - Trigger insight generation\n   - PUT /api/insights/{id}/viewed - Mark as viewed\n   - PUT /api/insights/{id}/dismissed - Dismiss insight\n   - PUT /api/insights/{id}/feedback - Submit helpful/not helpful\n\n6. Store insights in database using MLInsight model (already defined in Prisma schema)",
        "testStrategy": "1. Unit tests for each insight type generator\n2. Test insight prioritization logic\n3. Test anomaly detection thresholds\n4. Test natural language generation\n5. Integration test: end-to-end insight generation\n6. Test user feedback tracking\n7. Test insight expiration handling",
        "priority": "medium",
        "dependencies": [
          "6"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Build Insights Feed Mobile UI",
        "description": "Create mobile screens to display ML-generated insights, recommendations, and allow user feedback.",
        "details": "1. Create new screens:\n   - `app/insights/index.tsx` - Insights feed/dashboard\n   - `app/insights/[id].tsx` - Detailed insight view\n\n2. Insights Feed (`app/insights/index.tsx`):\n   - List of insight cards sorted by priority and recency\n   - Card components per insight type:\n     - Correlation: Show correlation strength badge\n     - Prediction: Show predicted value and arrow\n     - Anomaly: Show warning indicator\n     - Recommendation: Show actionable tip\n   - Swipe to dismiss functionality\n   - Pull-to-refresh to generate new insights\n   - Empty state: 'Keep logging meals to unlock insights'\n\n3. Insight Card Design:\n```typescript\ninterface InsightCard {\n  icon: string;           // Based on insightType\n  title: string;          // From insight.title\n  description: string;    // Truncated insight.description\n  priority: 'high' | 'medium' | 'low'; // Color coding\n  correlation?: number;   // Show badge if correlation insight\n  timestamp: Date;        // When generated\n}\n```\n\n4. Detailed Insight View (`app/insights/[id].tsx`):\n   - Full description text\n   - Recommendation with call-to-action\n   - Supporting chart/data if applicable\n   - 'Was this helpful?' feedback buttons\n   - Share insight button (future)\n\n5. Create API client in `lib/api/insights.ts`:\n```typescript\nexport const insightsApi = {\n  getAll: () => apiClient.get('/api/insights'),\n  getById: (id: string) => apiClient.get(`/api/insights/${id}`),\n  markViewed: (id: string) => apiClient.put(`/api/insights/${id}/viewed`),\n  dismiss: (id: string) => apiClient.put(`/api/insights/${id}/dismissed`),\n  submitFeedback: (id: string, helpful: boolean) =>\n    apiClient.put(`/api/insights/${id}/feedback`, { helpful }),\n}\n```\n\n6. Add insights badge to tab bar showing unread count",
        "testStrategy": "1. Component tests for insight cards\n2. Test swipe-to-dismiss interaction\n3. Test feedback submission flow\n4. Test empty and loading states\n5. Test priority-based sorting\n6. Visual regression tests for card styles",
        "priority": "medium",
        "dependencies": [
          "8"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Implement AR Portion Size Measurement",
        "description": "Add AR capability to measure food portion dimensions and improve nutrition estimation accuracy.",
        "details": "1. Install AR dependencies:\n   - expo-three (already installed - three.js is in dependencies)\n   - expo-gl (already installed)\n   - @react-three/fiber for React Native\n\n2. Create AR measurement component in `lib/components/ARPortionMeasure.tsx`:\n   - Initialize AR session with plane detection\n   - Render measurement guides on detected surfaces\n   - Allow user to place measurement points\n   - Calculate bounding box dimensions (width, height, depth)\n   - Return dimensions in centimeters\n\n3. Update food scanning flow (`app/scan-food.tsx`):\n   - Add 'Measure with AR' button after capturing photo\n   - Launch AR measurement overlay\n   - Pass dimensions to food analysis API\n   - Update `mockMeasurements` with real AR data\n\n4. AR measurement flow:\n   1. User captures food photo\n   2. User taps 'Measure Portion'\n   3. AR view opens with plane detection\n   4. User taps to place corner points (4 points for bounding box)\n   5. App calculates volume and converts to portion weight\n   6. Dimensions sent to /api/food/analyze\n\n5. Dimension to weight conversion (in food_analysis_service.py):\n   - Already implemented in `_estimate_portion_from_dimensions()`\n   - Uses food density estimates\n   - Returns estimated weight in grams\n\n6. Calibration feature:\n   - Include reference object option (credit card, hand)\n   - Use known dimensions to calibrate scale\n   - Improve accuracy for subsequent measurements\n\n7. Fallback handling:\n   - If AR not supported (older devices), show manual size picker\n   - Options: Small, Medium, Large with example photos",
        "testStrategy": "1. Unit tests for dimension calculation\n2. Test AR component mounting/unmounting\n3. Test plane detection callbacks\n4. Integration test with mock AR data\n5. Test fallback to manual size picker\n6. Manual testing on physical device with AR support\n7. Test calibration accuracy with known objects",
        "priority": "low",
        "dependencies": [
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Install AR Dependencies and Configure Native Modules",
            "description": "Add required AR dependencies for React Native/Expo including @react-three/fiber, react-three-fiber, and configure native AR capabilities for iOS (ARKit) and Android (ARCore).",
            "dependencies": [],
            "details": "1. Install @react-three/fiber and react-three-fiber packages\n2. Configure expo plugins for AR in app.json (expo-camera already configured)\n3. Set up iOS ARKit permissions in Info.plist (NSCameraUsageDescription already exists)\n4. Configure Android ARCore requirements in AndroidManifest.xml\n5. Verify expo-gl and three.js integration\n6. Create basic AR session test to verify setup\n7. Document AR capability requirements for devices (iOS 11+, ARCore-compatible Android)",
            "status": "done",
            "testStrategy": "1. Test package installation with npm/yarn\n2. Verify expo-gl renders basic 3D scene\n3. Test AR session initialization on iOS simulator (limited) and physical device\n4. Verify ARKit permissions prompt\n5. Test Android ARCore availability detection",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T19:10:26.758Z"
          },
          {
            "id": 2,
            "title": "Create Interactive AR Measurement Component",
            "description": "Build the core ARPortionMeasure component that allows users to tap 4 corner points to create a bounding box and measure food portion dimensions in real-world coordinates.",
            "dependencies": [
              1
            ],
            "details": "1. Create lib/components/ARPortionMeasure.tsx component\n2. Initialize AR session with plane detection enabled\n3. Implement tap-to-place point placement (4 corners for bounding box)\n4. Convert screen coordinates to world coordinates using AR raycasting\n5. Calculate real-world dimensions (width, height, depth) from placed points\n6. Display visual guides showing detected plane surface\n7. Render bounding box overlay with dimension labels\n8. Add point placement indicators and connection lines\n9. Implement reset/undo functionality for point placement\n10. Return ARMeasurement type with confidence scoring based on plane detection quality\n11. Handle edge cases: insufficient plane detection, invalid point placement",
            "status": "done",
            "testStrategy": "1. Unit tests for coordinate conversion calculations\n2. Component tests for point placement state management\n3. Test bounding box dimension calculations with known distances\n4. Test plane detection callbacks and state updates\n5. Integration test with mock AR session data\n6. Manual testing on physical device with various surfaces\n7. Test reset/undo functionality",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T19:16:24.790Z"
          },
          {
            "id": 3,
            "title": "Build AR Measurement Modal/Overlay Screen",
            "description": "Create the modal screen (app/ar-measure-portion.tsx) that launches the AR measurement experience with user instructions and controls.",
            "dependencies": [
              2
            ],
            "details": "1. Create app/ar-measure-portion.tsx as a modal screen\n2. Integrate ARPortionMeasure component into modal\n3. Design instruction UI:\n   - Step-by-step guide for users (detect plane, place 4 corners)\n   - Visual indicators for current step\n   - Progress indicator during plane detection\n4. Add control buttons:\n   - Confirm measurement (validates 4 points placed)\n   - Cancel and return to scan screen\n   - Reset measurement (clear all points)\n5. Display real-time measurement quality indicator\n6. Show current dimensions as user places points\n7. Handle AR session lifecycle (start on mount, cleanup on unmount)\n8. Add error states: no plane detected, AR not supported\n9. Implement navigation: return measured dimensions to caller",
            "status": "done",
            "testStrategy": "1. Component mounting/unmounting tests\n2. Test navigation with expo-router params\n3. Test confirm button validation (requires 4 points)\n4. Test cancel navigation back to scan screen\n5. Test reset functionality clears all state\n6. Integration test: full flow from scan to measurement to confirmation",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T19:22:51.508Z"
          },
          {
            "id": 4,
            "title": "Integrate AR Measurement into Food Scanning Flow",
            "description": "Update scan-food.tsx to include 'Measure with AR' button after photo capture, launch the AR measurement modal, and pass captured dimensions to the food analysis API.",
            "dependencies": [
              3
            ],
            "details": "1. Update scan-food.tsx after photo capture (line 114 area)\n2. Add 'Measure with AR' button alongside 'Analyze Food' button\n3. Implement AR measurement flow:\n   - Launch ar-measure-portion modal\n   - Receive ARMeasurement result from modal\n   - Store measurements in component state\n4. Update foodAnalysisApi.analyzeFood() call to include measurements\n5. Replace mockMeasurements with real AR data\n6. Display measurement quality indicator in UI (high/medium/low badge)\n7. Show captured dimensions in preview (width x height x depth)\n8. Allow re-measurement before final analysis\n9. Handle AR not available gracefully (hide button, show alternative)\n10. Update UI flow: Photo → Measure (optional) → Analyze → Results",
            "status": "done",
            "testStrategy": "1. Test button visibility after photo capture\n2. Test modal launch with expo-router\n3. Test receiving ARMeasurement data from modal\n4. Test API call includes measurements in request\n5. Test UI updates with measurement quality indicator\n6. Integration test: full scan flow with AR measurement\n7. Test fallback when AR not available",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement Reference Object Calibration Feature",
            "description": "Build calibration wizard allowing users to use a credit card or other reference object to improve AR measurement accuracy.",
            "dependencies": [
              2
            ],
            "details": "1. Create lib/components/ARCalibration.tsx component\n2. Implement credit card calibration mode:\n   - Standard dimensions: 85.60mm × 53.98mm\n   - AR measurement of credit card\n   - Calculate calibration factor: measured/actual\n3. Build calibration wizard UI:\n   - Introduction screen explaining calibration\n   - Place credit card on surface instructions\n   - Measure card with AR (4 corner points)\n   - Validation: check if dimensions are reasonable (within 20% of standard)\n   - Success/failure feedback\n4. Store calibration factor in AsyncStorage/SecureStore\n5. Apply calibration to subsequent measurements (multiply by factor)\n6. Add calibration status indicator in AR measurement screen\n7. Optional: Allow re-calibration from settings\n8. Optional: Support other reference objects (smartphone, hand span)\n9. Create lib/utils/calibration.ts for storage and retrieval",
            "status": "done",
            "testStrategy": "1. Test calibration factor calculation\n2. Test storage and retrieval of calibration data\n3. Test validation of measured card dimensions\n4. Test applying calibration to measurements\n5. Component tests for calibration wizard\n6. Integration test: calibrate then measure food\n7. Test calibration persistence across app restarts",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Create Fallback Manual Size Picker for Non-AR Devices",
            "description": "Build a manual size selection UI for devices without AR/LiDAR support, providing Small/Medium/Large presets with visual references.",
            "dependencies": [],
            "details": "1. Create lib/components/ManualSizePicker.tsx component\n2. Implement size selector options:\n   - Small (e.g., 5cm × 5cm × 5cm → ~87g assuming 0.7 density)\n   - Medium (e.g., 10cm × 10cm × 10cm → ~700g)\n   - Large (e.g., 15cm × 15cm × 15cm → ~2.3kg)\n   - Custom (slider input for each dimension)\n3. Add visual reference images for each size:\n   - Small: Size of a golf ball\n   - Medium: Size of a baseball\n   - Large: Size of a grapefruit\n4. Implement custom slider:\n   - Width slider (1-30cm)\n   - Height slider (1-30cm)\n   - Depth slider (1-30cm)\n   - Real-time volume calculation display\n5. Convert selected size to ARMeasurement type:\n   - Set confidence: 'low' (manual estimate)\n   - Set planeDetected: false\n   - Set distance, width, height, depth\n6. Integrate into scan-food.tsx as fallback when AR unavailable\n7. Show manual picker when device lacks AR support or user declines AR permissions",
            "status": "done",
            "testStrategy": "1. Component tests for size selection state\n2. Test dimension calculations for presets\n3. Test slider value updates and bounds\n4. Test conversion to ARMeasurement format\n5. Test integration with scan flow\n6. Visual regression tests for UI\n7. Test device AR capability detection",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T18:02:37.997Z"
          },
          {
            "id": 7,
            "title": "Add Dimension-to-Weight Conversion Utilities",
            "description": "Create client-side utilities for volume calculation, food density lookup, and weight estimation to complement the ML service's backend estimation.",
            "dependencies": [],
            "details": "1. Create lib/utils/portion-estimation.ts utility file\n2. Implement volume calculation:\n   - volumeFromDimensions(width, height, depth): cm³\n   - applyShapeFactor(volume, shapeFactor): adjusted cm³\n3. Create food density lookup table:\n   - Common foods with g/cm³ density values\n   - Categorized by food type (fruits, vegetables, proteins, grains)\n   - Default density for unknown foods\n4. Implement weight estimation:\n   - estimateWeight(volume, foodType): grams\n   - Confidence score based on food type match\n   - Apply min/max bounds (1g - 5000g)\n5. Add unit conversion helpers:\n   - cmToInches(cm), inchesToCm(inches)\n   - gramsToOz(grams), ozToGrams(oz)\n   - volumeCm3ToMl(cm3), mlToVolumeCm3(ml)\n6. Create TypeScript interfaces for density data\n7. Export utility functions for use in components",
            "status": "done",
            "testStrategy": "1. Unit tests for volume calculations with known dimensions\n2. Test shape factor application\n3. Test density lookup for various food types\n4. Test weight estimation accuracy\n5. Test unit conversions (bidirectional)\n6. Test bounds enforcement (min/max weight)\n7. Test confidence scoring logic",
            "updatedAt": "2025-12-05T17:51:26.304Z",
            "parentId": "undefined"
          },
          {
            "id": 8,
            "title": "Write Comprehensive Tests for AR Measurement System",
            "description": "Create unit, component, and integration tests covering the entire AR measurement feature including edge cases and device compatibility.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6,
              7
            ],
            "details": "1. Unit tests for dimension calculations:\n   - Test bounding box calculation from 4 points\n   - Test coordinate conversion (screen to world)\n   - Test volume and weight calculations\n   - Test calibration factor application\n2. Component tests for ARPortionMeasure:\n   - Test point placement state management\n   - Test plane detection callbacks\n   - Test measurement completion validation\n   - Test reset functionality\n3. Component tests for ar-measure-portion modal:\n   - Test modal lifecycle (mount, unmount)\n   - Test navigation with params\n   - Test instruction UI state transitions\n4. Integration tests for measurement flow:\n   - Test full flow: scan → measure → analyze\n   - Test with calibration applied\n   - Test fallback to manual picker\n   - Test error handling (no plane, invalid points)\n5. Mock tests for devices without AR:\n   - Mock AR availability check\n   - Test manual picker display\n   - Test manual measurements passed to API\n6. Test calibration accuracy:\n   - Test with known reference object dimensions\n   - Test calibration persistence\n   - Test validation logic\n7. Add test fixtures:\n   - Mock ARMeasurement data\n   - Mock AR session responses\n   - Mock plane detection results\n8. Create test documentation in README or docs/testing.md",
            "status": "done",
            "testStrategy": "1. Run full test suite with jest\n2. Verify 80%+ code coverage for AR modules\n3. Test on iOS simulator (limited AR)\n4. Test on physical iOS device with ARKit\n5. Test on Android device with ARCore\n6. Test on older devices without AR support\n7. Regression testing after changes",
            "parentId": "undefined",
            "updatedAt": "2025-12-05T19:45:52.543Z"
          }
        ],
        "updatedAt": "2025-12-05T19:45:57.388Z"
      },
      {
        "id": 11,
        "title": "Generate OpenAPI Documentation and Polish Production Readiness",
        "description": "Add comprehensive API documentation, perform security audit, and optimize performance for production deployment.",
        "details": "1. Generate OpenAPI/Swagger documentation:\n   - Backend (Express): Add swagger-jsdoc and swagger-ui-express\n   - ML Service (FastAPI): Already has built-in docs at /docs\n   - Document all endpoints with request/response schemas\n   - Add authentication requirements\n   - Include example requests and responses\n\n2. Express API documentation setup:\n```javascript\nimport swaggerJsdoc from 'swagger-jsdoc';\nimport swaggerUi from 'swagger-ui-express';\n\nconst options = {\n  definition: {\n    openapi: '3.0.0',\n    info: { title: 'Nutri API', version: '1.0.0' },\n    servers: [{ url: '/api' }],\n    components: {\n      securitySchemes: {\n        bearerAuth: { type: 'http', scheme: 'bearer' }\n      }\n    }\n  },\n  apis: ['./src/routes/*.ts'],\n};\n\napp.use('/api-docs', swaggerUi.serve, swaggerUi.setup(swaggerJsdoc(options)));\n```\n\n3. Performance optimization:\n   - Add database query logging to identify slow queries\n   - Implement connection pooling for PostgreSQL\n   - Add Redis caching for frequently accessed data (user profile, daily summary)\n   - Compress API responses with compression middleware\n   - Optimize Prisma queries with select/include\n\n4. Security audit checklist:\n   - Review all authentication flows\n   - Verify rate limiting is effective\n   - Check for SQL injection (Prisma handles this)\n   - Verify XSS prevention in sanitize middleware\n   - Review CORS configuration\n   - Ensure sensitive data not logged\n   - Check JWT secret rotation capability\n\n5. Production configuration:\n   - Environment variable validation on startup\n   - Health check endpoints for load balancers\n   - Graceful shutdown handling\n   - Error tracking integration (Sentry ready)\n   - Logging configuration (structured JSON logs)\n\n6. Mobile app optimization:\n   - Review bundle size\n   - Implement proper loading states\n   - Add offline detection and handling\n   - Optimize image handling\n\n7. Create deployment documentation:\n   - Docker setup for backend and ML service\n   - Environment variables reference\n   - Database migration guide\n   - Monitoring recommendations",
        "testStrategy": "1. Validate OpenAPI spec with swagger-cli validate\n2. Load testing with k6 or artillery (100 concurrent users)\n3. Security scan with npm audit and OWASP ZAP\n4. Test rate limiting triggers correctly\n5. Test graceful shutdown\n6. Verify logging output format\n7. Test health check endpoints\n8. Performance benchmark for critical endpoints",
        "priority": "low",
        "dependencies": [
          "2",
          "3",
          "4",
          "6"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Implement Responsive UI Design for iPhone and iPad Devices",
        "description": "Comprehensive responsive design implementation covering all iPhone sizes (2020+) and iPads with both screen orientations. Lock iPhones to portrait mode while supporting iPad landscape/portrait. Test and optimize all 17 app screens across device categories.",
        "details": "## Technical Implementation Details\n\n### Breakpoint System (in logical pixels)\n```typescript\nconst breakpoints = {\n  // iPhones\n  iPhoneSE: { width: 375, height: 667 },      // iPhone SE 3rd gen\n  iPhoneMini: { width: 375, height: 812 },    // iPhone 12/13 Mini\n  iPhoneMedium: { width: 390, height: 844 },  // iPhone 12/13/14\n  iPhonePro: { width: 393, height: 852 },     // iPhone 14/15/16 Pro\n  iPhoneMax: { width: 430, height: 932 },     // iPhone Pro Max/Plus\n  \n  // iPads\n  iPadMini: { width: 744, height: 1133 },     // iPad Mini\n  iPad: { width: 820, height: 1180 },         // iPad/iPad Air 11\"\n  iPadPro11: { width: 834, height: 1194 },    // iPad Pro 11\"\n  iPadAir13: { width: 1032, height: 1376 },   // iPad Air 13\"\n  iPadPro13: { width: 1024, height: 1366 },   // iPad Pro 13\"\n};\n\nconst deviceCategories = {\n  small: ['iPhoneSE', 'iPhoneMini'],\n  medium: ['iPhoneMedium', 'iPhonePro'],\n  large: ['iPhoneMax'],\n  tablet: ['iPadMini', 'iPad', 'iPadPro11', 'iPadAir13', 'iPadPro13'],\n};\n```\n\n### Responsive Hook Example\n```typescript\n// lib/hooks/useResponsive.ts\nimport { useWindowDimensions, Platform } from 'react-native';\n\nexport function useResponsive() {\n  const { width, height } = useWindowDimensions();\n  \n  const isTablet = width >= 744;\n  const isLandscape = width > height;\n  const deviceCategory = getDeviceCategory(width);\n  \n  const scale = (size: number) => {\n    const baseWidth = 390; // iPhone 14 as baseline\n    return (width / baseWidth) * size;\n  };\n  \n  return { width, height, isTablet, isLandscape, deviceCategory, scale };\n}\n```\n\n### Orientation Lock (app.json)\n```json\n{\n  \"expo\": {\n    \"orientation\": \"portrait\",\n    \"ios\": {\n      \"supportsTablet\": true,\n      \"requireFullScreen\": false,\n      \"userInterfaceStyle\": \"automatic\"\n    }\n  }\n}\n```\n\n### iPad-specific orientation unlock (runtime)\n```typescript\n// In root _layout.tsx\nimport * as ScreenOrientation from 'expo-screen-orientation';\n\nuseEffect(() => {\n  async function configureOrientation() {\n    if (Platform.OS === 'ios' && Platform.isPad) {\n      await ScreenOrientation.unlockAsync();\n    } else {\n      await ScreenOrientation.lockAsync(\n        ScreenOrientation.OrientationLock.PORTRAIT_UP\n      );\n    }\n  }\n  configureOrientation();\n}, []);\n```\n\n### Simulator Testing Checklist\nEach screen must be tested on these simulators:\n- [ ] iPhone SE (3rd generation) - iOS 17+\n- [ ] iPhone 13 Mini - iOS 17+\n- [ ] iPhone 14 - iOS 17+\n- [ ] iPhone 15 Pro - iOS 17+\n- [ ] iPhone 15 Pro Max - iOS 17+\n- [ ] iPad Mini (6th generation) - Portrait\n- [ ] iPad Mini (6th generation) - Landscape\n- [ ] iPad Pro 11-inch - Portrait\n- [ ] iPad Pro 11-inch - Landscape\n- [ ] iPad Pro 13-inch - Portrait\n- [ ] iPad Pro 13-inch - Landscape\n\n### Safe Area Considerations\n- Use SafeAreaView consistently\n- Handle Dynamic Island on iPhone 14 Pro+\n- Handle home indicator on all Face ID devices\n- Handle notch on older Face ID devices\n- Handle status bar on iPhone SE\n\n### Testing Commands\n```bash\n# List available simulators\nxcrun simctl list devices available\n\n# Boot specific simulator\nxcrun simctl boot \"iPhone SE (3rd generation)\"\nxcrun simctl boot \"iPhone 15 Pro Max\"\nxcrun simctl boot \"iPad Pro 13-inch (M4)\"\n\n# Run app on specific simulator\nnpx expo run:ios --device \"iPhone SE (3rd generation)\"\nnpx expo run:ios --device \"iPad Pro 13-inch (M4)\"\n```",
        "testStrategy": "## Testing Strategy\n\n### Unit Tests\n- Test useResponsive hook returns correct device categories\n- Test scale functions produce expected values\n- Test breakpoint detection logic\n\n### Visual Regression Testing\n- Screenshot each screen on each device category\n- Compare layouts visually\n- Verify no text truncation or overflow\n- Verify touch targets are accessible (44pt minimum)\n\n### Manual Testing Checklist per Screen\n\n#### For each of the 17 screens, verify:\n1. **Layout Integrity**\n   - No horizontal scrolling when not intended\n   - Content fits within safe areas\n   - Proper padding/margins on all edges\n\n2. **Typography**\n   - All text is readable\n   - No text truncation (unless intentional with ellipsis)\n   - Font sizes appropriate for device\n\n3. **Interactive Elements**\n   - Buttons are tappable (44pt minimum)\n   - Form fields are usable\n   - Scrolling works smoothly\n\n4. **Orientation (iPad only)**\n   - Smooth rotation transition\n   - Layout adapts correctly\n   - No content loss during rotation\n\n### Device Matrix\n| Screen | SE | Mini | Medium | Pro Max | iPad Mini P | iPad Mini L | iPad Pro P | iPad Pro L |\n|--------|:--:|:----:|:------:|:-------:|:-----------:|:-----------:|:----------:|:----------:|\n| welcome | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ |\n| signin | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ |\n| signup | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ |\n| forgot-password | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ |\n| reset-password | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ |\n| home (tabs/index) | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ |\n| profile | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ |\n| health | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ |\n| add-meal | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ |\n| scan-food | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ |\n| ar-scan-food | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ |\n| ar-measure | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ |\n| health-settings | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ |\n| health/[metricType] | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ |\n| health/add | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ |\n| not-found | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ |\n| layouts | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ | ☐ |\n\n### Acceptance Criteria\n- All screens render correctly on all device categories\n- No visual bugs, overflow, or truncation\n- Forms are usable on all devices\n- iPad supports both orientations seamlessly\n- Performance remains smooth on older devices (iPhone SE)\n- Safe areas properly respected on all devices",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Responsive Design Utility Library",
            "description": "Build the foundational responsive design utilities including breakpoint definitions, device category constants, and helper functions for device detection. Create lib/responsive/breakpoints.ts with all iPhone (2020+) and iPad screen dimensions.",
            "details": "Create the following files:\n- lib/responsive/breakpoints.ts - Device breakpoint constants\n- lib/responsive/types.ts - TypeScript types for device categories\n- lib/responsive/helpers.ts - Utility functions for device detection\n\nBreakpoints to define:\n- iPhoneSE: 375x667 pts\n- iPhoneMini: 375x812 pts  \n- iPhoneMedium: 390x844 pts\n- iPhonePro: 393x852 pts\n- iPhoneMax: 430x932 pts\n- iPadMini: 744x1133 pts\n- iPad: 820x1180 pts\n- iPadPro11: 834x1194 pts\n- iPadAir13: 1032x1376 pts\n- iPadPro13: 1024x1366 pts",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 12,
            "updatedAt": "2025-12-10T16:49:10.825Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Configure Orientation Lock for iPhone/iPad",
            "description": "Set up orientation locking - portrait-only for iPhones, both orientations for iPads. Update app.json and implement runtime orientation control using expo-screen-orientation.",
            "details": "1. Update app.json with orientation: \"portrait\" and ios.supportsTablet: true\n2. Install expo-screen-orientation if not present\n3. Implement runtime detection in _layout.tsx:\n   - Lock to PORTRAIT_UP for iPhone\n   - Unlock for iPad (Platform.isPad)\n4. Test orientation behavior on both device types",
            "status": "done",
            "dependencies": [
              "12.1"
            ],
            "parentTaskId": 12,
            "parentId": "undefined",
            "updatedAt": "2025-12-10T16:51:33.416Z"
          },
          {
            "id": 3,
            "title": "Implement useResponsive Hook",
            "description": "Create a comprehensive useResponsive hook that provides device category detection, scaling functions, and responsive utilities. This hook will be the primary interface for responsive design throughout the app.",
            "details": "Create lib/hooks/useResponsive.ts with:\n- useWindowDimensions integration\n- Device category detection (small/medium/large/tablet)\n- isTablet boolean\n- isLandscape boolean  \n- scale() function for proportional sizing\n- scaleFont() for typography\n- getSpacing() for responsive margins/padding\n- Platform-aware logic for iOS/Android differences\n\nExport types and hook from lib/hooks/index.ts",
            "status": "done",
            "dependencies": [
              "12.1"
            ],
            "parentTaskId": 12,
            "parentId": "undefined",
            "updatedAt": "2025-12-10T16:53:13.680Z"
          },
          {
            "id": 4,
            "title": "Create Responsive Typography and Spacing System",
            "description": "Build a responsive typography scale and spacing system that adapts to different device sizes. Create design tokens for font sizes, line heights, and spacing values.",
            "details": "Create lib/responsive/typography.ts:\n- Base font sizes for each device category\n- Responsive font scale (xs, sm, base, lg, xl, 2xl, 3xl)\n- Line height multipliers\n- Letter spacing values\n\nCreate lib/responsive/spacing.ts:\n- Spacing scale (xs: 4, sm: 8, md: 16, lg: 24, xl: 32, 2xl: 48)\n- Responsive padding/margin helpers\n- Safe area aware spacing\n\nEnsure minimum touch targets of 44pt on all devices",
            "status": "done",
            "dependencies": [
              "12.3"
            ],
            "parentTaskId": 12,
            "parentId": "undefined",
            "updatedAt": "2025-12-10T16:55:46.367Z"
          },
          {
            "id": 5,
            "title": "Build Responsive Component Primitives",
            "description": "Create reusable responsive component wrappers that handle common responsive patterns - containers, cards, grids, and form layouts that adapt to device size.",
            "details": "Create components in lib/components/responsive/:\n- ResponsiveContainer.tsx - Max-width container with padding\n- ResponsiveGrid.tsx - Adaptive grid (1-col phone, 2-col tablet)\n- ResponsiveCard.tsx - Card with adaptive sizing\n- ResponsiveForm.tsx - Form layout wrapper\n- ResponsiveText.tsx - Text with automatic font scaling\n\nEach component should:\n- Use useResponsive hook\n- Support iPad landscape/portrait layouts\n- Handle safe areas properly\n- Be fully typed with TypeScript",
            "status": "done",
            "dependencies": [
              "12.3",
              "12.4"
            ],
            "parentTaskId": 12,
            "parentId": "undefined",
            "updatedAt": "2025-12-10T16:58:06.022Z"
          },
          {
            "id": 6,
            "title": "Update Auth Screens for Responsive Design",
            "description": "Adapt all 5 authentication screens (welcome, signin, signup, forgot-password, reset-password) to be responsive across all device categories.",
            "details": "Update these screens:\n- app/auth/welcome.tsx\n- app/auth/signin.tsx\n- app/auth/signup.tsx\n- app/auth/forgot-password.tsx\n- app/auth/reset-password.tsx\n\nFor each screen:\n1. Import and use useResponsive hook\n2. Replace hardcoded dimensions with responsive values\n3. Ensure forms have appropriate widths on tablets (max-width)\n4. Adjust padding and margins for each device category\n5. Verify text is readable on all sizes\n6. Center content appropriately on larger screens\n7. Handle keyboard avoiding behavior on all sizes",
            "status": "done",
            "dependencies": [
              "12.5"
            ],
            "parentTaskId": 12,
            "parentId": "undefined",
            "updatedAt": "2025-12-10T18:18:05.270Z"
          },
          {
            "id": 7,
            "title": "Update Main Tab Screens for Responsive Design",
            "description": "Adapt the 3 main tab screens (home/index, profile, health) and tab layout to be responsive across all device categories.",
            "details": "Update these screens:\n- app/(tabs)/index.tsx - Home dashboard\n- app/(tabs)/profile.tsx - User profile\n- app/(tabs)/health.tsx - Health overview\n- app/(tabs)/_layout.tsx - Tab navigation\n\nFocus areas:\n1. Dashboard cards should use grid on tablets\n2. Profile layout may use side-by-side on landscape iPad\n3. Health metrics should display in responsive grid\n4. Tab bar should adapt sizing for tablets\n5. Charts and graphs must scale appropriately\n6. Lists should have appropriate row heights per device",
            "status": "done",
            "dependencies": [
              "12.5"
            ],
            "parentTaskId": 12,
            "parentId": "undefined",
            "updatedAt": "2025-12-10T19:07:23.738Z"
          },
          {
            "id": 8,
            "title": "Update Meal and Scanning Screens for Responsive Design",
            "description": "Adapt the 4 meal/scanning screens (add-meal, scan-food, ar-scan-food, ar-measure) to be responsive, with special attention to camera and AR views.",
            "details": "Update these screens:\n- app/add-meal.tsx - Manual meal entry form\n- app/scan-food.tsx - Camera food scanning\n- app/ar-scan-food.tsx - AR food recognition\n- app/ar-measure.tsx - AR portion measurement\n\nSpecial considerations:\n1. Camera views must fill appropriate area on all devices\n2. AR overlays need to scale with screen size\n3. Form inputs in add-meal need responsive widths\n4. Scanning UI controls must have 44pt+ touch targets\n5. Results display should use available space on tablets\n6. Modal presentations should be appropriately sized",
            "status": "done",
            "dependencies": [
              "12.5"
            ],
            "parentTaskId": 12,
            "parentId": "undefined",
            "updatedAt": "2025-12-11T17:01:14.494Z"
          },
          {
            "id": 9,
            "title": "Update Health Screens for Responsive Design",
            "description": "Adapt the 3 health-related screens (health-settings, health/[metricType], health/add) and error screen (+not-found) to be responsive.",
            "details": "Update these screens:\n- app/health-settings.tsx - Health settings\n- app/health/[metricType].tsx - Metric detail view\n- app/health/add.tsx - Add health metric\n- app/+not-found.tsx - 404 error page\n- app/_layout.tsx - Root layout\n\nFocus areas:\n1. Settings lists should have appropriate row heights\n2. Metric charts must scale for different screen sizes\n3. Add metric form should be responsive\n4. Error page should center content on all devices\n5. Root layout should handle safe areas consistently",
            "status": "done",
            "dependencies": [
              "12.5"
            ],
            "parentTaskId": 12,
            "parentId": "undefined",
            "updatedAt": "2025-12-11T17:06:57.390Z"
          },
          {
            "id": 10,
            "title": "iPhone Simulator Testing - All Categories",
            "description": "Run comprehensive simulator testing on all iPhone device categories (SE, Mini, Medium, Pro Max) verifying all 17 screens render correctly in portrait mode.",
            "details": "Test on these simulators:\n1. iPhone SE (3rd generation) - Small/Legacy (375x667)\n2. iPhone 13 Mini - Mini category (375x812)\n3. iPhone 14 - Medium category (390x844)\n4. iPhone 15 Pro - Pro category (393x852)\n5. iPhone 15 Pro Max - Max category (430x932)\n\nFor each device, verify ALL 17 screens:\n- Auth: welcome, signin, signup, forgot-password, reset-password\n- Tabs: home, profile, health\n- Meals: add-meal, scan-food, ar-scan-food, ar-measure\n- Health: health-settings, [metricType], add\n- System: not-found, layouts\n\nChecklist per screen:\n☐ No horizontal overflow\n☐ Text readable, no truncation\n☐ Touch targets >= 44pt\n☐ Safe areas respected\n☐ Forms usable with keyboard",
            "status": "done",
            "dependencies": [
              "12.6",
              "12.7",
              "12.8",
              "12.9"
            ],
            "parentTaskId": 12,
            "parentId": "undefined",
            "updatedAt": "2025-12-11T17:15:01.128Z"
          },
          {
            "id": 11,
            "title": "iPad Simulator Testing - Both Orientations",
            "description": "Run comprehensive simulator testing on iPad devices (Mini, Air, Pro) in both portrait and landscape orientations, verifying all 17 screens adapt correctly.",
            "details": "Test on these simulators:\n1. iPad Mini (6th generation) - Portrait & Landscape\n2. iPad Air 11-inch - Portrait & Landscape\n3. iPad Pro 11-inch - Portrait & Landscape\n4. iPad Pro 13-inch - Portrait & Landscape\n\nFor each device AND orientation, verify ALL 17 screens:\n- Auth: welcome, signin, signup, forgot-password, reset-password\n- Tabs: home, profile, health\n- Meals: add-meal, scan-food, ar-scan-food, ar-measure\n- Health: health-settings, [metricType], add\n- System: not-found, layouts\n\nChecklist per screen:\n☐ Layout adapts to orientation change\n☐ No content loss on rotation\n☐ Grids display correctly (multi-column where appropriate)\n☐ Forms centered with max-width\n☐ Charts scale appropriately\n☐ Touch targets >= 44pt\n☐ Smooth rotation animation",
            "status": "done",
            "dependencies": [
              "12.6",
              "12.7",
              "12.8",
              "12.9"
            ],
            "parentTaskId": 12,
            "parentId": "undefined",
            "updatedAt": "2025-12-11T17:16:45.237Z"
          },
          {
            "id": 12,
            "title": "Create Testing Documentation and Verification Report",
            "description": "Document all responsive design testing results, create a verification matrix with screenshots, and compile final report of any issues found and resolutions.",
            "details": "Create documentation in docs/responsive-design/:\n1. TESTING-MATRIX.md - Device x Screen verification grid\n2. BREAKPOINTS.md - Documentation of breakpoint system\n3. SCREENSHOTS/ - Folder with screenshots from each device\n4. ISSUES.md - Log of issues found and how resolved\n5. USAGE-GUIDE.md - How to use responsive utilities\n\nFinal verification matrix should show:\n| Screen | SE | Mini | Med | Max | iPad-P | iPad-L |\nWith ✓/✗ for each combination\n\nInclude simulator commands for future testing:\n- How to boot each simulator\n- How to run app on each device\n- How to take screenshots",
            "status": "done",
            "dependencies": [
              "12.10",
              "12.11"
            ],
            "parentTaskId": 12,
            "parentId": "undefined",
            "updatedAt": "2025-12-11T17:18:50.485Z"
          }
        ],
        "updatedAt": "2025-12-11T17:18:50.485Z"
      },
      {
        "id": 13,
        "title": "Integrate USDA FoodData Central Database with Scalable Food Classification System",
        "description": "Integrate the USDA FoodData Central API to expand the food database from ~100 items to 500K+ foods, AND implement a scalable multi-tier food classification architecture. This addresses the critical question: yes, integrating USDA's 500K+ food database requires a fundamentally different classification approach than the current ~100-class model.",
        "details": "## Overview\nThe current food database in `ml-service/app/data/food_database.py` has approximately 100 items with a simple classifier (`FoodAnalysisService._classify_food`). Integrating USDA FoodData Central's 500K+ foods requires a **multi-tier classification architecture** because:\n\n1. **No ML model can reliably classify 500K+ food classes** - even state-of-the-art research focuses on 500-2000 classes\n2. **USDA uses 5 distinct data types** with different classification schemas (Foundation, SR Legacy, Survey/FNDDS, Branded, Experimental)\n3. **User search + AI-assisted refinement** is more practical than pure image classification\n\n## USDA FoodData Central API\n- **API Endpoint**: https://api.nal.usda.gov/fdc/v1/\n- **API Key**: Free, requires registration at https://fdc.nal.usda.gov/api-key-signup.html\n- **Rate Limits**: 1,000 requests/hour per IP (can request increase)\n- **Data Types**: \n  - Foundation Foods (unprocessed/lightly processed)\n  - SR Legacy (comprehensive, final release 2018)\n  - Survey/FNDDS (dietary studies 2021-2023)\n  - Branded (commercial products)\n  - Experimental (research data)\n\n## Multi-Tier Classification Architecture\n\n### Tier 1: Coarse-Grained Visual Classifier (ML Model)\n**Purpose**: Classify images into 20-50 high-level food categories\n**Implementation**: `ml-service/app/ml_models/food_classifier_v3.py`\n\n```python\nclass FoodCategory(str, Enum):\n    # 25-30 categories mapping to USDA food groups\n    FRUITS_FRESH = \"fruits_fresh\"\n    FRUITS_PROCESSED = \"fruits_processed\"\n    VEGETABLES_LEAFY = \"vegetables_leafy\"\n    VEGETABLES_ROOT = \"vegetables_root\"\n    VEGETABLES_OTHER = \"vegetables_other\"\n    MEAT_RED = \"meat_red\"\n    MEAT_POULTRY = \"meat_poultry\"\n    SEAFOOD_FISH = \"seafood_fish\"\n    SEAFOOD_SHELLFISH = \"seafood_shellfish\"\n    DAIRY_MILK = \"dairy_milk\"\n    DAIRY_CHEESE = \"dairy_cheese\"\n    DAIRY_YOGURT = \"dairy_yogurt\"\n    GRAINS_BREAD = \"grains_bread\"\n    GRAINS_PASTA = \"grains_pasta\"\n    GRAINS_RICE = \"grains_rice\"\n    GRAINS_CEREAL = \"grains_cereal\"\n    LEGUMES = \"legumes\"\n    NUTS_SEEDS = \"nuts_seeds\"\n    BEVERAGES_HOT = \"beverages_hot\"\n    BEVERAGES_COLD = \"beverages_cold\"\n    SNACKS_SWEET = \"snacks_sweet\"\n    SNACKS_SAVORY = \"snacks_savory\"\n    MIXED_DISHES = \"mixed_dishes\"\n    FAST_FOOD = \"fast_food\"\n    CONDIMENTS_SAUCES = \"condiments_sauces\"\n    # ... additional categories\n\n@dataclass\nclass CoarseClassification:\n    category: FoodCategory\n    confidence: float\n    subcategory_hints: List[str]  # \"appears sliced\", \"grilled texture\", etc.\n    color_profile: Dict[str, float]  # dominant colors for refinement\n    texture_features: Dict[str, float]  # smooth, grainy, fibrous, etc.\n```\n\n**Model Architecture**:\n- Base: EfficientNet-B4 or ConvNeXt-Base (pretrained on ImageNet)\n- Fine-tuned on Food-2K + custom dataset\n- Output: Top-3 categories with confidence scores\n- Inference time: <100ms on CPU, <20ms on GPU\n\n### Tier 2: Category-Specific Fine-Grained Classifier (Optional ML)\n**Purpose**: Refine within categories (e.g., \"apple\" vs \"pear\" within FRUITS_FRESH)\n**Implementation**: Specialized sub-models loaded on-demand\n\n```python\nclass FinegrainedClassifierRegistry:\n    \"\"\"Registry of category-specific classifiers\"\"\"\n    \n    CLASSIFIERS = {\n        FoodCategory.FRUITS_FRESH: \"models/fruits_classifier_v1.onnx\",\n        FoodCategory.MEAT_RED: \"models/red_meat_classifier_v1.onnx\",\n        FoodCategory.MIXED_DISHES: None,  # Too complex, skip to search\n        # ...\n    }\n    \n    async def classify_finegrained(\n        self, \n        image: Image.Image, \n        category: FoodCategory\n    ) -> Optional[List[FinegrainedPrediction]]:\n        \"\"\"Returns None if no specialized classifier available\"\"\"\n        model_path = self.CLASSIFIERS.get(category)\n        if not model_path:\n            return None\n        # Load and run category-specific model\n        ...\n```\n\n**Category Coverage**:\n- Fruits: ~200 classes (achievable with 85%+ accuracy)\n- Vegetables: ~150 classes\n- Meat/Poultry: ~100 classes\n- Seafood: ~150 classes\n- Branded/Processed: Skip (use text search)\n- Mixed dishes: Skip (use user input + search)\n\n### Tier 3: USDA Search Integration (Primary Lookup)\n**Purpose**: Map classifications to specific USDA FDC entries\n**Implementation**: `server/src/services/foodDatabaseService.ts`\n\n```typescript\ninterface USDASearchStrategy {\n  // Combine visual classification with text search\n  searchWithClassificationContext(\n    query: string,\n    classificationHints: ClassificationHints\n  ): Promise<USDASearchResult[]>;\n}\n\ninterface ClassificationHints {\n  coarseCategory: string;\n  finegrainedSuggestions?: string[];\n  colorProfile?: Record<string, number>;\n  cookingMethod?: string;\n  brandDetected?: string;  // OCR from packaging\n  portionEstimate?: number;  // grams from AR\n}\n\nconst searchWithContext = async (\n  userQuery: string,\n  hints: ClassificationHints\n): Promise<USDAFoodItem[]> => {\n  // 1. Build enhanced search query\n  const enhancedQuery = buildEnhancedQuery(userQuery, hints);\n  \n  // 2. Filter by USDA data types based on category\n  const dataTypes = getRelevantDataTypes(hints.coarseCategory);\n  // Foundation/SR Legacy for whole foods\n  // Branded for packaged foods\n  // Survey for mixed dishes\n  \n  // 3. Execute search with USDA API\n  const results = await usdaApi.search({\n    query: enhancedQuery,\n    dataType: dataTypes,\n    pageSize: 25,\n    sortBy: 'dataType.keyword',  // Prefer Foundation over Branded\n  });\n  \n  // 4. Re-rank results using classification hints\n  return rerankResults(results, hints);\n};\n```\n\n### Tier 4: User Confirmation + Learning Loop\n**Purpose**: Correct misclassifications and improve over time\n**Implementation**: Feedback-driven learning system\n\n```typescript\n// server/src/services/foodFeedbackService.ts\ninterface FoodFeedback {\n  originalPrediction: string;\n  userSelection: string;  // FDC ID selected\n  imageHash: string;      // For deduplication\n  classificationHints: ClassificationHints;\n  timestamp: Date;\n  userId: string;\n}\n\n// Aggregate feedback for model retraining triggers\nconst aggregateFeedback = async (): Promise<RetrainingSignal> => {\n  // When >100 corrections for a category, signal retraining\n  ...\n};\n```\n\n## Backend API Routes (Extended)\n\n### Food Search Endpoints\n```\nGET /api/foods/search?q={query}&limit={limit}&page={page}&dataType={type}\nGET /api/foods/:fdcId\nGET /api/foods/:fdcId/nutrients\nGET /api/foods/popular\nGET /api/foods/recent  (user's recent selections)\n```\n\n### Classification-Assisted Endpoints\n```\nPOST /api/foods/classify-and-search\n  Body: { image: base64, dimensions?: ARDimensions }\n  Response: {\n    classification: { category, confidence, suggestions },\n    searchResults: USDAFoodItem[],\n    portionEstimate?: number\n  }\n\nPOST /api/foods/feedback\n  Body: { classificationId, selectedFdcId, wasCorrect }\n```\n\n## Caching Strategy (Multi-Layer)\n\n### Layer 1: Edge Cache (CDN)\n- Popular food queries: 24-hour TTL\n- Static food data: 7-day TTL\n\n### Layer 2: Redis Cache\n```typescript\n// Search results: 1-hour TTL\nawait redis.setex(`search:${hash(query+dataTypes)}`, 3600, JSON.stringify(results));\n\n// Individual food data: 24-hour TTL\nawait redis.setex(`food:${fdcId}`, 86400, JSON.stringify(foodData));\n\n// User's recent foods: 30-day TTL\nawait redis.zadd(`user:${userId}:foods`, timestamp, fdcId);\n\n// Classification results: 1-hour TTL (for same image)\nawait redis.setex(`classify:${imageHash}`, 3600, JSON.stringify(classification));\n```\n\n### Layer 3: Local SQLite Cache (Mobile)\n```typescript\n// Cache top 10K most searched foods locally\n// Weekly sync for updates\n// Enables offline search with degraded ranking\n```\n\n## Nutrient Mapping (Extended)\n\nMap USDA nutrient IDs to our schema (expanded set):\n```typescript\nconst NUTRIENT_MAPPING = {\n  // Core macros\n  1003: 'protein',      // g\n  1004: 'fat',          // g\n  1005: 'carbs',        // g (by difference)\n  1008: 'calories',     // kcal\n  \n  // Fiber & sugars\n  1079: 'fiber',        // g\n  2000: 'sugars_total', // g\n  1235: 'sugars_added', // g\n  \n  // Fats breakdown\n  1258: 'saturated_fat',     // g\n  1292: 'monounsaturated_fat', // g\n  1293: 'polyunsaturated_fat', // g\n  1257: 'trans_fat',         // g\n  1253: 'cholesterol',       // mg\n  \n  // Minerals\n  1093: 'sodium',       // mg\n  1092: 'potassium',    // mg\n  1087: 'calcium',      // mg\n  1089: 'iron',         // mg\n  1090: 'magnesium',    // mg\n  \n  // Vitamins\n  1106: 'vitamin_a',    // mcg RAE\n  1162: 'vitamin_c',    // mg\n  1114: 'vitamin_d',    // mcg\n  \n  // Amino acids (for Lysine/Arginine tracking)\n  1213: 'lysine',       // g\n  1220: 'arginine',     // g\n};\n```\n\n## ML Model Training Pipeline\n\n### Phase 1: Coarse Classifier Training\n```python\n# ml-service/scripts/train_coarse_classifier.py\n\n# Dataset: Food-2K + custom images\n# Classes: 25-30 food categories\n# Architecture: EfficientNet-B4\n# Training: 50 epochs, cosine LR schedule\n# Target: >90% top-1 accuracy, >98% top-3 accuracy\n\nfrom torchvision.models import efficientnet_b4\nfrom torch.utils.data import DataLoader\n\ndef train_coarse_classifier():\n    model = efficientnet_b4(pretrained=True)\n    model.classifier[-1] = nn.Linear(1792, NUM_CATEGORIES)\n    \n    # Training config\n    optimizer = AdamW(model.parameters(), lr=1e-4)\n    scheduler = CosineAnnealingLR(optimizer, T_max=50)\n    \n    # Mixed precision training\n    scaler = GradScaler()\n    \n    for epoch in range(50):\n        train_epoch(model, train_loader, optimizer, scaler)\n        validate(model, val_loader)\n        scheduler.step()\n```\n\n### Phase 2: Fine-Grained Classifier Training (Per Category)\n```python\n# Only for categories with clear visual distinctions\n# Skip: mixed dishes, branded foods, beverages\n\nTRAINABLE_CATEGORIES = [\n    'fruits_fresh',    # ~200 classes\n    'vegetables',      # ~150 classes\n    'meat_raw',        # ~50 classes\n    'seafood',         # ~150 classes\n]\n\ndef train_finegrained_classifier(category: str):\n    # Filter USDA FDC images for category\n    # Use Foundation Foods data type for quality images\n    # Augmentation: color jitter, rotation, scale\n    # Model: ResNet-50 or ConvNeXt-Small\n    ...\n```\n\n### Phase 3: Continuous Learning Pipeline\n```python\n# Trigger retraining when:\n# 1. >100 corrections for a category\n# 2. Monthly scheduled retraining\n# 3. New USDA data release\n\n@celery.task\ndef check_retraining_triggers():\n    feedback_stats = aggregate_user_feedback()\n    for category, stats in feedback_stats.items():\n        if stats.correction_count > 100:\n            queue_retraining_job(category, stats.feedback_data)\n```\n\n## Environment Variables (Extended)\n```env\n# USDA API\nUSDA_API_KEY=your-api-key-here\nUSDA_API_BASE_URL=https://api.nal.usda.gov/fdc/v1\nUSDA_RATE_LIMIT_PER_HOUR=1000\n\n# ML Model Configuration\nML_MODEL_COARSE_PATH=models/food_coarse_v1.onnx\nML_MODEL_INFERENCE_DEVICE=cpu  # or cuda\nML_MODEL_CONFIDENCE_THRESHOLD=0.6\n\n# Caching\nREDIS_URL=redis://localhost:6379\nCACHE_SEARCH_TTL_SECONDS=3600\nCACHE_FOOD_TTL_SECONDS=86400\n\n# Feature Flags\nENABLE_FINEGRAINED_CLASSIFICATION=true\nENABLE_BRANDED_FOOD_SEARCH=true\nENABLE_FEEDBACK_COLLECTION=true\n```\n\n## Feasibility Analysis\n\n### What IS Feasible:\n1. ✅ Integrating USDA API for 500K+ food search\n2. ✅ Coarse classification into 25-50 categories (>90% accuracy achievable)\n3. ✅ Fine-grained classification for select categories (fruits, vegetables, meat)\n4. ✅ Hybrid search combining visual hints + text queries\n5. ✅ Progressive enhancement with user feedback\n\n### What is NOT Feasible:\n1. ❌ Direct 500K-class image classifier (no model can do this reliably)\n2. ❌ Accurate classification of branded/packaged foods (need barcode/OCR)\n3. ❌ Mixed dishes classification without user input\n4. ❌ Real-time model retraining (must be batch/scheduled)\n\n### Recommended Approach:\n**Hybrid Search-First Architecture**\n- Use ML for coarse categorization + portion estimation\n- Let user search/select from USDA results\n- Learn from corrections to improve suggestions\n- Barcode scanning for packaged foods (Phase 2)\n\n## Success Metrics (Extended)\n- Coarse classification: >90% top-1, >98% top-3 accuracy\n- Search returns results in <500ms (with caching)\n- 95%+ of common foods found in search\n- User selects from top-5 suggestions >80% of time\n- Nutrition data accuracy verified against USDA source\n- Model retraining pipeline completes in <4 hours",
        "testStrategy": "## Testing Strategy (Comprehensive)\n\n### Unit Tests\n\n#### ML Classification Tests\n```python\n# ml-service/tests/test_coarse_classifier.py\nclass TestCoarseClassifier:\n    def test_classification_output_shape(self):\n        \"\"\"Verify model outputs correct number of categories\"\"\"\n        \n    def test_confidence_scores_sum_to_one(self):\n        \"\"\"Softmax outputs should sum to ~1.0\"\"\"\n        \n    def test_inference_time_under_threshold(self):\n        \"\"\"Inference should complete in <100ms CPU\"\"\"\n        \n    def test_batch_inference(self):\n        \"\"\"Multiple images processed correctly\"\"\"\n        \n    def test_model_handles_various_image_sizes(self):\n        \"\"\"Resizing/preprocessing works for any input size\"\"\"\n        \n    def test_model_handles_grayscale_images(self):\n        \"\"\"Graceful handling of non-RGB input\"\"\"\n```\n\n#### USDA API Client Tests\n```typescript\n// server/src/__tests__/foodDatabaseService.test.ts\ndescribe('FoodDatabaseService', () => {\n  test('searchFoods returns properly typed results');\n  test('searchFoods handles empty query gracefully');\n  test('searchFoods respects dataType filter');\n  test('searchFoods paginates correctly');\n  test('getFoodById returns complete nutrition data');\n  test('getFoodById handles non-existent FDC ID');\n  test('nutrient mapping transforms USDA format to app schema');\n  test('nutrient mapping handles missing nutrients gracefully');\n  test('rate limit handling backs off appropriately');\n});\n```\n\n#### Caching Tests\n```typescript\ndescribe('FoodCacheService', () => {\n  test('cache hit returns data without API call');\n  test('cache miss fetches from API and caches');\n  test('cache expiration triggers fresh fetch');\n  test('cache handles Redis connection failure gracefully');\n  test('search result deduplication works correctly');\n});\n```\n\n### Integration Tests\n\n#### Classification Pipeline Tests\n```python\n# ml-service/tests/integration/test_classification_pipeline.py\nclass TestClassificationPipeline:\n    async def test_end_to_end_classification(self):\n        \"\"\"Image → Coarse → Fine-grained → Search suggestions\"\"\"\n        \n    async def test_classification_with_ar_dimensions(self):\n        \"\"\"Classification combined with portion estimation\"\"\"\n        \n    async def test_fallback_when_classifier_unavailable(self):\n        \"\"\"System degrades gracefully without ML model\"\"\"\n        \n    async def test_classification_caching(self):\n        \"\"\"Same image hash returns cached result\"\"\"\n```\n\n#### USDA API Integration Tests\n```typescript\n// Run with actual API (rate-limited test account)\ndescribe('USDA API Integration', () => {\n  test('search for \"apple\" returns Foundation Foods results');\n  test('search for \"coca cola\" returns Branded results');\n  test('getFoodById retrieves full nutrient profile');\n  test('API handles special characters in query');\n  test('API timeout is handled gracefully');\n  test('concurrent requests respect rate limits');\n});\n```\n\n#### Hybrid Search Tests\n```typescript\ndescribe('Hybrid Search', () => {\n  test('classification hints improve search ranking');\n  test('color profile helps distinguish similar foods');\n  test('cooking method hint filters appropriate results');\n  test('portion estimate affects serving size suggestions');\n});\n```\n\n### Mobile Tests\n\n#### Search UI Tests\n```typescript\n// __tests__/screens/FoodSearchScreen.test.tsx\ndescribe('FoodSearchScreen', () => {\n  test('renders search input and results list');\n  test('debounces search input (300ms)');\n  test('displays loading state during search');\n  test('displays error state on API failure');\n  test('displays empty state with suggestions');\n  test('tapping result opens food detail');\n  test('recent searches displayed on focus');\n  test('clear search button works');\n});\n```\n\n#### Classification Integration Tests\n```typescript\ndescribe('FoodClassificationScreen', () => {\n  test('camera capture triggers classification');\n  test('classification results displayed with confidence');\n  test('user can override classification');\n  test('AR dimensions captured when available');\n  test('portion estimate displayed');\n  test('proceed to search with classification hints');\n});\n```\n\n#### Offline Behavior Tests\n```typescript\ndescribe('Offline Mode', () => {\n  test('cached foods searchable offline');\n  test('recent selections available offline');\n  test('graceful degradation message shown');\n  test('classification works offline (model in app)');\n  test('sync queue for pending feedback');\n});\n```\n\n### Performance Tests\n\n#### Classification Performance\n```python\ndef test_coarse_classifier_latency():\n    \"\"\"Target: <100ms on CPU, <20ms on GPU\"\"\"\n    model = load_coarse_classifier()\n    images = load_test_images(100)\n    \n    start = time.time()\n    for img in images:\n        model.predict(img)\n    avg_latency = (time.time() - start) / 100\n    \n    assert avg_latency < 0.1  # 100ms\n\ndef test_finegrained_classifier_latency():\n    \"\"\"Target: <150ms on CPU\"\"\"\n    ...\n```\n\n#### Search Performance\n```typescript\ndescribe('Search Performance', () => {\n  test('cached search returns in <50ms');\n  test('uncached search returns in <500ms');\n  test('concurrent searches (10) complete in <2s');\n  test('large result sets (100 items) render in <100ms');\n});\n```\n\n#### Memory Tests\n```python\ndef test_model_memory_footprint():\n    \"\"\"Coarse model should use <500MB RAM\"\"\"\n    import tracemalloc\n    tracemalloc.start()\n    \n    model = load_coarse_classifier()\n    current, peak = tracemalloc.get_traced_memory()\n    \n    assert peak < 500 * 1024 * 1024  # 500MB\n```\n\n### Accuracy Tests\n\n#### Classification Accuracy\n```python\n# Run on held-out test set\ndef test_coarse_classifier_accuracy():\n    \"\"\"Target: >90% top-1, >98% top-3\"\"\"\n    model = load_coarse_classifier()\n    test_set = load_test_dataset()\n    \n    top1_correct = 0\n    top3_correct = 0\n    \n    for image, label in test_set:\n        predictions = model.predict_top_k(image, k=3)\n        if predictions[0].label == label:\n            top1_correct += 1\n        if label in [p.label for p in predictions]:\n            top3_correct += 1\n    \n    top1_acc = top1_correct / len(test_set)\n    top3_acc = top3_correct / len(test_set)\n    \n    assert top1_acc > 0.90\n    assert top3_acc > 0.98\n\ndef test_per_category_accuracy():\n    \"\"\"Ensure no category has <80% accuracy\"\"\"\n    ...\n```\n\n#### Nutrition Data Accuracy\n```typescript\ndescribe('Nutrition Data Accuracy', () => {\n  test('calories within 5% of USDA source');\n  test('macros within 5% of USDA source');\n  test('portion scaling maintains ratios');\n  test('cooking method adjustments reasonable');\n});\n```\n\n### Edge Case Tests\n\n#### Classification Edge Cases\n```python\ndef test_multiple_foods_in_image():\n    \"\"\"Should return primary classification or multi-food flag\"\"\"\n    \ndef test_partially_eaten_food():\n    \"\"\"Should still classify correctly\"\"\"\n    \ndef test_food_in_packaging():\n    \"\"\"Should suggest barcode scanning\"\"\"\n    \ndef test_non_food_image():\n    \"\"\"Should return low confidence / 'unknown'\"\"\"\n    \ndef test_blurry_image():\n    \"\"\"Should request better image or proceed with caution\"\"\"\n```\n\n#### Search Edge Cases\n```typescript\ndescribe('Search Edge Cases', () => {\n  test('handles Unicode characters (日本語, émoji)');\n  test('handles very long queries (>200 chars)');\n  test('handles SQL injection attempts');\n  test('handles empty/whitespace-only queries');\n  test('handles queries with only special characters');\n});\n```\n\n#### API Failure Edge Cases\n```typescript\ndescribe('API Resilience', () => {\n  test('handles USDA API timeout');\n  test('handles USDA API 500 errors');\n  test('handles rate limit exceeded (429)');\n  test('handles malformed USDA response');\n  test('circuit breaker activates after repeated failures');\n  test('fallback to cached data when API unavailable');\n});\n```\n\n### Feedback Loop Tests\n\n```typescript\ndescribe('Feedback Collection', () => {\n  test('feedback submitted successfully');\n  test('feedback deduplication works');\n  test('feedback aggregation triggers retraining signal');\n  test('feedback privacy (no PII stored)');\n});\n```\n\n### Load Tests\n\n```typescript\ndescribe('Load Testing', () => {\n  test('100 concurrent searches complete in <5s');\n  test('1000 searches/minute sustained without errors');\n  test('cache hit ratio >80% after warmup');\n  test('memory usage stable under load');\n});\n```\n\n### Acceptance Criteria\n\n#### Core Functionality\n- [ ] Search returns relevant results for common foods\n- [ ] Classification correctly categorizes >90% of test images\n- [ ] Nutrition data matches USDA source within 5%\n- [ ] Performance meets latency targets (search <500ms, classify <100ms)\n\n#### User Experience\n- [ ] User selects from top-5 suggestions >80% of time\n- [ ] Search autocomplete feels responsive (<300ms)\n- [ ] Classification confidence displayed helpfully\n- [ ] Graceful degradation when services unavailable\n\n#### System Reliability\n- [ ] Circuit breaker prevents cascade failures\n- [ ] Rate limiting prevents USDA API abuse\n- [ ] Caching reduces API calls by >80%\n- [ ] Offline mode provides useful functionality\n\n#### Data Quality\n- [ ] Nutrient mapping covers all essential nutrients\n- [ ] Serving size conversions accurate\n- [ ] Cooking method adjustments reasonable\n- [ ] Feedback loop improves suggestions over time",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "Implement Barcode Scanner with Open Food Facts Integration",
        "description": "Add barcode scanning capability to the food logging flow using the device camera and integrate with Open Food Facts API for instant nutrition lookup. This addresses a critical competitive gap - MyFitnessPal's barcode scanner is one of their most-used features.",
        "details": "## Overview\nBarcode scanning is the #1 most requested feature for calorie tracking apps. Users expect instant food lookup by simply pointing their camera at a product barcode.\n\n## Technical Implementation\n\n### 1. Camera Barcode Scanner\n- Use `expo-camera` with barcode scanning enabled\n- Support EAN-13, EAN-8, UPC-A, UPC-E formats (standard food barcodes)\n- Implement scanning UI with viewfinder overlay and haptic feedback\n- Handle low-light conditions with torch toggle\n\n### 2. Open Food Facts API Integration\n- API endpoint: https://world.openfoodfacts.org/api/v0/product/{barcode}.json\n- Free, open-source database with 2M+ products\n- No API key required (rate limit: be respectful)\n- Implement caching layer to reduce API calls\n- Store successfully scanned products locally for offline access\n\n### 3. Data Mapping\nMap Open Food Facts response to Nutri's nutrition schema:\n- nutriments.energy-kcal_100g → calories (per 100g)\n- nutriments.proteins_100g → protein\n- nutriments.carbohydrates_100g → carbs\n- nutriments.fat_100g → fat\n- nutriments.fiber_100g → fiber\n- nutriments.sugars_100g → sugar\n- nutriments.sodium_100g → sodium\n- serving_size → portion reference\n\n### 4. UI/UX Flow\n1. User taps barcode icon in add-meal screen\n2. Camera opens with scanning viewfinder\n3. Barcode detected → API lookup\n4. Results shown: product name, image, nutrition per serving\n5. User can adjust serving size\n6. One-tap to add to meal log\n\n### 5. Fallback Handling\n- Product not found: Offer manual entry or AI food scanner\n- Network error: Check local cache first\n- Invalid barcode: Show helpful error message\n\n### 6. Files to Create/Modify\n- `app/scan-barcode.tsx` - New barcode scanner screen\n- `lib/api/openfoodfacts.ts` - API client\n- `lib/types/barcode.ts` - Type definitions\n- `app/add-meal.tsx` - Add barcode scanner button\n- `server/src/routes/foods.ts` - Cache endpoint (optional)\n\n## Success Metrics\n- Scanner accuracy: >95% successful reads\n- API hit rate: >80% products found\n- User adoption: 50%+ of food logs use barcode\n\n## Dependencies\n- expo-camera (already installed)\n- expo-haptics (for feedback)",
        "testStrategy": "1. Unit tests for Open Food Facts API client (mock responses)\n2. Unit tests for nutrition data mapping\n3. Integration tests for barcode detection\n4. E2E test: scan sample barcode → verify nutrition displayed\n5. Test offline caching behavior\n6. Test fallback flows (product not found, network error)",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-11T18:26:51.696Z"
      },
      {
        "id": 15,
        "title": "Build What-If Simulation Engine for Nutrition Impact Prediction",
        "description": "Create an interactive simulation feature that allows users to see predicted health impacts of potential nutrition changes before making them. This is a unique differentiator leveraging our ML correlation engine that no competitor offers.",
        "details": "## Overview\nWhat-If Simulation answers: \"If I change my diet in X way, how will it affect my health metrics?\" This transforms our ML predictions from passive insights to actionable planning tools.\n\n## Technical Implementation\n\n### 1. Simulation Engine (ML Service)\nCreate new service: `ml-service/app/services/simulation_engine.py`\n\nCore functionality:\n- Accept hypothetical nutrition changes (e.g., +20g protein daily)\n- Use trained LSTM models to predict health metric changes\n- Calculate confidence intervals for predictions\n- Return time-series projections (7-day, 14-day, 30-day)\n\n### 2. Simulation API Endpoints\n```python\nPOST /api/v1/simulate/nutrition-change\n{\n  \"user_id\": \"uuid\",\n  \"changes\": [\n    {\"nutrient\": \"protein\", \"delta\": 20, \"unit\": \"g\"},\n    {\"nutrient\": \"sugar\", \"delta\": -15, \"unit\": \"g\"}\n  ],\n  \"duration_days\": 14,\n  \"metrics_to_predict\": [\"rhr\", \"hrv_rmssd\", \"recovery_score\"]\n}\n\nResponse:\n{\n  \"predictions\": [\n    {\n      \"metric\": \"rhr\",\n      \"baseline\": 62,\n      \"projected\": 59,\n      \"confidence_interval\": [57, 61],\n      \"trajectory\": [62, 61, 61, 60, 60, 59, 59, ...]\n    }\n  ],\n  \"confidence_score\": 0.78,\n  \"based_on_correlations\": [...]\n}\n```\n\n### 3. Mobile UI Components\n- `app/simulate.tsx` - Main simulation screen\n- Slider controls for nutrition adjustments\n- Real-time prediction visualization\n- Before/after comparison charts\n- Save simulation as \"goal\" feature\n\n### 4. Visualization\n- Line chart showing projected metric trajectory\n- Confidence band visualization (shaded area)\n- Color coding: green (improvement), red (concern), gray (neutral)\n- Comparison overlay with current baseline\n\n### 5. Leveraging Existing ML\nBuild on `correlation_engine.py`:\n- Use discovered correlations to weight predictions\n- Apply time-lag findings (e.g., protein affects HRV after 48h)\n- Use per-user trained models for personalized predictions\n\n### 6. Safety Guardrails\n- Warn if simulated changes are extreme (>50% change)\n- Note that predictions are estimates, not medical advice\n- Cap prediction confidence for users with <30 days data\n\n## Success Metrics\n- Simulation accuracy: predictions within 15% of actual outcomes\n- User engagement: 30%+ users try simulation feature\n- Goal conversion: 20%+ of simulations saved as goals\n\n## Dependencies\n- Task 13: USDA database (for accurate nutrition data)\n- Existing correlation engine and LSTM models",
        "testStrategy": "1. Unit tests for simulation engine calculations\n2. Mock prediction model tests\n3. Integration tests for full simulation pipeline\n4. Validation: run simulations on historical data, compare to actual outcomes\n5. E2E test: create simulation → verify predictions displayed\n6. Test edge cases (insufficient data, extreme values)",
        "status": "pending",
        "dependencies": [
          "13"
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 16,
        "title": "Integrate Continuous Glucose Monitor (CGM) Data Sources",
        "description": "Add real-time glucose monitoring integration with major CGM platforms (Dexcom, Abbott Libre, Levels). This enables groundbreaking meal-specific glucose response tracking and positions Nutri as the premier nutrition-metabolic health app.",
        "details": "## Overview\nCGM integration is the next frontier in personalized nutrition. By correlating individual meal consumption with glucose response curves, we can provide unprecedented insights into how specific foods affect each user.\n\n## Technical Implementation\n\n### 1. CGM Platform Integrations\n\n#### Dexcom API\n- OAuth 2.0 authentication\n- Endpoints: /egvs (estimated glucose values)\n- 5-minute reading intervals\n- Developer portal: https://developer.dexcom.com/\n\n#### Abbott LibreView API\n- OAuth 2.0 authentication\n- Historical glucose data access\n- Developer access via partnership application\n\n#### Levels Health API (for Levels users)\n- REST API with API key\n- Enriched glucose data with metabolic scores\n- Partnership integration\n\n### 2. Data Models\n\nNew Prisma models:\n```prisma\nmodel GlucoseReading {\n  id          String   @id @default(uuid())\n  userId      String\n  value       Float    // mg/dL\n  source      GlucoseSource\n  recordedAt  DateTime\n  user        User     @relation(fields: [userId], references: [id])\n  \n  @@index([userId, recordedAt])\n}\n\nenum GlucoseSource {\n  DEXCOM\n  LIBRE\n  LEVELS\n  MANUAL\n}\n\nmodel MealGlucoseResponse {\n  id              String   @id @default(uuid())\n  mealId          String   @unique\n  meal            Meal     @relation(fields: [mealId], references: [id])\n  baselineGlucose Float\n  peakGlucose     Float\n  peakTime        Int      // minutes after meal\n  returnToBaseline Int     // minutes\n  areaUnderCurve  Float\n  glucoseScore    Float    // 0-100 metabolic response score\n}\n```\n\n### 3. Meal-Glucose Correlation\n\nNew ML service: `ml-service/app/services/glucose_analysis.py`\n\nFeatures:\n- Detect meal timing from glucose spikes\n- Calculate glucose response metrics per meal\n- Identify problematic foods for individual users\n- Learn personal glycemic index adjustments\n\n### 4. API Endpoints\n\n```\nPOST /api/v1/integrations/cgm/connect\nPOST /api/v1/integrations/cgm/sync\nGET /api/v1/glucose/readings?from=&to=\nGET /api/v1/glucose/meal-response/:mealId\nGET /api/v1/glucose/insights\n```\n\n### 5. Mobile UI\n\n- `app/settings/cgm-connect.tsx` - OAuth connection flow\n- `app/(tabs)/glucose.tsx` - Glucose dashboard tab (optional)\n- Glucose response card on meal detail view\n- Real-time glucose widget on home screen\n- Glucose trend visualization\n\n### 6. Privacy & Security\n\n- Encrypted storage of CGM credentials\n- User consent flow for data access\n- Data retention policies\n- HIPAA considerations for health data\n\n## Success Metrics\n- Integration success rate: >90% successful connections\n- Data sync reliability: >99% uptime\n- User value: 80%+ of CGM users find insights valuable\n\n## Dependencies\n- Backend OAuth infrastructure\n- Secure credential storage\n- Existing meal logging system",
        "testStrategy": "1. Unit tests for glucose data processing\n2. Mock OAuth flow tests\n3. Integration tests for each CGM provider API\n4. Test meal-glucose correlation calculations\n5. E2E test: connect CGM → sync data → view meal response\n6. Security audit for credential handling\n7. Test reconnection and token refresh flows",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 17,
        "title": "Design Hierarchical Food Recognition Architecture",
        "description": "Document and implement the hierarchical food recognition architecture where the image classifier outputs food CATEGORIES (300-500 classes) that serve as search queries for USDA FoodData Central, NOT direct 500K-class classification which is infeasible.",
        "details": "## Architecture Decision Record\n\n### Problem Statement\nUSDA FoodData Central contains 500K+ foods. Should we train a classifier to recognize all of them?\n\n### Decision: NO - Use Hierarchical Architecture\n\n### Rationale\n\n**State of the Art Accuracy by Class Count:**\n- Food-101 (101 classes): 95% accuracy\n- Food-172 (172 classes): 94% accuracy  \n- Food-251 (251 classes): 81% accuracy\n- Food-256 (256 classes): 83% accuracy\n\nPattern: Every 2.5x increase = ~13% accuracy drop. At 500K classes, accuracy would be effectively random.\n\n**Visual Ambiguity Problem:**\nMany USDA items are visually identical:\n- \"Chicken breast, grilled, skin removed\" vs \"with skin\"\n- \"Brown rice, cooked\" vs \"white rice, cooked\"\n- Different brands of identical products\n\nNo classifier can distinguish these - only metadata and user confirmation can.\n\n### Target Architecture\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    FOOD RECOGNITION FLOW                     │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  📷 Photo ──► Classifier (300-500 categories)               │\n│                    │                                        │\n│                    ▼                                        │\n│              \"chicken_breast\" (89% confidence)              │\n│                    │                                        │\n│                    ▼                                        │\n│  USDA Search: query=\"chicken breast\"                        │\n│                    │                                        │\n│                    ▼                                        │\n│  Results: [grilled/fried/roasted/raw variants...]           │\n│                    │                                        │\n│                    ▼                                        │\n│  User Confirms: \"Chicken breast, grilled, no skin\"          │\n│                    │                                        │\n│                    ▼                                        │\n│  AR Portion: 150g × nutrition/100g = Final Nutrition        │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n### Parallel Path: Barcode Scanner\n```\n📦 Barcode ──► USDA Branded Lookup ──► Exact Nutrition\n```\n\n### Implementation Tasks\n\n1. **Classifier Upgrade (Task 2)**\n   - Expand from 111 → 300-500 categories\n   - Architecture: Vision Transformer (ViT-B/16) or EfficientNet-B4\n   - Target: 85%+ top-5 accuracy\n   - Output: category string for USDA search\n\n2. **USDA Integration (Task 13)**\n   - Implement search API\n   - Category → search results mapping\n   - User confirmation UI\n\n3. **Barcode Scanner (Task 14)**\n   - Direct USDA branded lookup\n   - No classification needed\n\n### Category Taxonomy Design\n\nExpand current 111 classes following USDA structure:\n- Proteins: chicken_breast, chicken_thigh, beef_steak, ground_beef, salmon_fillet...\n- Grains: white_rice, brown_rice, pasta, bread_white, bread_wheat...\n- Vegetables: broccoli, spinach, carrot, potato_baked, potato_mashed...\n- Fruits: apple, banana, orange, strawberry, blueberry...\n- Dairy: milk, yogurt_plain, yogurt_greek, cheese_cheddar...\n- Mixed: pizza, burger, sandwich, salad, soup...\n\n### Success Metrics\n- Classifier top-5 accuracy: >85%\n- USDA search recall: >95% (correct item in top 10 results)\n- User confirmation rate: <2 taps to find correct item\n- End-to-end nutrition accuracy: within 10% of actual\n\n### References\n- SOTA Food Classification: ViT achieving 95% on Food-101\n- MyFitnessPal Meal Scan: Uses same hierarchical approach\n- USDA FoodData Central API: https://fdc.nal.usda.gov/api-guide/",
        "testStrategy": "1. Document review with team\n2. Validate category taxonomy covers 95%+ of common foods\n3. Prototype search flow with mock classifier output\n4. User testing of confirmation UI (target <2 taps)\n5. Accuracy benchmarking on held-out food images",
        "status": "pending",
        "dependencies": [
          "13"
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 18,
        "title": "Create Backend API Dockerfile",
        "description": "Create an optimized multi-stage Dockerfile for the Express.js backend server with proper caching, security configurations, and health check support.",
        "details": "Create `server/Dockerfile` with the following implementation:\n\n**Stage 1 - Dependencies:**\n```dockerfile\nFROM node:20-alpine AS deps\nWORKDIR /app\nCOPY package.json package-lock.json ./\nRUN npm ci --only=production\n```\n\n**Stage 2 - Builder:**\n```dockerfile\nFROM node:20-alpine AS builder\nWORKDIR /app\nCOPY package.json package-lock.json ./\nRUN npm ci\nCOPY prisma ./prisma/\nRUN npx prisma generate\nCOPY tsconfig.json ./\nCOPY src ./src/\nRUN npm run build\n```\n\n**Stage 3 - Production:**\n```dockerfile\nFROM node:20-alpine AS runner\nWORKDIR /app\nENV NODE_ENV=production\n\n# Create non-root user\nRUN addgroup --system --gid 1001 nodejs && \\\n    adduser --system --uid 1001 expressjs\n\n# Copy production dependencies and built assets\nCOPY --from=deps /app/node_modules ./node_modules\nCOPY --from=builder /app/dist ./dist\nCOPY --from=builder /app/node_modules/.prisma ./node_modules/.prisma\nCOPY --from=builder /app/prisma ./prisma\nCOPY package.json ./\n\nUSER expressjs\nEXPOSE 3000\n\nHEALTHCHECK --interval=30s --timeout=10s --start-period=10s --retries=3 \\\n  CMD node -e \"require('http').get('http://localhost:3000/health', (r) => process.exit(r.statusCode === 200 ? 0 : 1))\"\n\nCMD [\"node\", \"dist/index.js\"]\n```\n\n**Also create `server/.dockerignore`:**\n```\nnode_modules\ndist\n.env*\n*.log\ncoverage\n__tests__\n*.md\n.git\n```\n\n**Key requirements:**\n- Target image size under 200MB\n- Run as non-root user (expressjs)\n- Include Prisma client generation\n- HEALTHCHECK instruction pointing to /health endpoint\n- Proper layer caching for node_modules",
        "testStrategy": "1. Build image locally: `docker build -t nutri-backend:test ./server`\n2. Verify image size: `docker images nutri-backend:test` (should be <200MB)\n3. Run container: `docker run -d -p 3000:3000 --name test-backend nutri-backend:test`\n4. Check health: `docker inspect --format='{{.State.Health.Status}}' test-backend`\n5. Verify non-root: `docker exec test-backend whoami` (should output 'expressjs')\n6. Test API: `curl http://localhost:3000/health`",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create multi-stage Dockerfile with dependency caching",
            "description": "Set up the base multi-stage Dockerfile structure with three stages (deps, builder, runner) using node:20-alpine base images and proper layer caching for node_modules.",
            "dependencies": [],
            "details": "Create `server/Dockerfile` with:\n\n**Stage 1 (deps):** Install production dependencies only using `npm ci --only=production`. This stage caches node_modules for production.\n\n**Stage 2 (builder):** Install all dependencies (including devDependencies) with `npm ci`. This stage will be used for TypeScript compilation and Prisma generation.\n\n**Stage 3 (runner):** Create the final production image by copying production node_modules from deps stage. Use node:20-alpine as base and set NODE_ENV=production.\n\nEach stage should have proper WORKDIR /app and copy package files first to leverage Docker layer caching when dependencies don't change.",
            "status": "pending",
            "testStrategy": "Build the Dockerfile locally: `docker build -t nutri-backend:stage1 ./server`. Verify the build completes successfully and check that layer caching works by running build twice - second build should use cached layers for dependencies.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Configure Prisma generation and TypeScript build in builder stage",
            "description": "Add Prisma client generation and TypeScript compilation steps to the builder stage, ensuring the compiled dist folder and .prisma client are available for production.",
            "dependencies": [
              1
            ],
            "details": "In the builder stage of `server/Dockerfile`:\n\n1. Copy prisma schema: `COPY prisma ./prisma/`\n2. Generate Prisma client: `RUN npx prisma generate`\n3. Copy TypeScript configuration: `COPY tsconfig.json ./`\n4. Copy source code: `COPY src ./src/`\n5. Build TypeScript: `RUN npm run build`\n\nThis produces:\n- `dist/` folder with compiled JavaScript\n- `node_modules/.prisma/` with generated Prisma client\n\nBoth will be copied to the production stage in the next subtask.",
            "status": "pending",
            "testStrategy": "Verify builder stage produces expected artifacts by running: `docker build --target builder -t nutri-backend:builder ./server` then inspect the image: `docker run --rm nutri-backend:builder ls -la dist/` and `docker run --rm nutri-backend:builder ls -la node_modules/.prisma/` to confirm files exist.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create production runtime with security hardening and health checks",
            "description": "Configure the final production stage with non-root user, copy built artifacts from previous stages, add health check, and create .dockerignore file for optimal image size.",
            "dependencies": [
              2
            ],
            "details": "Complete the runner stage in `server/Dockerfile`:\n\n1. Create non-root user: `RUN addgroup --system --gid 1001 nodejs && adduser --system --uid 1001 expressjs`\n2. Copy artifacts:\n   - `COPY --from=deps /app/node_modules ./node_modules`\n   - `COPY --from=builder /app/dist ./dist`\n   - `COPY --from=builder /app/node_modules/.prisma ./node_modules/.prisma`\n   - `COPY --from=builder /app/prisma ./prisma`\n   - `COPY package.json ./`\n3. Switch user: `USER expressjs`\n4. Expose port: `EXPOSE 3000`\n5. Add HEALTHCHECK: `HEALTHCHECK --interval=30s --timeout=10s --start-period=10s --retries=3 CMD node -e \"require('http').get('http://localhost:3000/health', (r) => process.exit(r.statusCode === 200 ? 0 : 1))\"`\n6. Set entrypoint: `CMD [\"node\", \"dist/index.js\"]`\n\n**Create `server/.dockerignore`:**\n```\nnode_modules\ndist\n.env*\n*.log\ncoverage\n__tests__\n*.md\n.git\n.gitignore\nnpm-debug.log*\n.DS_Store\n```",
            "status": "pending",
            "testStrategy": "1. Build final image: `docker build -t nutri-backend:test ./server`\n2. Verify size under 200MB: `docker images nutri-backend:test`\n3. Run container: `docker run -d -p 3000:3000 --env DATABASE_URL=postgresql://test:test@localhost:5432/test --env JWT_SECRET=test-secret --name test-backend nutri-backend:test`\n4. Check health status: `docker inspect --format='{{.State.Health.Status}}' test-backend` (should show 'healthy' after ~30s)\n5. Verify non-root user: `docker exec test-backend whoami` (should show 'expressjs')\n6. Test endpoint: `curl http://localhost:3000/health`\n7. Cleanup: `docker stop test-backend && docker rm test-backend`",
            "parentId": "undefined"
          }
        ],
        "complexity": 4,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down the Dockerfile creation into: 1) Set up multi-stage build structure with proper base images and dependency caching, 2) Configure Prisma client generation and TypeScript compilation in builder stage, 3) Create production runtime stage with non-root user, health checks, and security hardening. Include .dockerignore creation and local testing.",
        "updatedAt": "2025-12-17T23:20:53.506Z"
      },
      {
        "id": 19,
        "title": "Enhance ML Service Dockerfile for Production",
        "description": "Update the existing ML service Dockerfile to run as non-root user and optimize for production deployment with proper security configurations.",
        "details": "Update `ml-service/Dockerfile` to add non-root user support:\n\n**Modifications to existing Dockerfile:**\n```dockerfile\n# Stage 2: Runtime (update existing)\nFROM python:3.11-slim\n\nWORKDIR /app\n\n# Install runtime dependencies\nRUN apt-get update && apt-get install -y \\\n    libpq5 \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Create non-root user\nRUN groupadd --system --gid 1001 mlservice && \\\n    useradd --system --uid 1001 --gid mlservice mlservice\n\n# Copy Python dependencies from builder (update ownership)\nCOPY --from=builder --chown=mlservice:mlservice /root/.local /home/mlservice/.local\n\n# Update PATH for non-root user\nENV PATH=/home/mlservice/.local/bin:$PATH\n\n# Copy application code\nCOPY --chown=mlservice:mlservice ./app /app/app\n\n# Create directories for ML models with correct ownership\nRUN mkdir -p /app/app/ml_models && chown -R mlservice:mlservice /app\n\n# Switch to non-root user\nUSER mlservice\n\n# Expose port\nEXPOSE 8000\n\n# Environment variables\nENV PYTHONDONTWRITEBYTECODE=1 \\\n    PYTHONUNBUFFERED=1\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \\\n    CMD python -c \"import urllib.request; urllib.request.urlopen('http://localhost:8000/health')\"\n\n# Run the application\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--workers\", \"4\"]\n```\n\n**Create `ml-service/.dockerignore`:**\n```\n__pycache__\n*.pyc\n*.pyo\n.pytest_cache\n.mypy_cache\nvenv\n.env*\n*.log\ncoverage\ntests\n*.md\n.git\n```\n\n**Key changes:**\n- Add non-root user 'mlservice'\n- Set PYTHONDONTWRITEBYTECODE and PYTHONUNBUFFERED\n- Proper ownership of files\n- Keep existing health check and multi-stage build",
        "testStrategy": "1. Build image: `cd ml-service && docker build -t nutri-ml:test .`\n2. Verify non-root: `docker run --rm nutri-ml:test whoami` (should output 'mlservice')\n3. Test health endpoint: `docker run -d -p 8000:8000 --name test-ml nutri-ml:test && sleep 5 && curl http://localhost:8000/health`\n4. Check container health: `docker inspect --format='{{.State.Health.Status}}' test-ml`\n5. Verify image size is reasonable (target <1.5GB due to ML dependencies)",
        "priority": "high",
        "dependencies": [],
        "status": "in-progress",
        "subtasks": [
          {
            "id": 1,
            "title": "Modify Dockerfile to add non-root user with proper ownership",
            "description": "Update the existing ml-service/Dockerfile to create and configure a non-root user 'mlservice' with proper file ownership, PATH configuration, and security best practices.",
            "dependencies": [],
            "details": "Update the Dockerfile runtime stage to:\n1. Create system group 'mlservice' (GID 1001) and user 'mlservice' (UID 1001)\n2. Copy Python dependencies from builder with --chown=mlservice:mlservice to /home/mlservice/.local\n3. Update PATH environment variable to include /home/mlservice/.local/bin\n4. Copy application code with --chown=mlservice:mlservice\n5. Create /app/app/ml_models directory with proper ownership (chown -R mlservice:mlservice /app)\n6. Switch to non-root user with USER mlservice directive\n7. Ensure existing health check, multi-stage build, and uvicorn CMD remain functional\n8. Keep existing runtime dependencies (libpq5) installation",
            "status": "pending",
            "testStrategy": "1. Build image: `cd ml-service && docker build -t nutri-ml:test .`\n2. Verify non-root user: `docker run --rm nutri-ml:test whoami` (expected: 'mlservice')\n3. Check user ID: `docker run --rm nutri-ml:test id` (expected: uid=1001(mlservice) gid=1001(mlservice))\n4. Verify file ownership: `docker run --rm nutri-ml:test ls -la /app` (all files should be owned by mlservice:mlservice)\n5. Test service starts: `docker run -d -p 8000:8000 --name test-ml nutri-ml:test && sleep 5 && curl http://localhost:8000/health`",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create .dockerignore and add production environment variables",
            "description": "Create a .dockerignore file to exclude unnecessary files from the Docker build context and add PYTHONDONTWRITEBYTECODE and PYTHONUNBUFFERED environment variables for production security and performance.",
            "dependencies": [
              1
            ],
            "details": "1. Create ml-service/.dockerignore with exclusions:\n   - Python cache files: __pycache__, *.pyc, *.pyo\n   - Test and coverage directories: .pytest_cache, .mypy_cache, coverage, tests\n   - Virtual environments and configs: venv, .env*, *.log\n   - Documentation and version control: *.md, .git\n2. Add environment variables to Dockerfile:\n   - PYTHONDONTWRITEBYTECODE=1 (prevents .pyc file generation)\n   - PYTHONUNBUFFERED=1 (ensures real-time logging)\n3. Verify existing EXPOSE 8000 and HEALTHCHECK directives remain intact\n4. Ensure CMD uvicorn with --workers 4 configuration is preserved",
            "status": "pending",
            "testStrategy": "1. Verify .dockerignore: `docker build -t nutri-ml:test . --progress=plain` and check build context size is reduced\n2. Build image and verify environment variables: `docker run --rm nutri-ml:test env | grep PYTHON` (should show both variables)\n3. Test health check works: `docker run -d -p 8000:8000 --name test-ml nutri-ml:test && sleep 40 && docker inspect --format='{{.State.Health.Status}}' test-ml` (expected: 'healthy')\n4. Verify no .pyc files: `docker run --rm nutri-ml:test find /app -name '*.pyc'` (should be empty)\n5. Full integration test: Start container, verify service runs as mlservice user, health endpoint responds, and logs are unbuffered",
            "parentId": "undefined"
          }
        ],
        "complexity": 3,
        "recommendedSubtasks": 2,
        "expansionPrompt": "Split into: 1) Modify existing Dockerfile to add non-root user 'mlservice' with proper file ownership and PATH configuration, 2) Create .dockerignore and update environment variables for production security. Test that the service runs as non-root and health check works correctly.",
        "updatedAt": "2025-12-17T23:20:53.976Z"
      },
      {
        "id": 20,
        "title": "Create Production Docker Compose Configuration",
        "description": "Create docker-compose.prod.yml that mirrors production environment for local testing, including all services with production-like settings and .env.example documentation.",
        "details": "**Create `docker-compose.prod.yml`:**\n```yaml\nversion: '3.8'\n\nservices:\n  backend:\n    build:\n      context: ./server\n      dockerfile: Dockerfile\n    container_name: nutri-backend\n    restart: unless-stopped\n    environment:\n      - NODE_ENV=production\n      - DATABASE_URL=${DATABASE_URL}\n      - JWT_SECRET=${JWT_SECRET}\n      - JWT_EXPIRES_IN=${JWT_EXPIRES_IN:-7d}\n      - PORT=3000\n      - REDIS_URL=${REDIS_URL}\n      - ML_SERVICE_URL=http://ml-service:8000\n    ports:\n      - \"3000:3000\"\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    healthcheck:\n      test: [\"CMD\", \"node\", \"-e\", \"require('http').get('http://localhost:3000/health', r => process.exit(r.statusCode === 200 ? 0 : 1))\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n    networks:\n      - nutri-network\n    logging:\n      driver: json-file\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n\n  ml-service:\n    build:\n      context: ./ml-service\n      dockerfile: Dockerfile\n    container_name: nutri-ml-service\n    restart: unless-stopped\n    environment:\n      - DATABASE_URL=${ML_DATABASE_URL:-postgresql+asyncpg://postgres:postgres@postgres:5432/nutri_db}\n      - REDIS_URL=${REDIS_URL:-redis://redis:6379/0}\n      - ENVIRONMENT=production\n      - DEBUG=false\n      - LOG_LEVEL=INFO\n    expose:\n      - \"8000\"\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    healthcheck:\n      test: [\"CMD\", \"python\", \"-c\", \"import urllib.request; urllib.request.urlopen('http://localhost:8000/health')\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 40s\n    networks:\n      - nutri-network\n    logging:\n      driver: json-file\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n\n  postgres:\n    image: postgres:16-alpine\n    container_name: nutri-postgres-prod\n    restart: unless-stopped\n    environment:\n      POSTGRES_USER: ${POSTGRES_USER:-postgres}\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n      POSTGRES_DB: ${POSTGRES_DB:-nutri_db}\n    volumes:\n      - postgres_prod_data:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U ${POSTGRES_USER:-postgres}\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n    networks:\n      - nutri-network\n\n  redis:\n    image: redis:7-alpine\n    container_name: nutri-redis-prod\n    restart: unless-stopped\n    command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru\n    volumes:\n      - redis_prod_data:/data\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n    networks:\n      - nutri-network\n\nvolumes:\n  postgres_prod_data:\n  redis_prod_data:\n\nnetworks:\n  nutri-network:\n    driver: bridge\n```\n\n**Create/Update `.env.example`:**\n```env\n# Database (Required)\nDATABASE_URL=postgresql://postgres:password@localhost:5432/nutri_db\nML_DATABASE_URL=postgresql+asyncpg://postgres:password@localhost:5432/nutri_db\nPOSTGRES_USER=postgres\nPOSTGRES_PASSWORD=your-secure-password\nPOSTGRES_DB=nutri_db\n\n# Redis\nREDIS_URL=redis://localhost:6379/0\n\n# JWT (Required)\nJWT_SECRET=your-256-bit-secret-key-change-in-production\nJWT_EXPIRES_IN=7d\n\n# Environment\nNODE_ENV=production\n```",
        "testStrategy": "1. Copy `.env.example` to `.env.prod` and fill in test values\n2. Run: `docker-compose -f docker-compose.prod.yml --env-file .env.prod up --build`\n3. Verify all services start: `docker-compose -f docker-compose.prod.yml ps`\n4. Test backend: `curl http://localhost:3000/health`\n5. Verify ML service is only internally accessible (not on host port)\n6. Test backend can reach ML service internally\n7. Check logs: `docker-compose -f docker-compose.prod.yml logs backend`",
        "priority": "high",
        "dependencies": [
          "18",
          "19"
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create docker-compose.prod.yml with backend service configuration",
            "description": "Create the production Docker Compose file with backend service including health checks, restart policies, environment variables, port mappings, and dependency configuration on postgres and redis services.",
            "dependencies": [],
            "details": "Create `docker-compose.prod.yml` in the project root with version 3.8. Configure the backend service with:\n- Build context pointing to ./server with Dockerfile\n- Container name: nutri-backend\n- Restart policy: unless-stopped\n- Environment variables: NODE_ENV=production, DATABASE_URL, JWT_SECRET, JWT_EXPIRES_IN (default 7d), PORT=3000, REDIS_URL, ML_SERVICE_URL=http://ml-service:8000\n- Port mapping: 3000:3000\n- Dependencies on postgres and redis with health check conditions\n- Health check using Node.js HTTP request to /health endpoint (interval: 30s, timeout: 10s, retries: 3)\n- Network: nutri-network\n- JSON file logging with max-size 10m and max-file 3",
            "status": "pending",
            "testStrategy": "Build and start backend service: `docker-compose -f docker-compose.prod.yml up backend --build`. Verify container starts successfully, health check passes, and service responds to `curl http://localhost:3000/health`. Check logs show production environment.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Add ML service with internal networking and resource configuration",
            "description": "Configure the ml-service in docker-compose.prod.yml with internal-only networking (no host port exposure), proper dependencies, health checks, and production-appropriate settings.",
            "dependencies": [
              1
            ],
            "details": "Add ml-service to docker-compose.prod.yml:\n- Build context: ./ml-service with Dockerfile\n- Container name: nutri-ml-service\n- Restart policy: unless-stopped\n- Environment: DATABASE_URL (ML_DATABASE_URL with default), REDIS_URL (with default), ENVIRONMENT=production, DEBUG=false, LOG_LEVEL=INFO\n- Expose port 8000 internally only (no ports mapping to host)\n- Dependencies on postgres and redis with health check conditions\n- Health check using Python urllib to call /health endpoint (interval: 30s, timeout: 10s, retries: 3, start_period: 40s)\n- Network: nutri-network\n- JSON file logging with max-size 10m and max-file 3\n\nEnsure ML service is only accessible from backend service via internal network.",
            "status": "pending",
            "testStrategy": "Start all services: `docker-compose -f docker-compose.prod.yml up --build`. Verify ml-service container starts, health check passes, and is NOT accessible from host (`curl http://localhost:8000/health` should fail). Test backend can reach ML service by checking backend logs for successful ML_SERVICE_URL connections.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Configure postgres and redis services with production volumes and health checks",
            "description": "Set up postgres and redis services in docker-compose.prod.yml with named volumes for data persistence, health checks, restart policies, and production-optimized configurations.",
            "dependencies": [
              1
            ],
            "details": "Add postgres service:\n- Image: postgres:16-alpine\n- Container name: nutri-postgres-prod\n- Restart policy: unless-stopped\n- Environment: POSTGRES_USER (default postgres), POSTGRES_PASSWORD, POSTGRES_DB (default nutri_db)\n- Volume: postgres_prod_data:/var/lib/postgresql/data\n- Health check: pg_isready command (interval: 10s, timeout: 5s, retries: 5)\n- Network: nutri-network\n\nAdd redis service:\n- Image: redis:7-alpine\n- Container name: nutri-redis-prod\n- Restart policy: unless-stopped\n- Command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru\n- Volume: redis_prod_data:/data\n- Health check: redis-cli ping (interval: 10s, timeout: 5s, retries: 5)\n- Network: nutri-network\n\nDefine volumes section with postgres_prod_data and redis_prod_data.\nDefine networks section with nutri-network using bridge driver.",
            "status": "pending",
            "testStrategy": "Start postgres and redis: `docker-compose -f docker-compose.prod.yml up postgres redis`. Verify both services start with healthy status: `docker-compose -f docker-compose.prod.yml ps`. Check volumes are created: `docker volume ls | grep prod`. Test postgres connection: `docker exec nutri-postgres-prod pg_isready`. Test redis: `docker exec nutri-redis-prod redis-cli ping`.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Create/update .env.example with comprehensive documentation",
            "description": "Create or update .env.example file with all required environment variables, sensible defaults, security notes, and clear documentation for production deployment configuration.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Create `.env.example` in project root with documented sections:\n\n**Database (Required):**\n- DATABASE_URL=postgresql://postgres:password@localhost:5432/nutri_db\n- ML_DATABASE_URL=postgresql+asyncpg://postgres:password@localhost:5432/nutri_db\n- POSTGRES_USER=postgres\n- POSTGRES_PASSWORD=your-secure-password (with security note)\n- POSTGRES_DB=nutri_db\n\n**Redis:**\n- REDIS_URL=redis://localhost:6379/0\n\n**JWT (Required):**\n- JWT_SECRET=your-256-bit-secret-key-change-in-production (with strong security warning)\n- JWT_EXPIRES_IN=7d\n\n**Environment:**\n- NODE_ENV=production\n\nInclude comments explaining each variable's purpose, required vs optional status, and security implications. Add header with instructions to copy to .env.prod for local testing.",
            "status": "pending",
            "testStrategy": "Copy .env.example to .env.prod and fill in test values. Run full stack: `docker-compose -f docker-compose.prod.yml --env-file .env.prod up --build`. Verify all services start successfully: `docker-compose -f docker-compose.prod.yml ps`. Test inter-service communication: backend health check, backend to ML service connection, postgres connectivity, redis connectivity. Verify environment variables are correctly passed to containers.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Divide into: 1) Create docker-compose.prod.yml with backend service configuration including health checks and dependencies, 2) Add ML service with internal-only networking and proper resource limits, 3) Configure postgres and redis services with production volumes and health checks, 4) Create/update .env.example with all required environment variables and documentation. Test full stack startup and inter-service communication."
      },
      {
        "id": 21,
        "title": "Implement Comprehensive Health Check Endpoints",
        "description": "Enhance the backend health endpoint to check database and Redis connectivity, and add similar comprehensive checks to the ML service. Support both liveness and readiness probes.",
        "details": "**Update `server/src/index.ts` - Replace simple health check:**\n```typescript\nimport { PrismaClient } from '@prisma/client';\nimport { Redis } from 'ioredis'; // or use existing redis client\n\nconst prisma = new PrismaClient();\nconst packageJson = require('../package.json');\n\n// Liveness probe - just checks if server is running\napp.get('/health/live', (_req, res) => {\n  res.status(200).json({ status: 'ok' });\n});\n\n// Readiness probe - checks all dependencies\napp.get('/health', async (_req, res) => {\n  const checks: Record<string, { status: string; latency_ms?: number; error?: string }> = {};\n  let overallStatus = 'healthy';\n\n  // Database check\n  const dbStart = Date.now();\n  try {\n    await prisma.$queryRaw`SELECT 1`;\n    checks.database = { status: 'healthy', latency_ms: Date.now() - dbStart };\n  } catch (error) {\n    checks.database = { status: 'unhealthy', error: (error as Error).message };\n    overallStatus = 'unhealthy';\n  }\n\n  // Redis check (if configured)\n  if (process.env.REDIS_URL) {\n    const redisStart = Date.now();\n    try {\n      // Ping redis\n      checks.redis = { status: 'healthy', latency_ms: Date.now() - redisStart };\n    } catch (error) {\n      checks.redis = { status: 'unhealthy', error: (error as Error).message };\n      overallStatus = 'degraded'; // Redis is optional\n    }\n  }\n\n  // ML Service check (optional, don't fail if unavailable)\n  if (process.env.ML_SERVICE_URL) {\n    const mlStart = Date.now();\n    try {\n      const response = await fetch(`${process.env.ML_SERVICE_URL}/health`, {\n        signal: AbortSignal.timeout(5000),\n      });\n      if (response.ok) {\n        checks.ml_service = { status: 'healthy', latency_ms: Date.now() - mlStart };\n      } else {\n        checks.ml_service = { status: 'degraded', latency_ms: Date.now() - mlStart };\n      }\n    } catch (error) {\n      checks.ml_service = { status: 'unavailable', error: (error as Error).message };\n    }\n  }\n\n  const statusCode = overallStatus === 'healthy' ? 200 : overallStatus === 'degraded' ? 200 : 503;\n  res.status(statusCode).json({\n    status: overallStatus,\n    version: packageJson.version,\n    timestamp: new Date().toISOString(),\n    checks,\n  });\n});\n```\n\n**Update ML Service `ml-service/app/main.py`:**\n```python\nfrom fastapi import FastAPI\nfrom datetime import datetime\nimport time\nfrom app.config import settings\n\n@app.get('/health')\nasync def health_check():\n    checks = {}\n    overall_status = 'healthy'\n    \n    # Database check\n    db_start = time.time()\n    try:\n        async with get_db() as db:\n            await db.execute('SELECT 1')\n        checks['database'] = {'status': 'healthy', 'latency_ms': int((time.time() - db_start) * 1000)}\n    except Exception as e:\n        checks['database'] = {'status': 'unhealthy', 'error': str(e)}\n        overall_status = 'unhealthy'\n    \n    # Redis check\n    redis_start = time.time()\n    try:\n        await redis_client.ping()\n        checks['redis'] = {'status': 'healthy', 'latency_ms': int((time.time() - redis_start) * 1000)}\n    except Exception as e:\n        checks['redis'] = {'status': 'unhealthy', 'error': str(e)}\n        overall_status = 'degraded'\n    \n    # Model check\n    checks['model'] = {'status': 'healthy' if model_loaded else 'unavailable'}\n    \n    status_code = 200 if overall_status in ['healthy', 'degraded'] else 503\n    return JSONResponse(\n        status_code=status_code,\n        content={\n            'status': overall_status,\n            'version': settings.VERSION,\n            'timestamp': datetime.utcnow().isoformat() + 'Z',\n            'checks': checks\n        }\n    )\n\n@app.get('/health/live')\nasync def liveness():\n    return {'status': 'ok'}\n```",
        "testStrategy": "1. Unit test health endpoint with mocked dependencies\n2. Integration test with real DB/Redis: verify response structure\n3. Test failure scenarios: stop postgres, verify status=unhealthy and 503\n4. Test degraded state: stop redis, verify status=degraded and 200\n5. Verify /health/live always returns 200\n6. Test response latency <100ms under normal conditions\n7. Verify version field matches package.json",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Enhance backend /health endpoint with database connectivity check",
            "description": "Update server/src/index.ts to add Prisma database connectivity check with latency measurement and proper error handling",
            "dependencies": [],
            "details": "Import PrismaClient and use $queryRaw to execute SELECT 1 query. Measure latency using Date.now() before and after query. Wrap in try-catch to handle errors. Return status 'healthy' on success with latency_ms, or 'unhealthy' with error message on failure. Set overall status to 'unhealthy' if database check fails. Import package.json to include version in response.",
            "status": "pending",
            "testStrategy": "Unit test with mocked Prisma client. Integration test with real database connection. Test failure scenario by stopping PostgreSQL and verifying 503 status code and unhealthy status in response.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Add Redis connectivity check with optional dependency handling",
            "description": "Implement Redis ping check in /health endpoint with proper error handling for when Redis is not configured or unavailable",
            "dependencies": [
              1
            ],
            "details": "Check if REDIS_URL environment variable exists. If configured, initialize Redis client (ioredis) and ping. Measure latency. On success, add redis check with 'healthy' status. On failure, add 'unhealthy' status but set overall status to 'degraded' (not unhealthy) since Redis is optional. Skip Redis check entirely if REDIS_URL not set.",
            "status": "pending",
            "testStrategy": "Test with REDIS_URL configured: verify ping succeeds and returns healthy. Test with Redis stopped: verify degraded status (200 status code, not 503). Test without REDIS_URL: verify no redis check in response.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add ML service reachability check from backend",
            "description": "Implement optional ML service health check using fetch with timeout to verify connectivity to ml-service",
            "dependencies": [
              1
            ],
            "details": "Check if ML_SERVICE_URL environment variable exists. Use fetch with AbortSignal.timeout(5000) to call ${ML_SERVICE_URL}/health. On successful response (response.ok), mark as 'healthy' with latency. On error or non-ok response, mark as 'unavailable' or 'degraded'. Don't fail overall health check if ML service is down (it's optional).",
            "status": "pending",
            "testStrategy": "Test with ML_SERVICE_URL configured and service running: verify healthy status. Test with service unreachable: verify unavailable status but overall health remains healthy/degraded. Test timeout scenario with 5s limit. Test without ML_SERVICE_URL: verify no ml_service check in response.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement /health/live liveness probe endpoint",
            "description": "Create simple liveness probe endpoint that returns immediate OK status without dependency checks",
            "dependencies": [],
            "details": "Add GET /health/live route in server/src/index.ts. Return simple JSON response {status: 'ok'} with 200 status code. No async checks, no database/redis/ml connectivity verification. This endpoint is for container orchestrators to verify the process is running.",
            "status": "pending",
            "testStrategy": "Unit test verifying /health/live returns 200 and {status: 'ok'}. Test that it responds even when database is down (unlike /health). Verify response time is <10ms.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Create comprehensive health checks for ML service in Python",
            "description": "Implement /health and /health/live endpoints in ml-service FastAPI app with database, Redis, and model status checks",
            "dependencies": [
              4
            ],
            "details": "Update ml-service/app/main.py. Add /health/live endpoint returning {'status': 'ok'}. Enhance /health endpoint: 1) Database check using async db.execute('SELECT 1') with latency measurement, 2) Redis check using await redis_client.ping() with latency, 3) Model status check (healthy if model_loaded else unavailable). Set overall_status to 'healthy', 'degraded' (if Redis fails), or 'unhealthy' (if DB fails). Return 200 for healthy/degraded, 503 for unhealthy. Include version from settings.VERSION and timestamp.",
            "status": "pending",
            "testStrategy": "Unit tests with mocked database/redis clients. Integration tests with real connections. Test database failure: verify unhealthy status and 503. Test Redis failure: verify degraded status and 200. Test model not loaded: verify model check shows unavailable. Test /health/live returns ok regardless of dependencies.",
            "parentId": "undefined"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down into: 1) Enhance backend /health endpoint to check database connectivity with Prisma and measure latency, 2) Add Redis connectivity check with proper error handling for optional dependency, 3) Add ML service reachability check from backend, 4) Implement /health/live liveness probe endpoint, 5) Create similar comprehensive health checks for ML service in Python with database and redis checks. Write tests for all health check scenarios."
      },
      {
        "id": 22,
        "title": "Create GitHub Actions Test Workflow Enhancement",
        "description": "Enhance the existing CI workflow to improve caching, add security scanning, and prepare for the build workflow integration.",
        "details": "**Update `.github/workflows/ci.yml` to enhance existing workflow:**\n\nAdd security scanning job after existing jobs:\n```yaml\n  security-scan:\n    name: Security Scan\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: 'npm'\n\n      - name: Run npm audit (root)\n        run: npm audit --audit-level=high || true\n        continue-on-error: true\n\n      - name: Run npm audit (server)\n        run: cd server && npm audit --audit-level=high || true\n        continue-on-error: true\n\n      - name: Setup Python for ML service scan\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n\n      - name: Run safety check (ML service)\n        run: |\n          pip install safety\n          cd ml-service && safety check -r requirements.txt || true\n        continue-on-error: true\n\n      - name: Run Trivy vulnerability scanner\n        uses: aquasecurity/trivy-action@master\n        with:\n          scan-type: 'fs'\n          scan-ref: '.'\n          severity: 'CRITICAL,HIGH'\n          exit-code: '0'  # Don't fail build, just report\n```\n\nUpdate ci-success job to include security-scan:\n```yaml\n  ci-success:\n    name: CI Success\n    runs-on: ubuntu-latest\n    needs: [lint, backend-tests, mobile-tests, ml-service-lint, security-scan]\n    if: always()\n    steps:\n      - name: Check all jobs\n        run: |\n          if [[ \"${{ needs.lint.result }}\" != \"success\" ]] || \\\n             [[ \"${{ needs.backend-tests.result }}\" != \"success\" ]] || \\\n             [[ \"${{ needs.mobile-tests.result }}\" != \"success\" ]] || \\\n             [[ \"${{ needs.ml-service-lint.result }}\" != \"success\" ]]; then\n            echo \"One or more jobs failed!\"\n            exit 1\n          fi\n          # Security scan is informational, don't fail on it\n          echo \"All required CI jobs passed!\"\n```\n\n**Add caching improvements for backend tests:**\n```yaml\n      - name: Cache Prisma client\n        uses: actions/cache@v4\n        with:\n          path: server/node_modules/.prisma\n          key: prisma-${{ runner.os }}-${{ hashFiles('server/prisma/schema.prisma') }}\n```",
        "testStrategy": "1. Create a PR and verify workflow runs\n2. Check security-scan job executes without blocking\n3. Verify caching works - second run should be faster\n4. Check Trivy reports in workflow output\n5. Verify npm audit runs for both root and server\n6. Confirm ci-success job still requires core jobs to pass",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Add security-scan job with npm audit, safety check, and Trivy scanner",
            "description": "Create a new security-scan job in .github/workflows/ci.yml that runs after existing jobs. Include npm audit for root and server directories, safety check for ML service Python dependencies, and Trivy filesystem vulnerability scanner.",
            "dependencies": [],
            "details": "Add a new job to .github/workflows/ci.yml after ml-service-lint job:\n\n1. Configure job to run on ubuntu-latest\n2. Checkout code using actions/checkout@v4\n3. Setup Node.js using actions/setup-node@v4 with cache: 'npm'\n4. Run npm audit for root directory with --audit-level=high, use continue-on-error: true\n5. Run npm audit for server directory with --audit-level=high, use continue-on-error: true\n6. Setup Python 3.11 using actions/setup-python@v5\n7. Install safety package and run safety check on ml-service/requirements.txt with continue-on-error: true\n8. Add Trivy vulnerability scanner using aquasecurity/trivy-action@master with scan-type: 'fs', scan-ref: '.', severity: 'CRITICAL,HIGH', and exit-code: '0'\n\nAll security checks should be informational only (not fail the build) using continue-on-error or exit-code: '0'.",
            "status": "pending",
            "testStrategy": "1. Push changes and verify security-scan job appears in workflow run\n2. Check npm audit executes for both root and server directories\n3. Verify safety check runs on ML service requirements\n4. Confirm Trivy scanner produces vulnerability report in workflow output\n5. Test that security issues don't block the workflow (continue-on-error works)",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Add Prisma client caching to backend-tests job",
            "description": "Enhance the backend-tests job with Prisma client caching to improve CI performance by avoiding regeneration of Prisma client on subsequent runs when schema hasn't changed.",
            "dependencies": [],
            "details": "Update the backend-tests job in .github/workflows/ci.yml:\n\n1. Add a new step after Node.js setup and before dependency installation\n2. Use actions/cache@v4 to cache Prisma client\n3. Set cache path to 'server/node_modules/.prisma'\n4. Use cache key format: prisma-${{ runner.os }}-${{ hashFiles('server/prisma/schema.prisma') }}\n5. This ensures cache is invalidated only when schema.prisma changes\n\nThe caching step should be positioned strategically to maximize reuse while ensuring Prisma client is available for tests.",
            "status": "pending",
            "testStrategy": "1. Push changes and trigger workflow twice\n2. On second run, verify 'Cache Prisma client' step shows cache hit\n3. Compare backend-tests job duration between first run (cache miss) and second run (cache hit)\n4. Verify tests still pass with cached Prisma client\n5. Modify schema.prisma and confirm cache is invalidated (new cache key generated)",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Update ci-success job to include security-scan dependency",
            "description": "Modify the ci-success job to include security-scan in its needs array while ensuring it doesn't fail the build. Security scan results should be informational only.",
            "dependencies": [
              1
            ],
            "details": "Update the ci-success job in .github/workflows/ci.yml:\n\n1. Add 'security-scan' to the needs array: [lint, backend-tests, mobile-tests, ml-service-lint, security-scan]\n2. Keep if: always() condition to ensure job runs even if security-scan has issues\n3. Update the check script to only verify core jobs (lint, backend-tests, mobile-tests, ml-service-lint) succeeded\n4. Do NOT check security-scan result in the failure logic\n5. Add a comment in the success echo statement noting that security scan is informational\n6. Ensure the job still returns exit code 1 if any core jobs fail, but ignores security-scan status\n\nThis makes security-scan visible in the workflow without blocking merges.",
            "status": "pending",
            "testStrategy": "1. Create PR and verify workflow runs with all jobs including security-scan\n2. Verify ci-success job waits for security-scan to complete\n3. Confirm ci-success passes even if security-scan finds issues\n4. Test that ci-success fails if core jobs (lint, backend-tests, etc.) fail\n5. Check workflow summary shows security-scan results but doesn't block PR\n6. Verify final success message acknowledges all required jobs passed",
            "parentId": "undefined"
          }
        ],
        "complexity": 4,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Split into: 1) Add security-scan job with npm audit for both root and server, safety check for ML service, and Trivy filesystem scanner, 2) Add Prisma client caching to backend-tests job to improve performance, 3) Update ci-success job dependencies to include security-scan (informational only, don't fail build on security issues). Test that workflow runs successfully with all checks."
      },
      {
        "id": 23,
        "title": "Create GitHub Actions Build and Push Workflow",
        "description": "Create a new workflow that builds Docker images for backend and ML service, scans them for vulnerabilities, and pushes to GitHub Container Registry on merge to main.",
        "details": "**Create `.github/workflows/build.yml`:**\n```yaml\nname: Build and Push\n\non:\n  push:\n    branches: [master]\n  workflow_dispatch:  # Allow manual trigger\n\nenv:\n  REGISTRY: ghcr.io\n  BACKEND_IMAGE: ghcr.io/${{ github.repository }}/backend\n  ML_SERVICE_IMAGE: ghcr.io/${{ github.repository }}/ml-service\n\njobs:\n  build-backend:\n    name: Build Backend\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      packages: write\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n\n      - name: Login to GitHub Container Registry\n        uses: docker/login-action@v3\n        with:\n          registry: ${{ env.REGISTRY }}\n          username: ${{ github.actor }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: Extract metadata\n        id: meta\n        uses: docker/metadata-action@v5\n        with:\n          images: ${{ env.BACKEND_IMAGE }}\n          tags: |\n            type=sha,prefix=\n            type=raw,value=latest,enable=${{ github.ref == 'refs/heads/master' }}\n\n      - name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./server\n          push: true\n          tags: ${{ steps.meta.outputs.tags }}\n          labels: ${{ steps.meta.outputs.labels }}\n          cache-from: type=gha\n          cache-to: type=gha,mode=max\n\n      - name: Run Trivy vulnerability scanner\n        uses: aquasecurity/trivy-action@master\n        with:\n          image-ref: ${{ env.BACKEND_IMAGE }}:${{ github.sha }}\n          format: 'sarif'\n          output: 'trivy-backend.sarif'\n          severity: 'CRITICAL,HIGH'\n\n      - name: Upload Trivy scan results\n        uses: github/codeql-action/upload-sarif@v3\n        if: always()\n        with:\n          sarif_file: 'trivy-backend.sarif'\n\n    outputs:\n      image: ${{ env.BACKEND_IMAGE }}\n      digest: ${{ steps.meta.outputs.digest }}\n\n  build-ml-service:\n    name: Build ML Service\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      packages: write\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n\n      - name: Login to GitHub Container Registry\n        uses: docker/login-action@v3\n        with:\n          registry: ${{ env.REGISTRY }}\n          username: ${{ github.actor }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: Extract metadata\n        id: meta\n        uses: docker/metadata-action@v5\n        with:\n          images: ${{ env.ML_SERVICE_IMAGE }}\n          tags: |\n            type=sha,prefix=\n            type=raw,value=latest,enable=${{ github.ref == 'refs/heads/master' }}\n\n      - name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./ml-service\n          push: true\n          tags: ${{ steps.meta.outputs.tags }}\n          labels: ${{ steps.meta.outputs.labels }}\n          cache-from: type=gha\n          cache-to: type=gha,mode=max\n\n      - name: Run Trivy vulnerability scanner\n        uses: aquasecurity/trivy-action@master\n        with:\n          image-ref: ${{ env.ML_SERVICE_IMAGE }}:${{ github.sha }}\n          format: 'sarif'\n          output: 'trivy-ml.sarif'\n          severity: 'CRITICAL,HIGH'\n\n      - name: Upload Trivy scan results\n        uses: github/codeql-action/upload-sarif@v3\n        if: always()\n        with:\n          sarif_file: 'trivy-ml.sarif'\n\n    outputs:\n      image: ${{ env.ML_SERVICE_IMAGE }}\n      digest: ${{ steps.meta.outputs.digest }}\n\n  build-success:\n    name: Build Success\n    runs-on: ubuntu-latest\n    needs: [build-backend, build-ml-service]\n    steps:\n      - name: Summary\n        run: |\n          echo \"## Build Summary\" >> $GITHUB_STEP_SUMMARY\n          echo \"\" >> $GITHUB_STEP_SUMMARY\n          echo \"✅ Backend Image: \\`${{ needs.build-backend.outputs.image }}:${{ github.sha }}\\`\" >> $GITHUB_STEP_SUMMARY\n          echo \"✅ ML Service Image: \\`${{ needs.build-ml-service.outputs.image }}:${{ github.sha }}\\`\" >> $GITHUB_STEP_SUMMARY\n```",
        "testStrategy": "1. Push to master branch and verify workflow triggers\n2. Check images appear in GitHub Packages\n3. Verify image tags include SHA and 'latest'\n4. Check Trivy scan results in Security tab\n5. Pull and run images locally to verify they work\n6. Test manual workflow_dispatch trigger\n7. Verify build time <10 minutes with caching",
        "priority": "high",
        "dependencies": [
          "18",
          "19"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create build.yml workflow file with triggers and environment configuration",
            "description": "Create the initial `.github/workflows/build.yml` file with workflow triggers (push to master branch and workflow_dispatch), and define environment variables for GitHub Container Registry (GHCR) and image names.",
            "dependencies": [],
            "details": "Create `.github/workflows/build.yml` with:\n- Workflow name: 'Build and Push'\n- Triggers: `on.push.branches: [master]` and `on.workflow_dispatch` for manual runs\n- Environment variables: REGISTRY (ghcr.io), BACKEND_IMAGE, ML_SERVICE_IMAGE using `${{ github.repository }}` for dynamic repo name\n- Ensure proper YAML formatting and indentation\n\nThis establishes the workflow foundation that subsequent jobs will build upon.",
            "status": "pending",
            "testStrategy": "Verify workflow file syntax with `actionlint` or GitHub's workflow validator. Check that file is properly placed in `.github/workflows/` directory. Confirm environment variable interpolation is correct.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement build-backend job with Docker Buildx and GHCR authentication",
            "description": "Create the build-backend job that sets up Docker Buildx, authenticates with GitHub Container Registry, extracts metadata for image tagging, and builds/pushes the backend Docker image with layer caching.",
            "dependencies": [
              1
            ],
            "details": "Add build-backend job with:\n- Job permissions: `contents: read`, `packages: write`\n- Steps: checkout@v4, setup-buildx-action@v3, login-action@v3 (using GITHUB_TOKEN)\n- metadata-action@v5 for tags: SHA prefix and latest (conditional on master branch)\n- build-push-action@v5 with context: ./server, push: true, GitHub Actions cache (type=gha)\n- Job outputs: image name and digest from metadata step\n\nReferences Task 18 (Backend Dockerfile) which must exist at ./server/Dockerfile.",
            "status": "pending",
            "testStrategy": "Push to master and verify: 1) Buildx setup succeeds, 2) GHCR login succeeds, 3) Image builds and pushes to ghcr.io, 4) Image has both SHA and latest tags, 5) Cache is utilized on subsequent runs (check build time improvement).",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement build-ml-service job with same Docker build pattern",
            "description": "Create the build-ml-service job that mirrors the backend build process but targets the ML service, running in parallel with the backend build job.",
            "dependencies": [
              1
            ],
            "details": "Add build-ml-service job (parallel to build-backend) with:\n- Same permissions and step structure as build-backend\n- Context changed to: ./ml-service\n- Image metadata using ML_SERVICE_IMAGE environment variable\n- Same tagging strategy (SHA + latest on master)\n- Same caching strategy (GitHub Actions cache)\n- Job outputs: ML service image name and digest\n\nReferences Task 19 (ML Service Dockerfile) which must exist at ./ml-service/Dockerfile.",
            "status": "pending",
            "testStrategy": "Push to master and verify: 1) Both jobs run in parallel (check workflow timeline), 2) ML service image builds successfully, 3) Image pushed to ghcr.io with correct tags, 4) Build completes even if backend job is still running.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Add Trivy vulnerability scanning with SARIF upload for both images",
            "description": "Integrate Trivy security scanning into both build jobs to scan Docker images for CRITICAL and HIGH severity vulnerabilities, output results in SARIF format, and upload to GitHub Security tab via CodeQL action.",
            "dependencies": [
              2,
              3
            ],
            "details": "Add to both build-backend and build-ml-service jobs:\n- trivy-action@master step after build-push\n- Scan image-ref using SHA tag: `${{ env.*_IMAGE }}:${{ github.sha }}`\n- Output format: 'sarif', severity filter: 'CRITICAL,HIGH'\n- Separate output files: trivy-backend.sarif and trivy-ml.sarif\n- codeql-action/upload-sarif@v3 with `if: always()` to upload even on scan failures\n- Ensure SARIF files are uploaded to GitHub Security tab for vulnerability tracking",
            "status": "pending",
            "testStrategy": "After workflow runs: 1) Check Security tab for Trivy scan results, 2) Verify SARIF files are uploaded even if vulnerabilities found, 3) Confirm only CRITICAL/HIGH severities reported, 4) Test with known vulnerable base image to ensure scanner detects issues.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Create build-success summary job with image output reporting",
            "description": "Implement the final build-success job that depends on both build jobs completing successfully and outputs a formatted summary of built images with their tags and digests to the GitHub Actions summary.",
            "dependencies": [
              2,
              3
            ],
            "details": "Add build-success job with:\n- Depends on: [build-backend, build-ml-service] using `needs`\n- Single step that writes to $GITHUB_STEP_SUMMARY\n- Output format: Markdown with build summary header\n- Display both image names with SHA tags using job outputs: `${{ needs.build-backend.outputs.image }}:${{ github.sha }}`\n- Include digest information from metadata outputs\n- Use checkmark emojis (✅) for visual clarity\n\nThis provides clear deployment information for the subsequent deploy workflow (Task 24).",
            "status": "pending",
            "testStrategy": "After successful workflow run: 1) Verify build-success job only runs if both builds succeed, 2) Check workflow summary displays formatted output with correct image tags, 3) Test failure scenario by breaking one build job - confirm summary job doesn't run, 4) Verify image tags match actual pushed images in GHCR.",
            "parentId": "undefined"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down into: 1) Create build.yml workflow with triggers (push to master, workflow_dispatch), 2) Implement build-backend job with Docker Buildx, GHCR authentication, metadata extraction, and caching, 3) Implement build-ml-service job with same pattern, 4) Add Trivy vulnerability scanning for both images with SARIF output and GitHub Security integration, 5) Create build-success summary job that outputs image tags and digests. Test workflow with actual push to master.",
        "updatedAt": "2025-12-18T08:54:50.236Z"
      },
      {
        "id": 24,
        "title": "Create GitHub Actions Deploy Workflow",
        "description": "Create a deployment workflow that triggers Coolify webhook after successful build, verifies deployment health, and sends notifications.",
        "details": "**Create `.github/workflows/deploy.yml`:**\n```yaml\nname: Deploy\n\non:\n  workflow_run:\n    workflows: [\"Build and Push\"]\n    types: [completed]\n    branches: [master]\n  workflow_dispatch:  # Manual trigger for rollback\n    inputs:\n      image_tag:\n        description: 'Image tag to deploy (default: latest)'\n        required: false\n        default: 'latest'\n\njobs:\n  deploy:\n    name: Deploy to Production\n    runs-on: ubuntu-latest\n    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Trigger Coolify Backend Deployment\n        id: deploy-backend\n        run: |\n          response=$(curl -s -w \"\\n%{http_code}\" -X POST \\\n            -H \"Authorization: Bearer ${{ secrets.COOLIFY_WEBHOOK_SECRET }}\" \\\n            \"${{ secrets.COOLIFY_BACKEND_WEBHOOK_URL }}\")\n          http_code=$(echo \"$response\" | tail -n1)\n          body=$(echo \"$response\" | sed '$d')\n          echo \"HTTP Status: $http_code\"\n          echo \"Response: $body\"\n          if [ \"$http_code\" != \"200\" ] && [ \"$http_code\" != \"201\" ]; then\n            echo \"Backend deployment trigger failed!\"\n            exit 1\n          fi\n\n      - name: Trigger Coolify ML Service Deployment\n        id: deploy-ml\n        run: |\n          response=$(curl -s -w \"\\n%{http_code}\" -X POST \\\n            -H \"Authorization: Bearer ${{ secrets.COOLIFY_WEBHOOK_SECRET }}\" \\\n            \"${{ secrets.COOLIFY_ML_WEBHOOK_URL }}\")\n          http_code=$(echo \"$response\" | tail -n1)\n          body=$(echo \"$response\" | sed '$d')\n          echo \"HTTP Status: $http_code\"\n          echo \"Response: $body\"\n          if [ \"$http_code\" != \"200\" ] && [ \"$http_code\" != \"201\" ]; then\n            echo \"ML service deployment trigger failed!\"\n            exit 1\n          fi\n\n      - name: Wait for deployment\n        run: sleep 60  # Wait for containers to start\n\n      - name: Verify Backend Health\n        id: health-backend\n        run: |\n          for i in {1..10}; do\n            response=$(curl -s -o /dev/null -w \"%{http_code}\" \\\n              --connect-timeout 5 \\\n              --max-time 10 \\\n              \"${{ secrets.PRODUCTION_API_URL }}/health\" || echo \"000\")\n            echo \"Attempt $i: HTTP $response\"\n            if [ \"$response\" = \"200\" ]; then\n              echo \"Backend is healthy!\"\n              exit 0\n            fi\n            sleep 10\n          done\n          echo \"Backend health check failed!\"\n          exit 1\n\n      - name: Send Success Notification\n        if: success()\n        run: |\n          curl -X POST \"${{ secrets.DISCORD_WEBHOOK_URL }}\" \\\n            -H \"Content-Type: application/json\" \\\n            -d '{\n              \"embeds\": [{\n                \"title\": \"✅ Deployment Successful\",\n                \"description\": \"Nutri has been deployed to production\",\n                \"color\": 5763719,\n                \"fields\": [\n                  {\"name\": \"Commit\", \"value\": \"'\"${{ github.sha }}\"'\", \"inline\": true},\n                  {\"name\": \"Branch\", \"value\": \"master\", \"inline\": true}\n                ],\n                \"timestamp\": \"'\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"'\"\n              }]\n            }' || true\n\n      - name: Send Failure Notification\n        if: failure()\n        run: |\n          curl -X POST \"${{ secrets.DISCORD_WEBHOOK_URL }}\" \\\n            -H \"Content-Type: application/json\" \\\n            -d '{\n              \"embeds\": [{\n                \"title\": \"❌ Deployment Failed\",\n                \"description\": \"Nutri deployment to production failed\",\n                \"color\": 15548997,\n                \"fields\": [\n                  {\"name\": \"Commit\", \"value\": \"'\"${{ github.sha }}\"'\", \"inline\": true},\n                  {\"name\": \"Action\", \"value\": \"[View Logs]('\"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"')\", \"inline\": true}\n                ],\n                \"timestamp\": \"'\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"'\"\n              }]\n            }' || true\n```\n\n**Required GitHub Secrets:**\n- COOLIFY_WEBHOOK_SECRET\n- COOLIFY_BACKEND_WEBHOOK_URL\n- COOLIFY_ML_WEBHOOK_URL\n- PRODUCTION_API_URL\n- DISCORD_WEBHOOK_URL (optional)",
        "testStrategy": "1. Set up test Coolify webhook URLs\n2. Add required secrets to GitHub repository\n3. Trigger workflow manually with workflow_dispatch\n4. Verify Coolify receives webhook calls\n5. Test health check retry logic\n6. Verify Discord notifications are sent\n7. Test failure notification by using invalid webhook URL\n8. Test manual rollback trigger with specific image tag",
        "priority": "high",
        "dependencies": [
          "23"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create deploy.yml with workflow triggers and inputs",
            "description": "Create `.github/workflows/deploy.yml` with workflow_run trigger listening to 'Build and Push' workflow completion, and workflow_dispatch for manual deployments with image_tag input parameter.",
            "dependencies": [],
            "details": "Set up the workflow file with two trigger types: 1) workflow_run that triggers on successful completion of 'Build and Push' workflow on master branch, 2) workflow_dispatch with optional image_tag input (default: 'latest'). Configure the deploy job to run on ubuntu-latest and only execute if the workflow_run was successful OR if manually triggered. This establishes the foundation for the deployment workflow.",
            "status": "pending",
            "testStrategy": "Verify workflow file syntax with `actionlint .github/workflows/deploy.yml`, check that workflow appears in GitHub Actions UI, test manual trigger via workflow_dispatch to ensure input parameter is captured correctly.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Coolify webhook triggers with authentication",
            "description": "Add steps to trigger Coolify webhooks for both backend and ML service deployments with proper authentication headers and response validation.",
            "dependencies": [
              1
            ],
            "details": "Create two separate steps using curl to POST to Coolify webhook URLs with Authorization Bearer token from COOLIFY_WEBHOOK_SECRET. Capture both HTTP status code and response body using `-s -w \"\\n%{http_code}\"`. Validate that responses are 200 or 201, otherwise fail the workflow. Add echo statements for debugging. Structure: COOLIFY_BACKEND_WEBHOOK_URL first, then COOLIFY_ML_WEBHOOK_URL. Include proper error messages if triggers fail.",
            "status": "pending",
            "testStrategy": "Test with mock webhook endpoints (webhook.site or similar) to verify request format and headers, validate error handling by testing with invalid URLs, confirm both backend and ML service triggers execute in sequence.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add deployment wait period and health check with retry logic",
            "description": "Implement a 60-second wait period followed by backend health check verification with 10 retry attempts and exponential backoff.",
            "dependencies": [
              2
            ],
            "details": "Add sleep 60 step to allow containers to start. Create health check step that polls PRODUCTION_API_URL/health endpoint up to 10 times with 10-second intervals. Use curl with --connect-timeout 5 and --max-time 10 flags. Handle connection failures by defaulting to '000' status code. Exit with success (0) on first 200 response, or fail (1) after all retries exhausted. Include iteration counter in echo output for debugging.",
            "status": "pending",
            "testStrategy": "Test health check logic locally with a mock endpoint that returns 503 for first 3 attempts then 200, verify timeout handling by testing against a non-responsive endpoint, confirm retry loop exits immediately on success.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Discord notification integration for deployment status",
            "description": "Add success and failure notification steps that send rich embed messages to Discord webhook with deployment details and links.",
            "dependencies": [
              3
            ],
            "details": "Create two conditional notification steps: 1) Success notification (if: success()) with green embed (color: 5763719) showing ✅ title, commit SHA, branch, and timestamp, 2) Failure notification (if: failure()) with red embed (color: 15548997) showing ❌ title, commit SHA, and link to workflow run logs. Use curl to POST JSON payloads to DISCORD_WEBHOOK_URL. Include `|| true` to prevent notification failures from affecting workflow status. Format timestamp using `date -u +%Y-%m-%dT%H:%M:%SZ`.",
            "status": "pending",
            "testStrategy": "Send test notifications to Discord webhook during development, verify embed formatting and colors display correctly, test that notification failures don't break the workflow (|| true works), validate that GitHub links in failure messages are clickable and correct.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Configure required GitHub repository secrets",
            "description": "Document and configure all required GitHub secrets: COOLIFY_WEBHOOK_SECRET, COOLIFY_BACKEND_WEBHOOK_URL, COOLIFY_ML_WEBHOOK_URL, PRODUCTION_API_URL, and DISCORD_WEBHOOK_URL.",
            "dependencies": [],
            "details": "Create a secrets checklist and add placeholder values to GitHub repository settings. Required secrets: 1) COOLIFY_WEBHOOK_SECRET - Bearer token for Coolify authentication, 2) COOLIFY_BACKEND_WEBHOOK_URL - Full webhook URL for backend deployment, 3) COOLIFY_ML_WEBHOOK_URL - Full webhook URL for ML service, 4) PRODUCTION_API_URL - Base URL for health checks (e.g., https://api.nutri.com), 5) DISCORD_WEBHOOK_URL - Discord webhook endpoint (optional but recommended). Document these in deployment docs and ensure they're referenced correctly in workflow file.",
            "status": "pending",
            "testStrategy": "Verify all secrets are accessible in workflow by adding debug steps that echo masked values (first 4 chars only), test with intentionally incorrect secrets to ensure proper error messages, confirm DISCORD_WEBHOOK_URL is truly optional (workflow succeeds without it).",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "End-to-end workflow testing with actual Coolify deployment",
            "description": "Test the complete deployment workflow with real Coolify webhooks, verify health checks, monitor deployment process, and validate notifications.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5
            ],
            "details": "Execute full workflow test: 1) Merge a test PR to trigger 'Build and Push' workflow, 2) Verify deploy workflow triggers automatically on completion, 3) Monitor Coolify for webhook reception and deployment start, 4) Watch health check retries in action logs, 5) Verify Discord success notification, 6) Test manual rollback using workflow_dispatch with previous image tag, 7) Test failure scenario by temporarily breaking health endpoint or using invalid webhook URL, 8) Confirm failure notification is sent. Document any issues and iterations needed.",
            "status": "pending",
            "testStrategy": "Run complete workflow in production environment, verify Coolify deploys correct container versions, confirm health checks pass within expected timeframe (60s + retries), validate both success and failure notifications appear in Discord, test manual trigger with specific image tags, check GitHub Actions logs for any errors or warnings.",
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Divide into: 1) Create deploy.yml with workflow_run trigger from 'Build and Push' workflow and manual trigger with image_tag input, 2) Implement Coolify webhook triggers for both backend and ML service with authentication, 3) Add wait period and health check verification with retry logic for backend, 4) Implement Discord/notification integration for success and failure cases, 5) Configure required GitHub secrets (COOLIFY_WEBHOOK_SECRET, COOLIFY_BACKEND_WEBHOOK_URL, COOLIFY_ML_WEBHOOK_URL, PRODUCTION_API_URL, DISCORD_WEBHOOK_URL), 6) Test workflow with actual Coolify webhooks and verify deployment health.",
        "updatedAt": "2025-12-18T08:55:37.663Z"
      },
      {
        "id": 25,
        "title": "Create Database Migration Workflow",
        "description": "Implement a safe database migration process that runs Prisma migrations as part of deployment with backup and rollback capabilities.",
        "details": "**Create `.github/workflows/migrate.yml`:**\n```yaml\nname: Database Migration\n\non:\n  workflow_dispatch:\n    inputs:\n      dry_run:\n        description: 'Dry run (show changes without applying)'\n        required: true\n        default: 'true'\n        type: choice\n        options:\n          - 'true'\n          - 'false'\n\njobs:\n  migrate:\n    name: Run Migrations\n    runs-on: ubuntu-latest\n    environment: production  # Requires approval\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: cd server && npm ci\n\n      - name: Generate Prisma client\n        run: cd server && npx prisma generate\n\n      - name: Show pending migrations (Dry Run)\n        if: inputs.dry_run == 'true'\n        env:\n          DATABASE_URL: ${{ secrets.DATABASE_URL }}\n        run: |\n          cd server\n          npx prisma migrate status\n          echo \"---\"\n          echo \"Pending migrations preview:\"\n          npx prisma migrate diff --from-migrations ./prisma/migrations --to-schema-datamodel ./prisma/schema.prisma\n\n      - name: Run migrations\n        if: inputs.dry_run == 'false'\n        env:\n          DATABASE_URL: ${{ secrets.DATABASE_URL }}\n        run: |\n          cd server\n          echo \"Running migrations...\"\n          npx prisma migrate deploy\n          echo \"Migration complete!\"\n          npx prisma migrate status\n\n      - name: Notify on success\n        if: success() && inputs.dry_run == 'false'\n        run: |\n          curl -X POST \"${{ secrets.DISCORD_WEBHOOK_URL }}\" \\\n            -H \"Content-Type: application/json\" \\\n            -d '{\"content\": \"✅ Database migration completed successfully\"}' || true\n\n      - name: Notify on failure\n        if: failure()\n        run: |\n          curl -X POST \"${{ secrets.DISCORD_WEBHOOK_URL }}\" \\\n            -H \"Content-Type: application/json\" \\\n            -d '{\"content\": \"❌ Database migration failed! Check GitHub Actions logs.\"}' || true\n```\n\n**Create migration helper script `scripts/deploy/migrate.sh`:**\n```bash\n#!/bin/bash\nset -e\n\necho \"=== Nutri Database Migration ===\"\necho \"Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)\"\necho \"\"\n\n# Check if DATABASE_URL is set\nif [ -z \"$DATABASE_URL\" ]; then\n    echo \"Error: DATABASE_URL environment variable is not set\"\n    exit 1\nfi\n\n# Navigate to server directory\ncd \"$(dirname \"$0\")/../../server\"\n\n# Show current status\necho \"Current migration status:\"\nnpx prisma migrate status\necho \"\"\n\n# Parse arguments\nDRY_RUN=true\nif [ \"$1\" = \"--apply\" ]; then\n    DRY_RUN=false\nfi\n\nif [ \"$DRY_RUN\" = true ]; then\n    echo \"DRY RUN MODE - showing changes without applying\"\n    echo \"Run with --apply to execute migrations\"\n    echo \"\"\n    echo \"Pending changes:\"\n    npx prisma migrate diff --from-migrations ./prisma/migrations --to-schema-datamodel ./prisma/schema.prisma || true\nelse\n    echo \"APPLYING MIGRATIONS...\"\n    npx prisma migrate deploy\n    echo \"\"\n    echo \"Migration complete. New status:\"\n    npx prisma migrate status\nfi\n```\n\n**Update deploy workflow to include migration step:**\nAdd before deployment triggers:\n```yaml\n      - name: Run database migrations\n        env:\n          DATABASE_URL: ${{ secrets.DATABASE_URL }}\n        run: |\n          cd server\n          npm ci\n          npx prisma generate\n          npx prisma migrate deploy\n```",
        "testStrategy": "1. Test dry run mode shows pending migrations without applying\n2. Test migrate deploy in staging environment\n3. Verify migration rollback with Prisma migrate down (manual)\n4. Test migration failure handling (create intentionally failing migration)\n5. Verify environment protection requires approval\n6. Test notification webhooks\n7. Run migration with no pending changes (should be no-op)",
        "priority": "high",
        "dependencies": [
          "21"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create GitHub Actions migrate.yml workflow file",
            "description": "Create `.github/workflows/migrate.yml` with workflow_dispatch trigger, dry_run input parameter (choice: true/false), and production environment configuration.",
            "dependencies": [],
            "details": "Set up the workflow structure with:\n- `workflow_dispatch` trigger with `dry_run` input (boolean choice, default: 'true')\n- Job named `migrate` running on `ubuntu-latest`\n- Environment set to `production` for approval requirement\n- Steps for checkout, Node.js 20 setup with npm cache\n- Dependencies installation: `cd server && npm ci`\n- Prisma client generation: `cd server && npx prisma generate`\n\nReference existing workflows in `.github/workflows/` for consistency with project patterns. Ensure DATABASE_URL is read from secrets.",
            "status": "pending",
            "testStrategy": "1. Push workflow file to repository\n2. Navigate to Actions tab and manually trigger workflow\n3. Verify dry_run parameter appears with true/false options\n4. Confirm production environment protection is active (should require approval)\n5. Test with dry_run=true first (safe mode)",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement dry run mode in migrate workflow",
            "description": "Add workflow steps that show pending migrations without applying them when dry_run=true using Prisma migrate status and migrate diff commands.",
            "dependencies": [
              1
            ],
            "details": "Add conditional step with `if: inputs.dry_run == 'true'`:\n- Run `npx prisma migrate status` to show current migration state\n- Run `npx prisma migrate diff --from-migrations ./prisma/migrations --to-schema-datamodel ./prisma/schema.prisma` to preview pending changes\n- Include clear output formatting with echo statements\n- Set DATABASE_URL from secrets: `${{ secrets.DATABASE_URL }}`\n\nThis step must execute in the `server` directory and handle cases where no migrations are pending gracefully.",
            "status": "pending",
            "testStrategy": "1. Create a test schema change in server/prisma/schema.prisma\n2. Trigger workflow with dry_run=true\n3. Verify output shows pending migrations without applying\n4. Confirm DATABASE_URL connection works\n5. Check that no actual database changes occurred",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement migration execution step with error handling",
            "description": "Add workflow step that runs actual migrations when dry_run=false using Prisma migrate deploy with proper error handling and status reporting.",
            "dependencies": [
              2
            ],
            "details": "Add conditional step with `if: inputs.dry_run == 'false'`:\n- Run `npx prisma migrate deploy` to apply migrations\n- Include pre-execution echo: \"Running migrations...\"\n- Post-execution confirmation: \"Migration complete!\"\n- Run `npx prisma migrate status` after deployment to verify\n- Set DATABASE_URL from secrets\n- Ensure proper error propagation (set -e behavior)\n\nExecute in `server` directory. The step should fail the workflow if migration fails, triggering failure notifications.",
            "status": "pending",
            "testStrategy": "1. Set up test database with DATABASE_URL secret\n2. Create a valid migration in server/prisma/migrations/\n3. Trigger workflow with dry_run=false\n4. Verify migration applies successfully\n5. Check migration status shows all migrations applied\n6. Test with invalid migration to confirm error handling works",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Add Discord notification steps to migrate workflow",
            "description": "Implement success and failure notification steps that send messages to Discord webhook for migration results.",
            "dependencies": [
              3
            ],
            "details": "Add two notification steps:\n\n**Success notification** (`if: success() && inputs.dry_run == 'false'`):\n- POST to `${{ secrets.DISCORD_WEBHOOK_URL }}`\n- Message: \"✅ Database migration completed successfully\"\n- Include `|| true` to prevent notification failures from failing workflow\n\n**Failure notification** (`if: failure()`):\n- POST to `${{ secrets.DISCORD_WEBHOOK_URL }}`\n- Message: \"❌ Database migration failed! Check GitHub Actions logs.\"\n- Include workflow run URL if possible\n- Include `|| true` for graceful degradation\n\nUse curl with Content-Type: application/json.",
            "status": "pending",
            "testStrategy": "1. Set DISCORD_WEBHOOK_URL secret in repository\n2. Trigger successful migration (dry_run=false)\n3. Verify success message appears in Discord\n4. Create intentionally failing migration\n5. Trigger workflow and verify failure notification\n6. Test with invalid webhook URL to ensure workflow doesn't fail",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Create migration helper script scripts/deploy/migrate.sh",
            "description": "Create bash script for manual migration execution with dry-run mode, status checking, and proper error handling for local and CI use.",
            "dependencies": [],
            "details": "Create `scripts/deploy/migrate.sh`:\n- Shebang: `#!/bin/bash` with `set -e`\n- Header with timestamp: `date -u +%Y-%m-%dT%H:%M:%SZ`\n- Check DATABASE_URL is set (exit 1 if missing)\n- Navigate to server directory: `cd \"$(dirname \"$0\")/../../server\"`\n- Show current status: `npx prisma migrate status`\n- Parse `--apply` argument (default: dry run)\n- Dry run: show pending changes with `prisma migrate diff`\n- Apply mode: run `npx prisma migrate deploy` and show final status\n- Make executable: `chmod +x scripts/deploy/migrate.sh`\n\nFollows pattern from deployment-infrastructure-prd.md example.",
            "status": "pending",
            "testStrategy": "1. Run `./scripts/deploy/migrate.sh` without DATABASE_URL (should error)\n2. Export test DATABASE_URL\n3. Run `./scripts/deploy/migrate.sh` (dry run) - should show status\n4. Create test migration\n5. Run `./scripts/deploy/migrate.sh --apply` (should apply)\n6. Verify migration was applied: `cd server && npx prisma migrate status`",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Integrate migration step into deploy workflow",
            "description": "Add database migration execution to the deployment workflow (.github/workflows/deploy.yml) before application deployment triggers.",
            "dependencies": [
              1,
              3,
              5
            ],
            "details": "Update `.github/workflows/deploy.yml` (created in task 24):\n- Add migration step after checkout/setup, before Coolify webhook\n- Position: after Node.js setup, before deployment trigger\n- Step name: \"Run database migrations\"\n- Set DATABASE_URL from secrets\n- Commands:\n  ```\n  cd server\n  npm ci\n  npx prisma generate\n  npx prisma migrate deploy\n  ```\n- Ensure this step fails the workflow if migrations fail\n- Add comment explaining this runs before deployment\n\nReference task 24 implementation for integration point.",
            "status": "pending",
            "testStrategy": "1. Review task 24's deploy.yml structure\n2. Add migration step in correct position\n3. Create test migration in server/prisma/migrations/\n4. Trigger deploy workflow (manual dispatch)\n5. Verify migration runs before Coolify webhook\n6. Check logs show migration execution\n7. Confirm failed migration prevents deployment",
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Break down into: 1) Create migrate.yml workflow with workflow_dispatch trigger and dry_run parameter, 2) Configure production environment protection requiring approval, 3) Implement dry run mode that shows pending migrations without applying (prisma migrate status, prisma migrate diff), 4) Implement actual migration execution (prisma migrate deploy) with proper error handling, 5) Create migration helper script (scripts/deploy/migrate.sh) for manual execution, 6) Add Discord notifications and integrate migration step into deploy workflow. Test extensively with test database first.",
        "updatedAt": "2025-12-18T08:56:41.600Z"
      },
      {
        "id": 26,
        "title": "Implement Structured Logging for Backend",
        "description": "Add JSON structured logging to the Express backend with correlation IDs, proper log levels, and sensitive data redaction.",
        "details": "**Install dependencies in server/:**\n```bash\nnpm install pino pino-http @types/pino-http\n```\n\n**Create `server/src/config/logger.ts`:**\n```typescript\nimport pino from 'pino';\nimport { randomUUID } from 'crypto';\n\nconst isProduction = process.env.NODE_ENV === 'production';\n\nexport const logger = pino({\n  level: process.env.LOG_LEVEL || (isProduction ? 'info' : 'debug'),\n  formatters: {\n    level: (label) => ({ level: label }),\n  },\n  timestamp: pino.stdTimeFunctions.isoTime,\n  redact: {\n    paths: [\n      'req.headers.authorization',\n      'req.body.password',\n      'req.body.token',\n      'res.headers[\"set-cookie\"]',\n      '*.password',\n      '*.token',\n      '*.jwt',\n      '*.secret',\n    ],\n    censor: '[REDACTED]',\n  },\n  transport: isProduction ? undefined : {\n    target: 'pino-pretty',\n    options: {\n      colorize: true,\n      translateTime: 'SYS:standard',\n    },\n  },\n});\n\nexport const generateCorrelationId = (): string => randomUUID();\n\nexport type Logger = typeof logger;\n```\n\n**Create `server/src/middleware/requestLogger.ts`:**\n```typescript\nimport pinoHttp from 'pino-http';\nimport { logger, generateCorrelationId } from '../config/logger';\nimport { Request, Response } from 'express';\n\nexport const requestLogger = pinoHttp({\n  logger,\n  genReqId: (req: Request) => {\n    const existingId = req.headers['x-correlation-id'];\n    return (existingId as string) || generateCorrelationId();\n  },\n  customProps: (req: Request) => ({\n    correlationId: req.id,\n    service: 'nutri-backend',\n    environment: process.env.NODE_ENV,\n  }),\n  customLogLevel: (_req: Request, res: Response, err?: Error) => {\n    if (res.statusCode >= 500 || err) return 'error';\n    if (res.statusCode >= 400) return 'warn';\n    return 'info';\n  },\n  customSuccessMessage: (req: Request, res: Response) => {\n    return `${req.method} ${req.url} completed with ${res.statusCode}`;\n  },\n  customErrorMessage: (_req: Request, res: Response, err: Error) => {\n    return `Request failed: ${err.message}`;\n  },\n  serializers: {\n    req: (req) => ({\n      method: req.method,\n      url: req.url,\n      query: req.query,\n      params: req.params,\n      // Don't log body by default - too verbose\n    }),\n    res: (res) => ({\n      statusCode: res.statusCode,\n    }),\n  },\n});\n```\n\n**Update `server/src/index.ts`:**\n```typescript\nimport { requestLogger } from './middleware/requestLogger';\nimport { logger } from './config/logger';\n\n// Add after cors middleware\napp.use(requestLogger);\n\n// Update server start log\nif (process.env.NODE_ENV !== 'test') {\n  app.listen(PORT, () => {\n    logger.info({ port: PORT, env: config.nodeEnv }, 'Server started');\n  });\n}\n```\n\n**Update error handler to use logger:**\n```typescript\nimport { logger } from '../config/logger';\n\n// In errorHandler middleware:\nlogger.error({\n  err,\n  correlationId: req.id,\n  statusCode,\n  path: req.path,\n  method: req.method,\n}, 'Request error');\n```\n\n**Add pino-pretty for dev (devDependency):**\n```bash\nnpm install -D pino-pretty\n```",
        "testStrategy": "1. Start server and verify JSON log output in production mode\n2. Verify correlation ID appears in logs and response headers\n3. Test password/token redaction - should show [REDACTED]\n4. Verify log levels work (DEBUG shows more than INFO)\n5. Test error logging includes stack trace\n6. Verify pino-pretty works in development\n7. Test high-volume requests don't cause log overflow\n8. Verify logs are compatible with common log aggregators (JSON format)",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Install pino dependencies and configure logger with redaction",
            "description": "Install pino, pino-http, @types/pino-http, and pino-pretty (dev). Create server/src/config/logger.ts with structured logging configuration including sensitive data redaction, correlation ID generation, and environment-based formatting.",
            "dependencies": [],
            "details": "Run `npm install pino pino-http @types/pino-http` and `npm install -D pino-pretty` in server/ directory. Create server/src/config/logger.ts implementing: pino instance with level based on LOG_LEVEL env var (default 'info' production, 'debug' development), ISO timestamp formatting, redaction paths for sensitive fields (authorization headers, password, token, jwt, secret fields), correlation ID generator using randomUUID from crypto, pino-pretty transport for development (colorized, human-readable), and export logger instance with TypeScript type. Test locally that logger initializes without errors.",
            "status": "pending",
            "testStrategy": "Verify pino packages are in package.json dependencies. Import logger in a test file and verify it doesn't throw errors. Check that redaction paths are correctly configured in the pino config object.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create requestLogger middleware with custom log levels",
            "description": "Implement pino-http middleware in server/src/middleware/requestLogger.ts that integrates structured logging with Express, handles correlation IDs from headers, and applies custom log levels based on HTTP status codes.",
            "dependencies": [
              1
            ],
            "details": "Create server/src/middleware/requestLogger.ts using pinoHttp from pino-http. Configure genReqId to extract x-correlation-id header or generate new UUID, add customProps with correlationId/service/environment metadata, implement customLogLevel function (500+ or error → 'error', 400-499 → 'warn', else 'info'), add customSuccessMessage and customErrorMessage formatters, configure serializers for req (method, url, query, params only) and res (statusCode only) to avoid verbose body logging. Export requestLogger middleware function.",
            "status": "pending",
            "testStrategy": "Unit test the middleware with mock req/res objects. Verify correlation ID is extracted from header or generated. Test that different status codes (200, 400, 500) produce correct log levels. Verify serializers omit sensitive data like request body.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate requestLogger into Express app and replace console.log",
            "description": "Update server/src/index.ts to use the requestLogger middleware and replace all console.log statements with structured logger calls throughout the backend codebase.",
            "dependencies": [
              2
            ],
            "details": "In server/src/index.ts: import requestLogger from './middleware/requestLogger' and logger from './config/logger', add app.use(requestLogger) after CORS middleware but before routes, replace server start console.log with logger.info({ port: PORT, env: config.nodeEnv }, 'Server started'). Search codebase for all console.log/console.error statements in server/src and replace with appropriate logger.info/logger.error/logger.warn/logger.debug calls. Verify NODE_ENV=production produces JSON logs and NODE_ENV=development produces colorized pino-pretty output.",
            "status": "pending",
            "testStrategy": "Start server in development mode and verify human-readable colorized logs. Set NODE_ENV=production and verify JSON structured output. Make test requests and confirm correlation IDs appear in logs. Verify no console.log statements remain in server/src (use grep to check).",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Update errorHandler middleware to use structured logging",
            "description": "Modify the existing error handling middleware in server/src/middleware/errorHandler.ts to use structured logging with correlation IDs, proper error serialization, and contextual request information.",
            "dependencies": [
              2
            ],
            "details": "In server/src/middleware/errorHandler.ts: import logger from '../config/logger', replace any console.error calls with logger.error(), include error object, correlationId (req.id), statusCode, path (req.path), method (req.method), and error message in log context. Ensure error stack traces are included in development but sanitized in production. Test with various error scenarios (validation errors, auth errors, 500 errors) to verify proper logging with correlation IDs. Verify sensitive data (passwords, tokens) are redacted even in error logs.",
            "status": "pending",
            "testStrategy": "Trigger various error scenarios (invalid input, unauthorized access, server errors) and verify structured error logs appear with correlation IDs. Check that error stack traces are present in development logs. Verify sensitive fields in error contexts are redacted. Test that correlation ID in error log matches the one from request log.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Split into: 1) Install pino, pino-http, and configure logger in server/src/config/logger.ts with proper redaction and correlation ID generation, 2) Create requestLogger middleware that integrates with existing Express app, logs requests/responses with custom log levels, 3) Update server/src/index.ts to use requestLogger and replace console.log with structured logger, 4) Update existing errorHandler middleware to use structured logging. Test that logs are JSON in production and human-readable in development.",
        "updatedAt": "2025-12-18T08:57:56.878Z"
      },
      {
        "id": 27,
        "title": "Implement Structured Logging for ML Service",
        "description": "Add JSON structured logging to the FastAPI ML service with correlation IDs, proper log levels, and sensitive data redaction.",
        "details": "**Update `ml-service/requirements.txt`:**\n```\nstructlog>=24.1.0\npython-json-logger>=2.0.7\n```\n\n**Create `ml-service/app/core/logging.py`:**\n```python\nimport structlog\nimport logging\nimport sys\nimport uuid\nfrom typing import Any\nfrom contextvars import ContextVar\n\n# Correlation ID context\ncorrelation_id_var: ContextVar[str] = ContextVar('correlation_id', default='')\n\ndef get_correlation_id() -> str:\n    return correlation_id_var.get() or str(uuid.uuid4())\n\ndef set_correlation_id(correlation_id: str) -> None:\n    correlation_id_var.set(correlation_id)\n\n# Sensitive data redaction\nSENSITIVE_KEYS = {'password', 'token', 'secret', 'authorization', 'api_key'}\n\ndef redact_sensitive_data(_, __, event_dict: dict) -> dict:\n    def redact(obj: Any) -> Any:\n        if isinstance(obj, dict):\n            return {\n                k: '[REDACTED]' if k.lower() in SENSITIVE_KEYS else redact(v)\n                for k, v in obj.items()\n            }\n        elif isinstance(obj, list):\n            return [redact(item) for item in obj]\n        return obj\n    \n    return redact(event_dict)\n\ndef add_service_context(_, __, event_dict: dict) -> dict:\n    event_dict['service'] = 'nutri-ml-service'\n    event_dict['correlation_id'] = get_correlation_id()\n    return event_dict\n\ndef configure_logging(environment: str = 'development') -> None:\n    processors = [\n        structlog.contextvars.merge_contextvars,\n        structlog.processors.add_log_level,\n        structlog.processors.TimeStamper(fmt='iso'),\n        add_service_context,\n        redact_sensitive_data,\n    ]\n    \n    if environment == 'production':\n        processors.append(structlog.processors.JSONRenderer())\n    else:\n        processors.append(structlog.dev.ConsoleRenderer())\n    \n    structlog.configure(\n        processors=processors,\n        wrapper_class=structlog.make_filtering_bound_logger(logging.INFO),\n        context_class=dict,\n        logger_factory=structlog.PrintLoggerFactory(),\n        cache_logger_on_first_use=True,\n    )\n\ndef get_logger(name: str = __name__) -> structlog.BoundLogger:\n    return structlog.get_logger(name)\n```\n\n**Create `ml-service/app/middleware/logging.py`:**\n```python\nimport time\nimport uuid\nfrom fastapi import Request, Response\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom app.core.logging import get_logger, set_correlation_id, get_correlation_id\n\nlogger = get_logger(__name__)\n\nclass LoggingMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request: Request, call_next) -> Response:\n        # Get or generate correlation ID\n        correlation_id = request.headers.get('x-correlation-id', str(uuid.uuid4()))\n        set_correlation_id(correlation_id)\n        \n        start_time = time.time()\n        \n        # Log request\n        logger.info(\n            'request_started',\n            method=request.method,\n            path=request.url.path,\n            query=str(request.query_params),\n        )\n        \n        try:\n            response = await call_next(request)\n            \n            # Add correlation ID to response\n            response.headers['x-correlation-id'] = correlation_id\n            \n            # Log response\n            duration_ms = (time.time() - start_time) * 1000\n            log_level = 'error' if response.status_code >= 500 else 'warn' if response.status_code >= 400 else 'info'\n            getattr(logger, log_level)(\n                'request_completed',\n                method=request.method,\n                path=request.url.path,\n                status_code=response.status_code,\n                duration_ms=round(duration_ms, 2),\n            )\n            \n            return response\n        except Exception as e:\n            logger.exception(\n                'request_failed',\n                method=request.method,\n                path=request.url.path,\n                error=str(e),\n            )\n            raise\n```\n\n**Update `ml-service/app/main.py`:**\n```python\nfrom app.core.logging import configure_logging, get_logger\nfrom app.middleware.logging import LoggingMiddleware\nfrom app.config import settings\n\n# Configure logging on startup\nconfigure_logging(settings.ENVIRONMENT)\nlogger = get_logger(__name__)\n\napp = FastAPI(...)\napp.add_middleware(LoggingMiddleware)\n\n@app.on_event('startup')\nasync def startup():\n    logger.info('ml_service_started', port=8000, environment=settings.ENVIRONMENT)\n```",
        "testStrategy": "1. Start ML service and verify JSON log output in production mode\n2. Verify correlation ID passed from backend appears in ML logs\n3. Test sensitive data redaction in request logs\n4. Verify log levels (debug, info, warn, error)\n5. Test exception logging includes stack trace\n6. Verify correlation ID in response headers\n7. Test dev console renderer in development mode\n8. Verify logs parse correctly with jq",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Install dependencies and create core logging module",
            "description": "Add structlog and python-json-logger to requirements.txt, then create app/core/logging.py with structured logging configuration, correlation ID context variables, and sensitive data redaction.",
            "dependencies": [],
            "details": "1. Update ml-service/requirements.txt with structlog>=24.1.0 and python-json-logger>=2.0.7\n2. Create ml-service/app/core/logging.py implementing:\n   - ContextVar for correlation IDs (correlation_id_var)\n   - get_correlation_id() and set_correlation_id() functions\n   - SENSITIVE_KEYS set for redaction (password, token, secret, authorization, api_key)\n   - redact_sensitive_data() processor that recursively redacts dict/list values\n   - add_service_context() processor adding service name and correlation_id\n   - configure_logging() with environment-based processors (JSONRenderer for prod, ConsoleRenderer for dev)\n   - get_logger() factory function\n3. Install dependencies: cd ml-service && pip install -r requirements.txt\n4. Test logging configuration imports without errors",
            "status": "pending",
            "testStrategy": "Run Python REPL: from app.core.logging import configure_logging, get_logger; configure_logging('development'); logger = get_logger(); logger.info('test', password='secret') - verify console output shows '[REDACTED]' for password field",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create logging middleware for FastAPI",
            "description": "Implement FastAPI middleware in app/middleware/logging.py to capture request/response with timing metrics and propagate correlation IDs through headers.",
            "dependencies": [
              1
            ],
            "details": "1. Create ml-service/app/middleware/ directory if not exists\n2. Create ml-service/app/middleware/logging.py implementing LoggingMiddleware(BaseHTTPMiddleware):\n   - Extract or generate correlation ID from x-correlation-id header\n   - Call set_correlation_id() to set context\n   - Log request_started with method, path, query params\n   - Measure request duration using time.time()\n   - Add x-correlation-id to response headers\n   - Log request_completed with status_code and duration_ms\n   - Use dynamic log level based on status code (error >=500, warn >=400, info otherwise)\n   - Log request_failed with exception details on errors\n3. Test middleware with mock Request/Response objects",
            "status": "pending",
            "testStrategy": "Unit test LoggingMiddleware.dispatch() with mock request/call_next, verify: 1) correlation ID extracted from headers, 2) correlation ID added to response, 3) request_started and request_completed logged with correct fields, 4) exception handling logs request_failed",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate logging into FastAPI application",
            "description": "Update ml-service/app/main.py to configure structured logging on startup and register the logging middleware.",
            "dependencies": [
              2
            ],
            "details": "1. Update ml-service/app/main.py imports:\n   - from app.core.logging import configure_logging, get_logger\n   - from app.middleware.logging import LoggingMiddleware\n   - from app.config import settings (verify ENVIRONMENT exists in config)\n2. Add configure_logging(settings.ENVIRONMENT) before app = FastAPI(...)\n3. Create logger = get_logger(__name__) after configuration\n4. Add app.add_middleware(LoggingMiddleware) after app instantiation\n5. Update @app.on_event('startup') to log ml_service_started with port and environment\n6. Start ML service and verify JSON logs in production mode: ENVIRONMENT=production uvicorn app.main:app\n7. Verify console logs in dev mode: ENVIRONMENT=development uvicorn app.main:app",
            "status": "pending",
            "testStrategy": "1. Start service with ENVIRONMENT=production, send test request, verify JSON log output with correlation_id field\n2. Start service with ENVIRONMENT=development, verify colored console output\n3. Send request with x-correlation-id header, verify same ID appears in response header and logs\n4. Test /health endpoint logs at info level",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Replace existing logging calls with structured logger",
            "description": "Audit and replace all existing Python logging.getLogger() calls throughout the ML service codebase with structlog get_logger(), ensuring consistent structured logging.",
            "dependencies": [
              3
            ],
            "details": "1. Find all existing logging usage: grep -r 'logging.getLogger\\|logger.info\\|logger.error\\|logger.debug\\|logger.warning' ml-service/app/ --include='*.py'\n2. Replace logging imports with: from app.core.logging import get_logger\n3. Replace logger = logging.getLogger(__name__) with logger = get_logger(__name__)\n4. Update logging calls to use structured format:\n   - Old: logger.info(f'Processing {metric_type}')\n   - New: logger.info('processing_metric', metric_type=metric_type)\n5. Focus on high-traffic files: app/main.py, app/routers/, app/services/, app/ml_models/\n6. Add context to error logs: logger.error('operation_failed', error=str(e), user_id=user_id)\n7. Test sensitive data redaction in actual service logs (password, token fields)",
            "status": "pending",
            "testStrategy": "1. grep for old logging patterns, verify none remain\n2. Start service and trigger various endpoints (predictions, health metrics)\n3. Verify all logs use JSON format in production mode\n4. Test exception logging includes stack traces: logger.exception('error_name')\n5. Verify sensitive data redaction by logging test payloads with password/token fields\n6. Check correlation IDs persist across service boundaries",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Break down into: 1) Install structlog and python-json-logger, create app/core/logging.py with configuration, redaction, and correlation ID context vars, 2) Create logging middleware (app/middleware/logging.py) that captures request/response with timing and propagates correlation IDs, 3) Update app/main.py to configure logging on startup and add middleware, 4) Replace existing logging calls throughout ML service with structured logger. Test JSON output in production mode and console rendering in dev.",
        "updatedAt": "2025-12-18T09:00:19.407Z"
      },
      {
        "id": 28,
        "title": "Create Server Setup Script for Hetzner",
        "description": "Create an idempotent setup script that provisions a new Hetzner server with all required software, security configurations, and Coolify installation.",
        "details": "**Create `scripts/deploy/setup-server.sh`:**\n```bash\n#!/bin/bash\nset -e\n\n# =============================================================================\n# Nutri Server Setup Script for Hetzner CX32\n# Run as root on fresh Ubuntu 22.04 LTS\n# =============================================================================\n\necho \"=== Nutri Production Server Setup ===\"\necho \"Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)\"\necho \"\"\n\n# Colors for output\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nYELLOW='\\033[1;33m'\nNC='\\033[0m' # No Color\n\ninfo() { echo -e \"${GREEN}[INFO]${NC} $1\"; }\nwarn() { echo -e \"${YELLOW}[WARN]${NC} $1\"; }\nerror() { echo -e \"${RED}[ERROR]${NC} $1\"; exit 1; }\n\n# =============================================================================\n# System Updates and Basic Security\n# =============================================================================\n\ninfo \"Updating system packages...\"\napt-get update -qq\nDEBIAN_FRONTEND=noninteractive apt-get upgrade -y -qq\n\ninfo \"Installing essential packages...\"\napt-get install -y -qq \\\n    curl \\\n    wget \\\n    git \\\n    ufw \\\n    fail2ban \\\n    unattended-upgrades \\\n    apt-listchanges \\\n    htop \\\n    ncdu\n\n# =============================================================================\n# Security Hardening\n# =============================================================================\n\ninfo \"Configuring firewall (UFW)...\"\nufw --force reset\nufw default deny incoming\nufw default allow outgoing\nufw allow 22/tcp comment 'SSH'\nufw allow 80/tcp comment 'HTTP'\nufw allow 443/tcp comment 'HTTPS'\nufw --force enable\nufw status verbose\n\ninfo \"Configuring fail2ban...\"\ncat > /etc/fail2ban/jail.local << 'EOF'\n[DEFAULT]\nbantime = 3600\nfindtime = 600\nmaxretry = 5\n\n[sshd]\nenabled = true\nport = ssh\nfilter = sshd\nlogpath = /var/log/auth.log\nmaxretry = 3\nbantime = 86400\nEOF\nsystemctl restart fail2ban\nsystemctl enable fail2ban\n\ninfo \"Configuring SSH security...\"\nsed -i 's/#PermitRootLogin yes/PermitRootLogin prohibit-password/' /etc/ssh/sshd_config\nsed -i 's/#PasswordAuthentication yes/PasswordAuthentication no/' /etc/ssh/sshd_config\nsed -i 's/PasswordAuthentication yes/PasswordAuthentication no/' /etc/ssh/sshd_config\nsystemctl restart sshd\n\ninfo \"Enabling automatic security updates...\"\ndpkg-reconfigure -f noninteractive unattended-upgrades\ncat > /etc/apt/apt.conf.d/50unattended-upgrades << 'EOF'\nUnattended-Upgrade::Allowed-Origins {\n    \"${distro_id}:${distro_codename}-security\";\n};\nUnattended-Upgrade::AutoFixInterruptedDpkg \"true\";\nUnattended-Upgrade::Remove-Unused-Dependencies \"true\";\nUnattended-Upgrade::Automatic-Reboot \"false\";\nEOF\n\n# =============================================================================\n# Swap Configuration\n# =============================================================================\n\nif [ ! -f /swapfile ]; then\n    info \"Creating 4GB swap file...\"\n    fallocate -l 4G /swapfile\n    chmod 600 /swapfile\n    mkswap /swapfile\n    swapon /swapfile\n    echo '/swapfile none swap sw 0 0' >> /etc/fstab\n    echo 'vm.swappiness=10' >> /etc/sysctl.conf\n    sysctl -p\nelse\n    info \"Swap file already exists, skipping...\"\nfi\n\n# =============================================================================\n# Docker Installation (if not present)\n# =============================================================================\n\nif ! command -v docker &> /dev/null; then\n    info \"Installing Docker...\"\n    curl -fsSL https://get.docker.com | sh\n    systemctl enable docker\n    systemctl start docker\n    \n    # Docker cleanup cron job\n    cat > /etc/cron.daily/docker-cleanup << 'EOF'\n#!/bin/bash\ndocker system prune -af --volumes --filter \"until=168h\" > /dev/null 2>&1\nEOF\n    chmod +x /etc/cron.daily/docker-cleanup\nelse\n    info \"Docker already installed, skipping...\"\nfi\n\n# =============================================================================\n# Coolify Installation\n# =============================================================================\n\nif [ ! -d /data/coolify ]; then\n    info \"Installing Coolify...\"\n    curl -fsSL https://cdn.coollabs.io/coolify/install.sh | bash\n    info \"Coolify installed! Access at https://your-server-ip:8000\"\nelse\n    info \"Coolify already installed, skipping...\"\nfi\n\n# =============================================================================\n# Summary\n# =============================================================================\n\necho \"\"\necho \"=== Setup Complete ===\"\necho \"\"\ninfo \"Next steps:\"\necho \"1. Access Coolify at https://<server-ip>:8000\"\necho \"2. Create admin account and configure SSL\"\necho \"3. Set up GitHub webhook integration\"\necho \"4. Configure environment variables in Coolify\"\necho \"\"\ninfo \"Security status:\"\nufw status | head -10\necho \"\"\ninfo \"Resource usage:\"\nfree -h\ndf -h /\necho \"\"\ninfo \"Docker version:\"\ndocker --version\n```\n\n**Make executable:**\n```bash\nchmod +x scripts/deploy/setup-server.sh\n```",
        "testStrategy": "1. Test on fresh Ubuntu 22.04 VM (Hetzner or local)\n2. Verify idempotency - run script twice, second run should skip completed steps\n3. Check UFW rules: `ufw status verbose`\n4. Verify fail2ban: `fail2ban-client status sshd`\n5. Test SSH still works after config changes\n6. Verify swap: `free -h` shows 4GB swap\n7. Verify Docker: `docker run hello-world`\n8. Check Coolify accessible at :8000\n9. Verify auto-updates: `apt-config dump | grep -i unattended`",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create setup-server.sh with system updates and package installation",
            "description": "Create the initial setup script structure with shebang, error handling, color output functions, and implement system update and essential package installation section",
            "dependencies": [],
            "details": "Create scripts/deploy/setup-server.sh with executable permissions. Implement: 1) Script header with set -e for error handling, 2) Color output functions (info, warn, error), 3) System updates section using apt-get update and upgrade with quiet flags, 4) Essential packages installation (curl, wget, git, ufw, fail2ban, unattended-upgrades, apt-listchanges, htop, ncdu) using DEBIAN_FRONTEND=noninteractive for non-interactive installation. Ensure all output is properly formatted with timestamps and color-coded status messages.",
            "status": "pending",
            "testStrategy": "Run script on fresh Ubuntu 22.04 VM. Verify: 1) Script executes without errors, 2) All packages are installed: dpkg -l | grep -E '(curl|wget|git|ufw|fail2ban)', 3) System is updated to latest versions, 4) Color output displays correctly, 5) Error handling works by introducing intentional failure",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement security hardening (UFW, fail2ban, SSH)",
            "description": "Add comprehensive security configuration including firewall rules, intrusion prevention, and SSH hardening to protect the server from unauthorized access",
            "dependencies": [
              1
            ],
            "details": "Implement security hardening section: 1) UFW firewall configuration - reset existing rules, set default deny incoming/allow outgoing, allow ports 22 (SSH), 80 (HTTP), 443 (HTTPS) with comments, force enable UFW, 2) fail2ban configuration - create /etc/fail2ban/jail.local with sshd jail enabled (maxretry=3, bantime=86400, findtime=600), restart and enable fail2ban service, 3) SSH hardening - modify /etc/ssh/sshd_config to set PermitRootLogin=prohibit-password and PasswordAuthentication=no using sed, restart sshd service, 4) Automatic security updates - configure unattended-upgrades with security-only updates, auto-fix interrupted dpkg, remove unused dependencies, disable automatic reboot.",
            "status": "pending",
            "testStrategy": "After running script: 1) Verify UFW status: ufw status verbose (should show rules for ports 22, 80, 443), 2) Check fail2ban: fail2ban-client status sshd (should show jail is active), 3) Verify SSH config: grep -E '(PermitRootLogin|PasswordAuthentication)' /etc/ssh/sshd_config, 4) Test SSH connection still works with key-based auth, 5) Verify unattended-upgrades config: cat /etc/apt/apt.conf.d/50unattended-upgrades",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add swap configuration with idempotency checks",
            "description": "Implement swap file creation with proper size, permissions, and swappiness settings, ensuring the script can safely run multiple times without recreating swap",
            "dependencies": [
              2
            ],
            "details": "Implement swap configuration section with idempotency: 1) Check if /swapfile already exists using conditional [ ! -f /swapfile ], 2) If not exists: create 4GB swap file using fallocate -l 4G, set permissions to 600, run mkswap and swapon, add entry to /etc/fstab for persistence, 3) Configure vm.swappiness=10 in /etc/sysctl.conf for optimal performance (prefer RAM over swap), apply with sysctl -p, 4) If swap exists: skip creation and output info message. Ensure script outputs appropriate messages for both scenarios.",
            "status": "pending",
            "testStrategy": "Test idempotency: 1) Run script first time, verify swap created: free -h (should show 4G swap), 2) Check swappiness: cat /proc/sys/vm/swappiness (should be 10), 3) Verify fstab entry: grep swapfile /etc/fstab, 4) Run script second time, verify it skips swap creation (should see 'already exists' message), 5) Reboot VM and verify swap is still active after boot",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Docker installation with cleanup automation",
            "description": "Add Docker installation using official script with idempotency checks, configure automatic cleanup cron job to prevent disk space issues from old images and containers",
            "dependencies": [
              3
            ],
            "details": "Implement Docker installation section: 1) Check if Docker is already installed using command -v docker, 2) If not installed: download and run official Docker installation script (curl -fsSL https://get.docker.com | sh), enable and start Docker service using systemctl, 3) Create /etc/cron.daily/docker-cleanup script that runs 'docker system prune -af --volumes --filter until=168h' (removes unused resources older than 7 days), make cleanup script executable (chmod +x), 4) If Docker exists: skip installation and output info message. Ensure Docker daemon is running after installation.",
            "status": "pending",
            "testStrategy": "Test Docker setup: 1) Run script on system without Docker, verify installation: docker --version, 2) Check Docker service: systemctl status docker (should be active and enabled), 3) Verify cleanup cron exists: cat /etc/cron.daily/docker-cleanup, 4) Test cron script manually: /etc/cron.daily/docker-cleanup, 5) Run setup script again, verify it skips Docker installation, 6) Test Docker works: docker run hello-world",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Add Coolify installation and comprehensive summary output",
            "description": "Implement Coolify installation with idempotency and create detailed summary section showing security status, resource usage, and next steps for administrator",
            "dependencies": [
              4
            ],
            "details": "Implement final sections: 1) Coolify installation - check if /data/coolify directory exists, if not: download and execute Coolify install script (curl -fsSL https://cdn.coollabs.io/coolify/install.sh | bash), output access URL, if exists: skip and inform, 2) Create comprehensive summary output section with: header banner, next steps list (access Coolify at https://<server-ip>:8000, create admin account, configure SSL, set up GitHub webhooks, configure environment variables), security status (UFW rules via 'ufw status | head -10'), resource usage (RAM via 'free -h', disk via 'df -h /'), Docker version. Use color-coded info messages throughout summary.",
            "status": "pending",
            "testStrategy": "Complete integration test: 1) Run full script on fresh Ubuntu 22.04 VM, 2) Verify all sections complete successfully, 3) Check summary output contains all required information, 4) Access Coolify at https://<server-ip>:8000 (should show login page), 5) Run script second time (idempotency test) - all sections should skip appropriately with 'already exists' messages, 6) Verify final state: UFW active, fail2ban running, swap enabled, Docker installed, Coolify accessible, 7) Check script execution time (should complete in <10 minutes on CX32)",
            "parentId": "undefined"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Divide into: 1) Create scripts/deploy/setup-server.sh with system updates and essential package installation, 2) Implement security hardening section (UFW firewall rules, fail2ban configuration, SSH hardening), 3) Add swap configuration with proper swappiness settings, 4) Implement Docker installation with cleanup cron job, 5) Add Coolify installation and create comprehensive summary output. Test script on fresh Ubuntu 22.04 VM and verify idempotency.",
        "updatedAt": "2025-12-18T09:03:05.861Z"
      },
      {
        "id": 29,
        "title": "Create Deployment Documentation",
        "description": "Create comprehensive deployment documentation including architecture diagrams, setup guides, troubleshooting, and runbooks for daily operations.",
        "details": "**Create `docs/deployment/README.md`:**\n```markdown\n# Nutri Deployment Documentation\n\n## Architecture Overview\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                        GitHub Repository                         │\n│  (push to main) ──────────────────────────────────────────────► │\n└─────────────────────────────────────────────────────────────────┘\n                                    │\n                                    ▼\n┌─────────────────────────────────────────────────────────────────┐\n│                       GitHub Actions CI/CD                       │\n│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐          │\n│  │ Lint & Test  │──│ Build Images │──│ Push to GHCR │          │\n│  └──────────────┘  └──────────────┘  └──────────────┘          │\n└─────────────────────────────────────────────────────────────────┘\n                                    │ (webhook)\n                                    ▼\n┌─────────────────────────────────────────────────────────────────┐\n│              Hetzner CX32 (4 vCPU, 8GB RAM)                     │\n│  ┌─────────────────────────────────────────────────────────┐   │\n│  │                    Coolify (PaaS Layer)                  │   │\n│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐      │   │\n│  │  │   Traefik   │  │  Backend    │  │ ML Service  │      │   │\n│  │  │ (SSL/Proxy) │  │  (Express)  │  │  (FastAPI)  │      │   │\n│  │  └─────────────┘  └─────────────┘  └─────────────┘      │   │\n│  └─────────────────────────────────────────────────────────┘   │\n└─────────────────────────────────────────────────────────────────┘\n         │                      │\n         ▼                      ▼\n┌────────────────────┐  ┌────────────────────┐\n│   Supabase         │  │   Upstash Redis    │\n│   (PostgreSQL)     │  │   (Cache)          │\n└────────────────────┘  └────────────────────┘\n```\n\n## Cost Breakdown\n\n| Service | Provider | Cost/Month |\n|---------|----------|------------|\n| VPS (CX32) | Hetzner | €5.49 |\n| Backups | Hetzner | €1.20 |\n| Database | Supabase Free | $0 |\n| Redis | Upstash Free | $0 |\n| **Total** | | **~$8/mo** |\n\n## Quick Links\n\n- [Initial Setup Guide](./SETUP.md)\n- [Troubleshooting Guide](./TROUBLESHOOTING.md)\n- [Operations Runbook](./RUNBOOK.md)\n- [Environment Variables](./ENVIRONMENT.md)\n```\n\n**Create `docs/deployment/SETUP.md`:**\n```markdown\n# Initial Setup Guide\n\n## Prerequisites\n\n- Hetzner Cloud account\n- Supabase account\n- Upstash account\n- GitHub repository with admin access\n- Domain name (optional but recommended)\n\n## Step 1: Provision Hetzner Server\n\n1. Log into [Hetzner Cloud Console](https://console.hetzner.cloud/)\n2. Create new project \"nutri-production\"\n3. Add SSH key to project\n4. Create server:\n   - Location: Nearest to users (e.g., Helsinki, Frankfurt)\n   - Image: Ubuntu 22.04\n   - Type: CX32 (4 vCPU, 8GB RAM)\n   - Networking: Public IPv4\n   - SSH Key: Select your key\n   - Backups: Enable ($1.20/mo)\n\n## Step 2: Run Server Setup Script\n\n```bash\nssh root@<server-ip>\ncurl -fsSL https://raw.githubusercontent.com/your-repo/nutri/master/scripts/deploy/setup-server.sh | bash\n```\n\n## Step 3: Configure Coolify\n\n1. Access Coolify at `http://<server-ip>:8000`\n2. Create admin account\n3. Settings > General:\n   - Set custom domain for Coolify dashboard\n   - Enable SSL via Let's Encrypt\n4. Add new source: GitHub (OAuth or Deploy Key)\n\n[Continue with detailed steps...]\n```\n\n**Create `docs/deployment/TROUBLESHOOTING.md`:**\n```markdown\n# Troubleshooting Guide\n\n## Common Issues\n\n### Container won't start\n\n**Symptoms:** Container status shows \"Exited\" or \"Restarting\"\n\n**Solutions:**\n1. Check logs: `docker logs <container-name>`\n2. Verify environment variables in Coolify\n3. Check health endpoint: `curl http://localhost:3000/health`\n4. Verify database connection\n\n### Database connection failed\n\n**Symptoms:** \"Connection refused\" or timeout errors\n\n**Solutions:**\n1. Verify DATABASE_URL format\n2. Check Supabase dashboard for connection limits\n3. Ensure IP whitelist includes server IP\n...\n```\n\n**Create `docs/deployment/RUNBOOK.md`:**\n```markdown\n# Operations Runbook\n\n## Daily Operations\n\n### Check System Health\n```bash\ncurl https://api.nutri.app/health\n```\n\n### View Logs\n```bash\n# In Coolify dashboard, or:\ndocker logs -f nutri-backend --tail 100\n```\n\n## Emergency Procedures\n\n### Rollback Deployment\n\n1. Go to GitHub Actions\n2. Find last successful deploy workflow\n3. Click \"Re-run jobs\"\n\nOr manually:\n```bash\n# SSH to server\ndocker pull ghcr.io/repo/backend:<previous-sha>\n# Update image in Coolify\n```\n...\n```\n\n**Create `docs/deployment/ENVIRONMENT.md`:**\nDocument all environment variables with descriptions and examples.",
        "testStrategy": "1. Have team member follow SETUP.md on fresh account\n2. Verify all links work\n3. Test troubleshooting steps actually resolve issues\n4. Review runbook procedures with ops team\n5. Ensure architecture diagram matches actual deployment\n6. Verify cost breakdown is accurate",
        "priority": "medium",
        "dependencies": [
          "28"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create docs/deployment/README.md with architecture diagram and navigation",
            "description": "Create the main deployment documentation landing page with ASCII architecture diagram showing GitHub → CI/CD → Hetzner/Coolify → Supabase/Upstash flow, cost breakdown table, and quick links to other documentation files.",
            "dependencies": [],
            "details": "Create `docs/deployment/README.md` containing:\n1. ASCII architecture diagram showing complete deployment pipeline (GitHub → GitHub Actions → Hetzner CX32 → Coolify → Services)\n2. Cost breakdown table with Hetzner VPS (€5.49), Backups (€1.20), Supabase (free), Upstash (free), totaling ~$8/mo\n3. Quick links section referencing SETUP.md, TROUBLESHOOTING.md, RUNBOOK.md, and ENVIRONMENT.md\n4. Brief overview of the tech stack and deployment approach\n5. Use clear markdown formatting with proper headers and code blocks",
            "status": "pending",
            "testStrategy": "1. Verify ASCII diagram renders correctly in GitHub and VS Code\n2. Confirm all cost figures match infrastructure-costs.md\n3. Test that all quick links point to correct files (they don't need to exist yet)\n4. Review with team for accuracy and clarity",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Write docs/deployment/SETUP.md with complete setup instructions",
            "description": "Create comprehensive step-by-step setup guide covering Hetzner server provisioning, Coolify installation, service configuration, domain setup, SSL configuration, and GitHub integration.",
            "dependencies": [
              1
            ],
            "details": "Create `docs/deployment/SETUP.md` with:\n1. Prerequisites section (accounts needed, access requirements, domain name)\n2. Step 1: Hetzner server provisioning (location selection, CX32 specs, SSH key setup, backup enabling)\n3. Step 2: Server setup script execution (SSH connection, running setup-server.sh)\n4. Step 3: Coolify configuration (initial access, admin account, domain setup, SSL/Let's Encrypt)\n5. Step 4: GitHub source integration (OAuth or Deploy Key setup)\n6. Step 5: Supabase database setup (project creation, connection string, IP whitelist)\n7. Step 6: Upstash Redis setup (database creation, connection URL)\n8. Step 7: Service deployment in Coolify (environment variables, health checks, webhooks)\n9. Step 8: DNS configuration and SSL verification\n10. Include screenshots placeholders and troubleshooting tips inline",
            "status": "pending",
            "testStrategy": "1. Have team member follow guide on fresh Hetzner/Supabase/Upstash accounts\n2. Document any missing steps or unclear instructions\n3. Verify all commands execute successfully\n4. Confirm final deployment is accessible and healthy\n5. Time the setup process to estimate onboarding duration",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create docs/deployment/TROUBLESHOOTING.md with common issues and solutions",
            "description": "Document common deployment issues, their symptoms, root causes, and step-by-step solutions including container failures, database connection problems, health check failures, SSL certificate issues, and memory/resource problems.",
            "dependencies": [
              1
            ],
            "details": "Create `docs/deployment/TROUBLESHOOTING.md` with:\n1. Container won't start (symptoms: Exited/Restarting status; solutions: check logs, verify env vars, test health endpoint, verify DB connection)\n2. Database connection failed (symptoms: connection refused/timeout; solutions: verify DATABASE_URL format, check Supabase limits, IP whitelist, connection pooling)\n3. Health check failures (symptoms: container marked unhealthy; solutions: verify /health endpoint, check dependencies, review logs)\n4. SSL certificate issues (symptoms: HTTPS not working; solutions: Let's Encrypt rate limits, DNS propagation, Traefik config)\n5. Out of memory errors (symptoms: container OOMKilled; solutions: check memory limits, optimize queries, review ML model sizes)\n6. Coolify webhook not triggering (symptoms: no deployment after push; solutions: verify webhook URL, check GitHub Actions logs, test webhook manually)\n7. Each issue includes: symptoms, possible causes, diagnostic commands, step-by-step solutions, prevention tips",
            "status": "pending",
            "testStrategy": "1. Intentionally create each issue in staging environment\n2. Follow documented solutions to verify they work\n3. Measure time to resolution for each issue\n4. Get feedback from team on clarity and completeness\n5. Add any additional issues discovered during testing",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Write docs/deployment/RUNBOOK.md with operations procedures",
            "description": "Create operations runbook with daily health checks, log viewing, monitoring procedures, emergency rollback steps, scaling procedures, backup verification, and incident response workflows.",
            "dependencies": [
              1
            ],
            "details": "Create `docs/deployment/RUNBOOK.md` with:\n1. Daily Operations: health check commands (curl health endpoints), log viewing (Coolify dashboard + docker logs), monitoring metrics to track\n2. Emergency Procedures: rollback deployment (GitHub Actions re-run + manual docker image swap), service restart (Coolify UI + docker commands), database failover (Supabase dashboard)\n3. Scaling Procedures: vertical scaling (Hetzner server resize), horizontal scaling (Coolify multi-container), database connection pool adjustments\n4. Backup Verification: test backup restoration, verify automated backups running, document RTO/RPO\n5. Incident Response: severity classification, notification procedures, escalation paths, post-mortem template\n6. Maintenance Windows: planned downtime procedures, communication templates, zero-downtime deployment steps\n7. Include actual commands with placeholders for environment-specific values",
            "status": "pending",
            "testStrategy": "1. Review with ops team for completeness\n2. Execute each daily operation command to verify syntax\n3. Test emergency rollback procedure in staging\n4. Verify all Coolify dashboard references are accurate\n5. Conduct tabletop exercise simulating incident response",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Document all environment variables in docs/deployment/ENVIRONMENT.md",
            "description": "Create comprehensive environment variable reference documenting all required and optional variables for backend, ML service, and infrastructure with descriptions, examples, security notes, and validation rules.",
            "dependencies": [
              1
            ],
            "details": "Create `docs/deployment/ENVIRONMENT.md` with:\n1. Backend API variables: NODE_ENV, DATABASE_URL, JWT_SECRET, JWT_EXPIRES_IN, PORT, REDIS_URL, ML_SERVICE_URL, CORS_ORIGIN\n2. ML Service variables: DATABASE_URL, REDIS_URL, ML_MODEL_PATH, TORCH_DEVICE, PREDICTION_CACHE_TTL\n3. Coolify/Infrastructure variables: COOLIFY_WEBHOOK_URL, GITHUB_TOKEN, DISCORD_WEBHOOK_URL\n4. For each variable include: name, description, required/optional, example value (sanitized), default value, validation rules, security considerations\n5. Security section: which variables are sensitive, how to rotate secrets, where secrets are stored (GitHub Secrets, Coolify env vars)\n6. Environment-specific overrides: development vs staging vs production differences\n7. Validation checklist: required variables by service, format validation commands\n8. Reference existing .env.example and server/.env.example for accuracy",
            "status": "pending",
            "testStrategy": "1. Cross-reference with .env.example files in repo\n2. Verify all variables in docker-compose.prod.yml are documented\n3. Check all variables in GitHub Actions workflows are covered\n4. Test example values actually work (with placeholders replaced)\n5. Ensure no actual secrets are included in documentation\n6. Validate against actual Coolify environment variable configuration",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down into: 1) Create docs/deployment/README.md with architecture diagram, cost breakdown, and navigation links, 2) Write docs/deployment/SETUP.md with step-by-step setup instructions from Hetzner provisioning to Coolify configuration, 3) Create docs/deployment/TROUBLESHOOTING.md covering common issues like container failures, database connection problems, and health check failures, 4) Write docs/deployment/RUNBOOK.md with daily operations procedures and emergency rollback steps, 5) Document all environment variables in docs/deployment/ENVIRONMENT.md. Have team member follow SETUP.md to validate.",
        "updatedAt": "2025-12-18T09:07:38.856Z"
      },
      {
        "id": 30,
        "title": "Set Up Uptime Monitoring",
        "description": "Configure external uptime monitoring for production endpoints using a free monitoring service with alerting capabilities.",
        "details": "**Option 1: UptimeRobot (Recommended - Free tier)**\n\n1. Create account at https://uptimerobot.com\n2. Add monitors:\n\n**Monitor 1 - Backend Health:**\n- Monitor Type: HTTP(s)\n- Friendly Name: Nutri Backend\n- URL: https://api.nutri.app/health\n- Monitoring Interval: 5 minutes\n- Alert contacts: Your email/Discord\n\n**Monitor 2 - ML Service (via Backend):**\n- Monitor Type: HTTP(s)\n- Friendly Name: Nutri ML Health\n- URL: https://api.nutri.app/api/food/health\n- Monitoring Interval: 5 minutes\n\n**Configure Alerts:**\n1. Alert Contacts > Add Contact\n2. Add Discord webhook:\n   - Type: Webhook\n   - URL: Your Discord webhook URL\n   - POST value:\n   ```json\n   {\"content\": \"*alertTypeFriendlyName* - *monitorFriendlyName* is *alertDetails*\"}\n   ```\n\n**Create Status Page (Optional):**\n1. UptimeRobot > My Settings > Status Pages\n2. Create page with both monitors\n3. Custom domain: status.nutri.app\n\n**Option 2: Better Stack (Alternative)**\n\n```bash\n# Create heartbeat endpoint\ncurl -X POST \"https://uptime.betterstack.com/api/v2/heartbeats\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"Nutri Backend\",\n    \"period\": 300,\n    \"grace\": 60,\n    \"call\": true,\n    \"email\": true\n  }'\n```\n\n**Create `scripts/deploy/setup-monitoring.sh`:**\n```bash\n#!/bin/bash\n# Documentation script for monitoring setup\n\necho \"=== Nutri Monitoring Setup ===\"\necho \"\"\necho \"1. Go to https://uptimerobot.com and create account\"\necho \"2. Add the following monitors:\"\necho \"\"\necho \"   Backend Health Check:\"\necho \"   - URL: https://api.nutri.app/health\"\necho \"   - Interval: 5 minutes\"\necho \"   - Expected status: 200\"\necho \"\"\necho \"3. Configure Discord notifications:\"\necho \"   - Create webhook in Discord server\"\necho \"   - Add webhook URL to UptimeRobot alert contacts\"\necho \"\"\necho \"4. (Optional) Create public status page\"\necho \"\"\n```\n\n**Add monitoring endpoint docs to `docs/deployment/MONITORING.md`:**\n```markdown\n# Monitoring Setup\n\n## Endpoints to Monitor\n\n| Endpoint | Expected Status | Interval |\n|----------|-----------------|----------|\n| /health | 200 | 5 min |\n| /health/live | 200 | 1 min |\n\n## Alert Escalation\n\n1. Discord notification (immediate)\n2. Email notification (after 5 min down)\n3. SMS (optional, after 15 min down)\n\n## Response Procedures\n\nSee [Runbook](./RUNBOOK.md) for response procedures.\n```",
        "testStrategy": "1. Configure monitors in UptimeRobot\n2. Intentionally break health endpoint, verify alert fires\n3. Verify alert reaches Discord within 5 minutes\n4. Test alert recovery notification\n5. Check response time graphs show expected latency\n6. Verify status page (if created) shows correct status",
        "priority": "medium",
        "dependencies": [
          "21"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 31,
        "title": "Integrate Error Tracking with Sentry",
        "description": "Set up Sentry error tracking for both backend and ML service to capture, group, and alert on production errors with source maps.",
        "details": "**Backend Integration (Express.js):**\n\n1. Install Sentry:\n```bash\ncd server && npm install @sentry/node @sentry/profiling-node\n```\n\n2. Create `server/src/config/sentry.ts`:\n```typescript\nimport * as Sentry from '@sentry/node';\nimport { ProfilingIntegration } from '@sentry/profiling-node';\nimport { Express } from 'express';\n\nexport function initSentry(app: Express): void {\n  if (!process.env.SENTRY_DSN) {\n    console.log('Sentry DSN not configured, skipping initialization');\n    return;\n  }\n\n  Sentry.init({\n    dsn: process.env.SENTRY_DSN,\n    environment: process.env.NODE_ENV || 'development',\n    release: process.env.npm_package_version,\n    integrations: [\n      new Sentry.Integrations.Http({ tracing: true }),\n      new Sentry.Integrations.Express({ app }),\n      new ProfilingIntegration(),\n    ],\n    tracesSampleRate: process.env.NODE_ENV === 'production' ? 0.1 : 1.0,\n    profilesSampleRate: 0.1,\n    beforeSend(event) {\n      // Scrub sensitive data\n      if (event.request?.headers) {\n        delete event.request.headers.authorization;\n        delete event.request.headers.cookie;\n      }\n      return event;\n    },\n  });\n}\n\nexport { Sentry };\n```\n\n3. Update `server/src/index.ts`:\n```typescript\nimport { initSentry, Sentry } from './config/sentry';\n\nconst app = express();\n\n// Initialize Sentry FIRST\ninitSentry(app);\n\n// Sentry request handler (must be first middleware)\nif (process.env.SENTRY_DSN) {\n  app.use(Sentry.Handlers.requestHandler());\n  app.use(Sentry.Handlers.tracingHandler());\n}\n\n// ... other middleware and routes ...\n\n// Sentry error handler (before your error handler)\nif (process.env.SENTRY_DSN) {\n  app.use(Sentry.Handlers.errorHandler());\n}\n\napp.use(errorHandler);\n```\n\n**ML Service Integration (Python):**\n\n1. Add to `ml-service/requirements.txt`:\n```\nsentry-sdk[fastapi]>=1.40.0\n```\n\n2. Create `ml-service/app/core/sentry.py`:\n```python\nimport sentry_sdk\nfrom sentry_sdk.integrations.fastapi import FastApiIntegration\nfrom sentry_sdk.integrations.sqlalchemy import SqlalchemyIntegration\nfrom app.config import settings\n\ndef init_sentry():\n    if not settings.SENTRY_DSN:\n        return\n    \n    sentry_sdk.init(\n        dsn=settings.SENTRY_DSN,\n        environment=settings.ENVIRONMENT,\n        release=settings.VERSION,\n        integrations=[\n            FastApiIntegration(),\n            SqlalchemyIntegration(),\n        ],\n        traces_sample_rate=0.1 if settings.ENVIRONMENT == 'production' else 1.0,\n        profiles_sample_rate=0.1,\n        send_default_pii=False,  # Don't send personally identifiable info\n        before_send=scrub_sensitive_data,\n    )\n\ndef scrub_sensitive_data(event, hint):\n    if 'request' in event and 'headers' in event['request']:\n        headers = event['request']['headers']\n        if 'authorization' in headers:\n            headers['authorization'] = '[REDACTED]'\n    return event\n```\n\n3. Update `ml-service/app/main.py`:\n```python\nfrom app.core.sentry import init_sentry\n\n# Initialize Sentry on startup\ninit_sentry()\n```\n\n**GitHub Actions - Upload Source Maps:**\nAdd to `.github/workflows/build.yml`:\n```yaml\n      - name: Upload source maps to Sentry\n        if: env.SENTRY_AUTH_TOKEN != ''\n        env:\n          SENTRY_AUTH_TOKEN: ${{ secrets.SENTRY_AUTH_TOKEN }}\n        run: |\n          npm install -g @sentry/cli\n          cd server && npm run build\n          sentry-cli releases new ${{ github.sha }}\n          sentry-cli releases files ${{ github.sha }} upload-sourcemaps ./dist\n          sentry-cli releases finalize ${{ github.sha }}\n```\n\n**Environment Variables to Add:**\n```\nSENTRY_DSN=https://xxx@xxx.ingest.sentry.io/xxx\nSENTRY_AUTH_TOKEN=xxx (for source map upload)\n```",
        "testStrategy": "1. Create Sentry project and get DSN\n2. Trigger test error: `throw new Error('Test Sentry integration')`\n3. Verify error appears in Sentry dashboard\n4. Check source maps resolve correctly\n5. Verify sensitive data is scrubbed (no auth tokens)\n6. Test alert notification for new errors\n7. Verify performance monitoring shows traces\n8. Test ML service integration separately",
        "priority": "medium",
        "dependencies": [
          "26",
          "27"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 32,
        "title": "Create Database Backup Script",
        "description": "Create a backup script for database exports with support for S3-compatible storage, retention policies, and restore procedures for the Supabase free tier.",
        "details": "**Note:** Supabase Pro tier includes automatic backups. This script is for free tier users.\n\n**Create `scripts/deploy/backup-database.sh`:**\n```bash\n#!/bin/bash\nset -e\n\n# =============================================================================\n# Nutri Database Backup Script\n# For use with Supabase free tier (no automatic backups)\n# =============================================================================\n\necho \"=== Nutri Database Backup ===\"\necho \"Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)\"\n\n# Configuration\nBACKUP_DIR=\"${BACKUP_DIR:-/backups}\"\nRETENTION_DAYS=\"${RETENTION_DAYS:-7}\"\nS3_BUCKET=\"${S3_BUCKET:-}\"\nS3_ENDPOINT=\"${S3_ENDPOINT:-}\"\n\n# Validate required environment variables\nif [ -z \"$DATABASE_URL\" ]; then\n    echo \"Error: DATABASE_URL is required\"\n    exit 1\nfi\n\n# Create backup directory\nmkdir -p \"$BACKUP_DIR\"\n\n# Generate backup filename\nTIMESTAMP=$(date +%Y%m%d_%H%M%S)\nBACKUP_FILE=\"nutri_backup_${TIMESTAMP}.sql.gz\"\nBACKUP_PATH=\"$BACKUP_DIR/$BACKUP_FILE\"\n\n# Extract connection details from DATABASE_URL\n# Format: postgresql://user:password@host:port/database\nproto=\"$(echo $DATABASE_URL | grep :// | sed -e's,^\\(.*://\\).*,\\1,g')\"\nurl=\"$(echo ${DATABASE_URL/$proto/})\"\nuser=\"$(echo $url | grep @ | cut -d@ -f1 | cut -d: -f1)\"\npassword=\"$(echo $url | grep @ | cut -d@ -f1 | cut -d: -f2)\"\nhostport=\"$(echo ${url/$user:$password@/} | cut -d/ -f1)\"\nhost=\"$(echo $hostport | cut -d: -f1)\"\nport=\"$(echo $hostport | cut -d: -f2)\"\ndatabase=\"$(echo $url | grep / | cut -d/ -f2-)\"\n\necho \"Backing up database: $database on $host\"\n\n# Create backup with pg_dump\nexport PGPASSWORD=\"$password\"\npg_dump -h \"$host\" -p \"$port\" -U \"$user\" -d \"$database\" \\\n    --format=custom \\\n    --no-owner \\\n    --no-acl \\\n    --verbose 2>&1 | gzip > \"$BACKUP_PATH\"\n\nBACKUP_SIZE=$(ls -lh \"$BACKUP_PATH\" | awk '{print $5}')\necho \"Backup created: $BACKUP_PATH ($BACKUP_SIZE)\"\n\n# Upload to S3 if configured\nif [ -n \"$S3_BUCKET\" ]; then\n    echo \"Uploading to S3: $S3_BUCKET\"\n    \n    S3_PATH=\"s3://$S3_BUCKET/nutri/backups/$BACKUP_FILE\"\n    \n    if [ -n \"$S3_ENDPOINT\" ]; then\n        # For S3-compatible storage (Backblaze B2, MinIO, etc.)\n        aws s3 cp \"$BACKUP_PATH\" \"$S3_PATH\" --endpoint-url \"$S3_ENDPOINT\"\n    else\n        aws s3 cp \"$BACKUP_PATH\" \"$S3_PATH\"\n    fi\n    \n    echo \"Uploaded to: $S3_PATH\"\n    \n    # Remove local file after S3 upload\n    rm \"$BACKUP_PATH\"\n    echo \"Local backup removed (stored in S3)\"\nfi\n\n# Clean up old backups\necho \"Cleaning up backups older than $RETENTION_DAYS days...\"\nfind \"$BACKUP_DIR\" -name \"nutri_backup_*.sql.gz\" -mtime +$RETENTION_DAYS -delete 2>/dev/null || true\n\n# S3 lifecycle should handle S3 cleanup, but we can list what's there\nif [ -n \"$S3_BUCKET\" ]; then\n    echo \"Current S3 backups:\"\n    aws s3 ls \"s3://$S3_BUCKET/nutri/backups/\" ${S3_ENDPOINT:+--endpoint-url $S3_ENDPOINT} | tail -5\nfi\n\necho \"\"\necho \"=== Backup Complete ===\"\n```\n\n**Create `scripts/deploy/restore-database.sh`:**\n```bash\n#!/bin/bash\nset -e\n\n# =============================================================================\n# Nutri Database Restore Script\n# =============================================================================\n\necho \"=== Nutri Database Restore ===\"\necho \"WARNING: This will overwrite the target database!\"\nread -p \"Are you sure? (yes/no): \" confirm\n\nif [ \"$confirm\" != \"yes\" ]; then\n    echo \"Restore cancelled.\"\n    exit 1\nfi\n\nBACKUP_FILE=\"$1\"\n\nif [ -z \"$BACKUP_FILE\" ]; then\n    echo \"Usage: $0 <backup-file.sql.gz>\"\n    echo \"\"\n    echo \"Available backups:\"\n    ls -lh /backups/nutri_backup_*.sql.gz 2>/dev/null || echo \"No local backups found\"\n    exit 1\nfi\n\nif [ -z \"$DATABASE_URL\" ]; then\n    echo \"Error: DATABASE_URL is required\"\n    exit 1\nfi\n\n# Parse DATABASE_URL (same as backup script)\n# ... [parsing code] ...\n\necho \"Restoring from: $BACKUP_FILE\"\necho \"Target database: $database on $host\"\n\nexport PGPASSWORD=\"$password\"\n\n# Restore\ngunzip -c \"$BACKUP_FILE\" | pg_restore \\\n    -h \"$host\" -p \"$port\" -U \"$user\" -d \"$database\" \\\n    --no-owner \\\n    --no-acl \\\n    --clean \\\n    --if-exists \\\n    --verbose\n\necho \"=== Restore Complete ===\"\n```\n\n**Create cron job for automated backups:**\n```bash\n# Add to server crontab\n0 3 * * * /opt/nutri/scripts/backup-database.sh >> /var/log/nutri-backup.log 2>&1\n```\n\n**Document in `docs/deployment/BACKUPS.md`:**\n```markdown\n# Backup and Recovery\n\n## Automatic Backups\n\n- **Supabase Pro:** Automatic daily backups included\n- **Supabase Free:** Use `scripts/deploy/backup-database.sh` with cron\n\n## Manual Backup\n\n```bash\nexport DATABASE_URL=\"postgresql://...\"\n./scripts/deploy/backup-database.sh\n```\n\n## Restore Procedure\n\n1. Download backup file\n2. Run restore script:\n   ```bash\n   ./scripts/deploy/restore-database.sh /path/to/backup.sql.gz\n   ```\n3. Verify data integrity\n4. Run migrations if needed: `npx prisma migrate deploy`\n```",
        "testStrategy": "1. Run backup script against dev database\n2. Verify backup file created and valid: `gunzip -t backup.sql.gz`\n3. Test restore to empty test database\n4. Verify data integrity after restore\n5. Test S3 upload with Backblaze B2 or MinIO\n6. Verify retention policy deletes old files\n7. Test cron job execution\n8. Document recovery time (RTO target: <30 min)",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 33,
        "title": "Configure Hetzner Server Snapshots",
        "description": "Enable and configure Hetzner automatic server snapshots for disaster recovery with documentation for restore procedures.",
        "details": "**Enable Hetzner Backups via Console:**\n\n1. Log into [Hetzner Cloud Console](https://console.hetzner.cloud/)\n2. Select your server\n3. Go to \"Backups\" tab\n4. Click \"Enable Backups\"\n   - Cost: ~20% of server price (€1.10/mo for CX32)\n   - Schedule: Daily automatic backups\n   - Retention: Last 7 daily + 4 weekly backups\n\n**Or via Hetzner CLI/API:**\n```bash\n# Install hcloud CLI\nbrew install hcloud\n\n# Configure\nhcloud context create nutri\n# Enter API token from Hetzner Console > Security > API Tokens\n\n# Enable backups on server\nhcloud server enable-backup <server-name>\n```\n\n**Create Manual Snapshot Script `scripts/deploy/create-snapshot.sh`:**\n```bash\n#!/bin/bash\nset -e\n\n# =============================================================================\n# Create Hetzner Server Snapshot\n# Use before major deployments or configuration changes\n# =============================================================================\n\nSERVER_NAME=\"${SERVER_NAME:-nutri-production}\"\nSNAPSHOT_DESC=\"${1:-Manual snapshot $(date +%Y-%m-%d_%H%M)}\"\n\necho \"=== Creating Hetzner Snapshot ===\"\necho \"Server: $SERVER_NAME\"\necho \"Description: $SNAPSHOT_DESC\"\necho \"\"\n\n# Check if hcloud is installed\nif ! command -v hcloud &> /dev/null; then\n    echo \"Error: hcloud CLI not installed\"\n    echo \"Install with: brew install hcloud\"\n    exit 1\nfi\n\n# Create snapshot\necho \"Creating snapshot (this may take a few minutes)...\"\nhcloud server create-image --type snapshot --description \"$SNAPSHOT_DESC\" \"$SERVER_NAME\"\n\necho \"\"\necho \"=== Snapshot Created ===\"\necho \"\"\necho \"Available snapshots:\"\nhcloud image list --type snapshot\n```\n\n**Document Restore Procedure in `docs/deployment/DISASTER-RECOVERY.md`:**\n```markdown\n# Disaster Recovery\n\n## Server Snapshot Recovery\n\n### When to Use\n- Server is unrecoverable\n- Corrupted system files\n- Major misconfiguration\n\n### Restore Steps\n\n1. **Via Hetzner Console:**\n   - Go to Images > Snapshots\n   - Select snapshot to restore\n   - Click \"Create Server from Image\"\n   - Configure same specs as original server\n   - Update DNS to point to new server IP\n\n2. **Via CLI:**\n   ```bash\n   # List available snapshots\n   hcloud image list --type snapshot\n   \n   # Create new server from snapshot\n   hcloud server create \\\n     --name nutri-production-restored \\\n     --type cx32 \\\n     --image <snapshot-id> \\\n     --location nbg1\n   \n   # Update DNS\n   # ... update A record to new IP\n   ```\n\n3. **Post-Restore Checklist:**\n   - [ ] Verify server accessible via SSH\n   - [ ] Check Coolify dashboard\n   - [ ] Verify all containers running\n   - [ ] Test health endpoints\n   - [ ] Update DNS if IP changed\n   - [ ] Update GitHub secrets with new webhook URLs\n\n### Recovery Time Estimate\n\n| Step | Duration |\n|------|----------|\n| Create server from snapshot | 5-10 min |\n| DNS propagation | 5-30 min |\n| Verify services | 5 min |\n| **Total** | **15-45 min** |\n\n## Monthly Recovery Test\n\nSchedule monthly recovery drill:\n1. Create snapshot of production\n2. Spin up test server from snapshot\n3. Verify services work\n4. Document any issues\n5. Delete test server\n```",
        "testStrategy": "1. Enable backups in Hetzner Console\n2. Create manual snapshot using script\n3. Verify snapshot appears in console\n4. Test restore by creating new server from snapshot\n5. Verify restored server has correct configuration\n6. Document actual recovery time\n7. Delete test server after verification\n8. Set up monthly recovery test reminder",
        "priority": "medium",
        "dependencies": [
          "28"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 34,
        "title": "Configure Server Security Hardening",
        "description": "Implement additional security hardening measures including SSH hardening, intrusion detection setup, and kernel security parameters.",
        "details": "**Create `scripts/deploy/harden-server.sh`:**\n```bash\n#!/bin/bash\nset -e\n\n# =============================================================================\n# Nutri Server Security Hardening\n# Run after initial setup script\n# =============================================================================\n\necho \"=== Security Hardening ===\"\n\n# =============================================================================\n# SSH Hardening\n# =============================================================================\n\necho \"Hardening SSH configuration...\"\ncat >> /etc/ssh/sshd_config << 'EOF'\n\n# Nutri Security Hardening\nProtocol 2\nMaxAuthTries 3\nMaxSessions 5\nLoginGraceTime 30\nClientAliveInterval 300\nClientAliveCountMax 2\nPermitEmptyPasswords no\nX11Forwarding no\nAllowTcpForwarding no\nAllowAgentForwarding no\nPermitUserEnvironment no\nEOF\n\n# Restart SSH\nsystemctl restart sshd\necho \"SSH hardened\"\n\n# =============================================================================\n# Kernel Security Parameters\n# =============================================================================\n\necho \"Configuring kernel security parameters...\"\ncat >> /etc/sysctl.conf << 'EOF'\n\n# Nutri Security Hardening\n\n# Network security\nnet.ipv4.tcp_syncookies = 1\nnet.ipv4.tcp_max_syn_backlog = 2048\nnet.ipv4.tcp_synack_retries = 2\nnet.ipv4.conf.all.rp_filter = 1\nnet.ipv4.conf.default.rp_filter = 1\nnet.ipv4.conf.all.accept_source_route = 0\nnet.ipv4.conf.default.accept_source_route = 0\nnet.ipv4.conf.all.accept_redirects = 0\nnet.ipv4.conf.default.accept_redirects = 0\nnet.ipv4.conf.all.send_redirects = 0\nnet.ipv4.conf.default.send_redirects = 0\nnet.ipv4.icmp_echo_ignore_broadcasts = 1\nnet.ipv4.icmp_ignore_bogus_error_responses = 1\n\n# Disable IPv6 if not needed\nnet.ipv6.conf.all.disable_ipv6 = 1\nnet.ipv6.conf.default.disable_ipv6 = 1\n\n# Memory protection\nkernel.randomize_va_space = 2\nkernel.dmesg_restrict = 1\nkernel.kptr_restrict = 2\nEOF\n\nsysctl -p\necho \"Kernel parameters configured\"\n\n# =============================================================================\n# Advanced Fail2Ban Configuration\n# =============================================================================\n\necho \"Configuring advanced fail2ban rules...\"\ncat > /etc/fail2ban/jail.d/nutri.conf << 'EOF'\n[nutri-api]\nenabled = true\nport = http,https\nfilter = nutri-api\nlogpath = /var/log/docker-nutri-backend.log\nmaxretry = 10\nfindtime = 60\nbantime = 3600\nEOF\n\ncat > /etc/fail2ban/filter.d/nutri-api.conf << 'EOF'\n[Definition]\nfailregex = ^.*\"statusCode\":401.*\"ip\":\"<HOST>\".*$\n            ^.*\"statusCode\":403.*\"ip\":\"<HOST>\".*$\nignoreregex =\nEOF\n\nsystemctl restart fail2ban\necho \"Fail2ban configured for API protection\"\n\n# =============================================================================\n# Docker Security\n# =============================================================================\n\necho \"Configuring Docker security...\"\ncat > /etc/docker/daemon.json << 'EOF'\n{\n  \"icc\": false,\n  \"userns-remap\": \"default\",\n  \"no-new-privileges\": true,\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": \"3\"\n  },\n  \"live-restore\": true\n}\nEOF\n\nsystemctl restart docker\necho \"Docker security configured\"\n\n# =============================================================================\n# File Permissions\n# =============================================================================\n\necho \"Securing file permissions...\"\nchmod 700 /root\nchmod 600 /etc/ssh/sshd_config\nchmod 644 /etc/passwd\nchmod 000 /etc/shadow  # Only root can read\nchmod 644 /etc/group\n\n# =============================================================================\n# Login Banner\n# =============================================================================\n\ncat > /etc/issue.net << 'EOF'\n***************************************************************************\n                      AUTHORIZED ACCESS ONLY\n***************************************************************************\nThis system is for authorized use only. All activities may be monitored\nand recorded. Unauthorized access will be prosecuted.\n***************************************************************************\nEOF\n\necho \"Banner /etc/issue.net\" >> /etc/ssh/sshd_config\nsystemctl restart sshd\n\necho \"\"\necho \"=== Security Hardening Complete ===\"\necho \"\"\necho \"Verify changes:\"\necho \"- SSH: ssh to server, should still work\"\necho \"- Fail2ban: fail2ban-client status\"\necho \"- Sysctl: sysctl -a | grep net.ipv4.tcp_syncookies\"\n```\n\n**Security Checklist `docs/deployment/SECURITY-CHECKLIST.md`:**\n```markdown\n# Security Checklist\n\n## Pre-Deployment\n\n- [ ] SSH keys configured, password auth disabled\n- [ ] UFW firewall enabled (22, 80, 443 only)\n- [ ] Fail2ban active and configured\n- [ ] Automatic security updates enabled\n- [ ] Kernel security parameters applied\n- [ ] Docker security settings configured\n\n## Application Security\n\n- [ ] All secrets stored in environment variables\n- [ ] No secrets in source code or logs\n- [ ] HTTPS enforced (redirect HTTP)\n- [ ] Security headers configured (HSTS, CSP)\n- [ ] Rate limiting enabled\n- [ ] CORS properly configured\n\n## Monitoring\n\n- [ ] Failed SSH attempts monitored\n- [ ] API abuse patterns monitored\n- [ ] Error tracking enabled (Sentry)\n- [ ] Uptime monitoring configured\n\n## Backup & Recovery\n\n- [ ] Database backups automated\n- [ ] Server snapshots enabled\n- [ ] Recovery procedure tested\n```",
        "testStrategy": "1. Run hardening script on test server\n2. Verify SSH still works with key auth\n3. Verify password auth fails\n4. Test fail2ban bans IP after failed attempts\n5. Verify kernel parameters with sysctl -a\n6. Test Docker containers still run\n7. Run security scanner (lynis) and compare scores\n8. Document any issues for production",
        "priority": "medium",
        "dependencies": [
          "28"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 35,
        "title": "Configure Application Security Headers",
        "description": "Implement security headers (HSTS, CSP, X-Frame-Options, etc.) in the backend API and configure HTTPS-only access with proper CORS settings.",
        "details": "**Install helmet for Express:**\n```bash\ncd server && npm install helmet\n```\n\n**Create `server/src/middleware/security.ts`:**\n```typescript\nimport helmet from 'helmet';\nimport { Express, Request, Response, NextFunction } from 'express';\n\nexport function configureSecurityHeaders(app: Express): void {\n  // Basic helmet protection\n  app.use(helmet());\n\n  // Strict Transport Security (HTTPS only)\n  app.use(\n    helmet.hsts({\n      maxAge: 31536000, // 1 year in seconds\n      includeSubDomains: true,\n      preload: true,\n    })\n  );\n\n  // Content Security Policy\n  app.use(\n    helmet.contentSecurityPolicy({\n      directives: {\n        defaultSrc: [\"'self'\"],\n        scriptSrc: [\"'self'\"],\n        styleSrc: [\"'self'\", \"'unsafe-inline'\"], // React may need inline styles\n        imgSrc: [\"'self'\", 'data:', 'https:'],\n        connectSrc: [\"'self'\", 'https://api.nutri.app'],\n        fontSrc: [\"'self'\"],\n        objectSrc: [\"'none'\"],\n        mediaSrc: [\"'self'\"],\n        frameSrc: [\"'none'\"],\n      },\n    })\n  );\n\n  // Prevent clickjacking\n  app.use(helmet.frameguard({ action: 'deny' }));\n\n  // Prevent MIME type sniffing\n  app.use(helmet.noSniff());\n\n  // XSS protection (legacy browsers)\n  app.use(helmet.xssFilter());\n\n  // Referrer policy\n  app.use(helmet.referrerPolicy({ policy: 'strict-origin-when-cross-origin' }));\n\n  // Remove X-Powered-By header\n  app.disable('x-powered-by');\n\n  // Custom security headers\n  app.use((_req: Request, res: Response, next: NextFunction) => {\n    // Permissions Policy (replaces Feature-Policy)\n    res.setHeader(\n      'Permissions-Policy',\n      'accelerometer=(), camera=(), geolocation=(), gyroscope=(), magnetometer=(), microphone=(), payment=(), usb=()'\n    );\n    \n    // Cross-Origin policies\n    res.setHeader('Cross-Origin-Opener-Policy', 'same-origin');\n    res.setHeader('Cross-Origin-Embedder-Policy', 'require-corp');\n    res.setHeader('Cross-Origin-Resource-Policy', 'same-origin');\n    \n    next();\n  });\n}\n```\n\n**Update CORS configuration in `server/src/index.ts`:**\n```typescript\nimport cors from 'cors';\nimport { configureSecurityHeaders } from './middleware/security';\n\nconst app = express();\n\n// Configure CORS with specific origins\nconst allowedOrigins = [\n  'http://localhost:3000',\n  'http://localhost:8081', // Expo dev\n  'https://nutri.app',\n  'https://www.nutri.app',\n  process.env.CORS_ORIGIN, // Allow override via env\n].filter(Boolean);\n\napp.use(\n  cors({\n    origin: (origin, callback) => {\n      // Allow requests with no origin (mobile apps, curl, etc.)\n      if (!origin) return callback(null, true);\n      \n      if (allowedOrigins.includes(origin)) {\n        callback(null, true);\n      } else {\n        callback(new Error('Not allowed by CORS'));\n      }\n    },\n    credentials: true,\n    methods: ['GET', 'POST', 'PUT', 'DELETE', 'PATCH', 'OPTIONS'],\n    allowedHeaders: ['Content-Type', 'Authorization', 'X-Correlation-ID'],\n    exposedHeaders: ['X-Correlation-ID'],\n    maxAge: 86400, // 24 hours\n  })\n);\n\n// Configure security headers\nconfigureSecurityHeaders(app);\n```\n\n**Add HTTPS redirect middleware (for when behind Traefik/reverse proxy):**\n```typescript\n// Trust proxy (Traefik)\napp.set('trust proxy', 1);\n\n// Redirect HTTP to HTTPS in production\nif (process.env.NODE_ENV === 'production') {\n  app.use((req, res, next) => {\n    if (req.header('x-forwarded-proto') !== 'https') {\n      res.redirect(301, `https://${req.header('host')}${req.url}`);\n    } else {\n      next();\n    }\n  });\n}\n```\n\n**Test headers with curl:**\n```bash\ncurl -I https://api.nutri.app/health\n```\n\nExpected headers:\n```\nStrict-Transport-Security: max-age=31536000; includeSubDomains; preload\nX-Content-Type-Options: nosniff\nX-Frame-Options: DENY\nContent-Security-Policy: default-src 'self'; ...\nReferrer-Policy: strict-origin-when-cross-origin\n```",
        "testStrategy": "1. Add security middleware and verify app still works\n2. Test all API endpoints work with new CORS config\n3. Use securityheaders.com to scan API and verify A+ rating\n4. Test mobile app still connects with CORS\n5. Verify X-Powered-By header is removed\n6. Test HTTPS redirect works in production mode\n7. Verify CSP doesn't break legitimate requests\n8. Test preflight OPTIONS requests work correctly",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 36,
        "title": "Configure Dependabot for Security Updates",
        "description": "Enable GitHub Dependabot for automated dependency vulnerability scanning and pull request creation for security updates.",
        "details": "**Create `.github/dependabot.yml`:**\n```yaml\nversion: 2\nupdates:\n  # JavaScript/TypeScript dependencies (root - mobile app)\n  - package-ecosystem: \"npm\"\n    directory: \"/\"\n    schedule:\n      interval: \"weekly\"\n      day: \"monday\"\n      time: \"09:00\"\n      timezone: \"UTC\"\n    open-pull-requests-limit: 10\n    labels:\n      - \"dependencies\"\n      - \"mobile\"\n    reviewers:\n      - \"onurtemizkan\"\n    commit-message:\n      prefix: \"deps(mobile):\"\n    ignore:\n      # Ignore major version updates for React Native (can be breaking)\n      - dependency-name: \"react-native\"\n        update-types: [\"version-update:semver-major\"]\n      - dependency-name: \"expo\"\n        update-types: [\"version-update:semver-major\"]\n\n  # Backend dependencies\n  - package-ecosystem: \"npm\"\n    directory: \"/server\"\n    schedule:\n      interval: \"weekly\"\n      day: \"monday\"\n      time: \"09:00\"\n      timezone: \"UTC\"\n    open-pull-requests-limit: 10\n    labels:\n      - \"dependencies\"\n      - \"backend\"\n    reviewers:\n      - \"onurtemizkan\"\n    commit-message:\n      prefix: \"deps(server):\"\n    groups:\n      # Group minor and patch updates together\n      production-dependencies:\n        patterns:\n          - \"*\"\n        exclude-patterns:\n          - \"@types/*\"\n          - \"typescript\"\n        update-types:\n          - \"minor\"\n          - \"patch\"\n\n  # Python ML Service dependencies\n  - package-ecosystem: \"pip\"\n    directory: \"/ml-service\"\n    schedule:\n      interval: \"weekly\"\n      day: \"tuesday\"\n      time: \"09:00\"\n      timezone: \"UTC\"\n    open-pull-requests-limit: 5\n    labels:\n      - \"dependencies\"\n      - \"ml-service\"\n    reviewers:\n      - \"onurtemizkan\"\n    commit-message:\n      prefix: \"deps(ml):\"\n    ignore:\n      # ML libraries can have breaking changes\n      - dependency-name: \"torch\"\n        update-types: [\"version-update:semver-major\"]\n      - dependency-name: \"scikit-learn\"\n        update-types: [\"version-update:semver-major\"]\n\n  # GitHub Actions\n  - package-ecosystem: \"github-actions\"\n    directory: \"/\"\n    schedule:\n      interval: \"weekly\"\n      day: \"wednesday\"\n      time: \"09:00\"\n      timezone: \"UTC\"\n    labels:\n      - \"dependencies\"\n      - \"ci\"\n    commit-message:\n      prefix: \"ci:\"\n\n  # Docker base images\n  - package-ecosystem: \"docker\"\n    directory: \"/server\"\n    schedule:\n      interval: \"weekly\"\n      day: \"thursday\"\n    labels:\n      - \"dependencies\"\n      - \"docker\"\n    commit-message:\n      prefix: \"docker(server):\"\n\n  - package-ecosystem: \"docker\"\n    directory: \"/ml-service\"\n    schedule:\n      interval: \"weekly\"\n      day: \"thursday\"\n    labels:\n      - \"dependencies\"\n      - \"docker\"\n    commit-message:\n      prefix: \"docker(ml):\"\n```\n\n**Enable GitHub Security Features:**\n\n1. Go to repository Settings > Security > Code security and analysis\n2. Enable:\n   - Dependency graph\n   - Dependabot alerts\n   - Dependabot security updates\n   - Secret scanning\n   - Push protection\n\n**Create security policy `.github/SECURITY.md`:**\n```markdown\n# Security Policy\n\n## Supported Versions\n\n| Version | Supported          |\n| ------- | ------------------ |\n| latest  | :white_check_mark: |\n\n## Reporting a Vulnerability\n\nPlease report security vulnerabilities by emailing security@nutri.app.\n\nDo NOT create public GitHub issues for security vulnerabilities.\n\n### Response Timeline\n\n- Initial response: 24 hours\n- Triage: 48 hours\n- Fix timeline: Depends on severity\n  - Critical: 24-48 hours\n  - High: 7 days\n  - Medium: 30 days\n  - Low: Next release\n\n## Security Measures\n\n- All dependencies automatically scanned\n- Security updates applied weekly\n- Docker images scanned for vulnerabilities\n- Secrets scanning enabled\n```\n\n**Document in `docs/deployment/SECURITY-UPDATES.md`:**\n```markdown\n# Dependency Security Updates\n\n## Automated Updates\n\nDependabot creates PRs for:\n- npm packages (mobile, server)\n- pip packages (ml-service)\n- GitHub Actions versions\n- Docker base images\n\n## Weekly Schedule\n\n| Day | Package Ecosystem |\n|-----|-------------------|\n| Monday | npm (mobile, server) |\n| Tuesday | pip (ml-service) |\n| Wednesday | GitHub Actions |\n| Thursday | Docker images |\n\n## Handling PRs\n\n1. Review changes and changelogs\n2. Run CI checks\n3. Merge if tests pass\n4. For breaking changes, test manually first\n```",
        "testStrategy": "1. Commit dependabot.yml and verify it appears in Insights > Dependency graph > Dependabot\n2. Manually trigger dependency check: Settings > Security > Code security\n3. Wait for first PRs to appear (within a week)\n4. Verify PR labels and commit message format\n5. Check grouping works for minor/patch updates\n6. Verify ignore rules work (no major React Native PRs)\n7. Test that CI runs on Dependabot PRs\n8. Enable secret scanning and verify no alerts",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 37,
        "title": "Create Rollback Script and Procedures",
        "description": "Create a rollback script that can quickly revert to a previous deployment version with both manual and automated options.",
        "details": "**Create `scripts/deploy/rollback.sh`:**\n```bash\n#!/bin/bash\nset -e\n\n# =============================================================================\n# Nutri Rollback Script\n# Rolls back to a previous deployment version\n# =============================================================================\n\necho \"=== Nutri Rollback ===\"\necho \"Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)\"\necho \"\"\n\n# Configuration\nREGISTRY=\"ghcr.io\"\nREPO=\"${GITHUB_REPOSITORY:-your-org/nutri}\"\nBACKEND_IMAGE=\"$REGISTRY/$REPO/backend\"\nML_IMAGE=\"$REGISTRY/$REPO/ml-service\"\n\n# Parse arguments\nTARGET_TAG=\"$1\"\nSERVICE=\"${2:-all}\"  # backend, ml, or all\n\nif [ -z \"$TARGET_TAG\" ]; then\n    echo \"Usage: $0 <image-tag> [service]\"\n    echo \"\"\n    echo \"Arguments:\"\n    echo \"  image-tag   Git SHA or tag to rollback to\"\n    echo \"  service     'backend', 'ml', or 'all' (default: all)\"\n    echo \"\"\n    echo \"Examples:\"\n    echo \"  $0 abc1234              # Rollback all services to abc1234\"\n    echo \"  $0 abc1234 backend      # Rollback only backend\"\n    echo \"\"\n    echo \"Recent tags:\"\n    echo \"--- Backend ---\"\n    curl -s -H \"Authorization: Bearer $GITHUB_TOKEN\" \\\n        \"https://api.github.com/orgs/$(dirname $REPO)/packages/container/$(basename $REPO)%2Fbackend/versions\" \\\n        2>/dev/null | jq -r '.[0:5] | .[].metadata.container.tags[]' 2>/dev/null || echo \"Unable to fetch tags\"\n    exit 1\nfi\n\n# Confirm rollback\necho \"Rolling back to: $TARGET_TAG\"\necho \"Services: $SERVICE\"\necho \"\"\nread -p \"Continue? (yes/no): \" confirm\nif [ \"$confirm\" != \"yes\" ]; then\n    echo \"Rollback cancelled.\"\n    exit 0\nfi\n\n# Function to trigger Coolify deployment\ntrigger_coolify_deploy() {\n    local webhook_url=\"$1\"\n    local service_name=\"$2\"\n    \n    echo \"Triggering $service_name deployment...\"\n    response=$(curl -s -w \"\\n%{http_code}\" -X POST \\\n        -H \"Authorization: Bearer $COOLIFY_WEBHOOK_SECRET\" \\\n        \"$webhook_url\")\n    \n    http_code=$(echo \"$response\" | tail -n1)\n    if [ \"$http_code\" = \"200\" ] || [ \"$http_code\" = \"201\" ]; then\n        echo \"✓ $service_name deployment triggered\"\n    else\n        echo \"✗ $service_name deployment failed (HTTP $http_code)\"\n        return 1\n    fi\n}\n\n# Function to wait for health check\nwait_for_health() {\n    local url=\"$1\"\n    local max_attempts=\"${2:-30}\"\n    \n    echo \"Waiting for $url to be healthy...\"\n    for i in $(seq 1 $max_attempts); do\n        if curl -sf \"$url\" > /dev/null 2>&1; then\n            echo \"✓ Service healthy after $i attempts\"\n            return 0\n        fi\n        sleep 5\n    done\n    echo \"✗ Service not healthy after $max_attempts attempts\"\n    return 1\n}\n\n# Perform rollback\nif [ \"$SERVICE\" = \"all\" ] || [ \"$SERVICE\" = \"backend\" ]; then\n    echo \"\"\n    echo \"=== Rolling back Backend ===\"\n    \n    # Pull the target image\n    docker pull \"$BACKEND_IMAGE:$TARGET_TAG\" || {\n        echo \"Error: Could not pull $BACKEND_IMAGE:$TARGET_TAG\"\n        exit 1\n    }\n    \n    # Tag as latest for Coolify\n    docker tag \"$BACKEND_IMAGE:$TARGET_TAG\" \"$BACKEND_IMAGE:latest\"\n    \n    # Trigger Coolify deployment\n    if [ -n \"$COOLIFY_BACKEND_WEBHOOK_URL\" ]; then\n        trigger_coolify_deploy \"$COOLIFY_BACKEND_WEBHOOK_URL\" \"Backend\"\n    else\n        echo \"Warning: COOLIFY_BACKEND_WEBHOOK_URL not set\"\n        echo \"Manually restart the container in Coolify\"\n    fi\n    \n    # Wait for health\n    sleep 10\n    wait_for_health \"${PRODUCTION_API_URL:-http://localhost:3000}/health\"\nfi\n\nif [ \"$SERVICE\" = \"all\" ] || [ \"$SERVICE\" = \"ml\" ]; then\n    echo \"\"\n    echo \"=== Rolling back ML Service ===\"\n    \n    docker pull \"$ML_IMAGE:$TARGET_TAG\" || {\n        echo \"Error: Could not pull $ML_IMAGE:$TARGET_TAG\"\n        exit 1\n    }\n    \n    docker tag \"$ML_IMAGE:$TARGET_TAG\" \"$ML_IMAGE:latest\"\n    \n    if [ -n \"$COOLIFY_ML_WEBHOOK_URL\" ]; then\n        trigger_coolify_deploy \"$COOLIFY_ML_WEBHOOK_URL\" \"ML Service\"\n    else\n        echo \"Warning: COOLIFY_ML_WEBHOOK_URL not set\"\n    fi\nfi\n\necho \"\"\necho \"=== Rollback Complete ===\"\necho \"Rolled back to: $TARGET_TAG\"\necho \"\"\necho \"Verify deployment:\"\necho \"  curl ${PRODUCTION_API_URL:-http://localhost:3000}/health\"\n```\n\n**Add to deploy workflow (`.github/workflows/deploy.yml`):**\nAlready included workflow_dispatch with image_tag input for manual rollback.\n\n**Document in `docs/deployment/ROLLBACK.md`:**\n```markdown\n# Rollback Procedures\n\n## Quick Rollback via GitHub Actions\n\n1. Go to Actions > Deploy workflow\n2. Click \"Run workflow\"\n3. Enter the image tag/SHA to rollback to\n4. Click \"Run workflow\"\n\n## Manual Rollback via Script\n\n```bash\n# Set required environment variables\nexport COOLIFY_WEBHOOK_SECRET=\"...\"\nexport COOLIFY_BACKEND_WEBHOOK_URL=\"...\"\nexport PRODUCTION_API_URL=\"https://api.nutri.app\"\n\n# Rollback all services to specific commit\n./scripts/deploy/rollback.sh abc1234\n\n# Rollback only backend\n./scripts/deploy/rollback.sh abc1234 backend\n```\n\n## Finding Previous Tags\n\n```bash\n# Via GitHub API\ncurl -H \"Authorization: Bearer $GITHUB_TOKEN\" \\\n  \"https://api.github.com/orgs/your-org/packages/container/nutri%2Fbackend/versions\" \\\n  | jq '.[].metadata.container.tags'\n\n# Via Docker\ndocker pull ghcr.io/your-org/nutri/backend:latest\ndocker history ghcr.io/your-org/nutri/backend:latest\n```\n\n## Rollback Time Target\n\n- Time to initiate: < 2 minutes\n- Deployment time: < 5 minutes\n- **Total rollback time: < 7 minutes**\n```",
        "testStrategy": "1. Deploy a new version to staging\n2. Run rollback script with previous tag\n3. Verify correct image is running: `docker inspect <container> | grep Image`\n4. Test health endpoint responds correctly\n5. Test rollback with invalid tag (should fail gracefully)\n6. Test GitHub Actions manual rollback trigger\n7. Time the entire rollback process (target <5 min)\n8. Test rollback of individual services",
        "priority": "medium",
        "dependencies": [
          "24"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 38,
        "title": "Implement In-App Purchases with Subscription Management",
        "description": "Implement a complete in-app purchase system for selling subscription plans (monthly/yearly) using StoreKit 2, including trial periods, promotional offers, discount codes, purchase restoration, server-side receipt validation, and proper entitlement management.",
        "details": "## Overview\nImplement a production-ready in-app purchase (IAP) system for the Nutri app using Apple's StoreKit 2 framework. The system will support auto-renewable subscriptions with multiple tiers, trial periods, promotional offers, and proper server-side validation.\n\n## Subscription Tiers (Suggested)\n1. **Nutri Free** - Basic tracking, limited history\n2. **Nutri Pro (Monthly)** - Full features, ML insights, unlimited history\n3. **Nutri Pro (Yearly)** - Same as monthly with discount (~2 months free)\n\n## Technical Requirements\n\n### 1. StoreKit 2 Implementation (React Native / Expo)\n- Use `expo-in-app-purchases` or `react-native-iap` (prefer react-native-iap v12+ for StoreKit 2 support)\n- Implement async/await patterns for all StoreKit operations\n- Handle Transaction.updates listener for real-time purchase updates\n- Implement proper transaction finishing to prevent duplicate charges\n- Support for App Store sandbox testing environment\n\n### 2. Product Configuration\n- Configure products in App Store Connect:\n  - `com.nutri.pro.monthly` - Monthly subscription\n  - `com.nutri.pro.yearly` - Yearly subscription\n- Set up subscription groups for upgrade/downgrade paths\n- Configure introductory offers (free trial, pay-up-front, pay-as-you-go)\n- Set up promotional offers for win-back campaigns\n- Configure offer codes for marketing campaigns\n\n### 3. Trial Periods & Offers\n- Free trial: 7-day trial for new subscribers\n- Introductory pricing: First month at 50% discount\n- Promotional offers: Configurable discounts for lapsed subscribers\n- Offer code redemption: Support App Store offer codes\n- Family Sharing: Proper handling if enabled\n\n### 4. Purchase Flow\n- Display subscription options with localized pricing (use Product.displayPrice)\n- Show trial eligibility status (Product.subscription.isEligibleForIntroOffer)\n- Handle purchase confirmation with biometric/password authentication\n- Process successful purchases and grant entitlements immediately\n- Handle purchase failures gracefully with user-friendly messages\n- Support deferred purchases (Ask to Buy for Family Sharing)\n\n### 5. Restore Purchases\n- Implement \"Restore Purchases\" button in settings/paywall\n- Use Transaction.currentEntitlements for efficient restoration\n- Handle cases where no purchases exist to restore\n- Sync restored purchases with backend\n- Required by App Store Review Guidelines\n\n### 6. Server-Side Validation (Backend)\n- Implement App Store Server API integration (not deprecated verifyReceipt)\n- Use App Store Server Notifications V2 for real-time status updates:\n  - SUBSCRIBED, DID_RENEW, DID_CHANGE_RENEWAL_STATUS\n  - DID_FAIL_TO_RENEW, EXPIRED, REFUND, REVOKE\n  - OFFER_REDEEMED, GRACE_PERIOD_EXPIRED\n- Store subscription status in database with proper schema\n- Handle billing retry state and grace periods\n- Implement JWT-based authentication for App Store Server API\n\n### 7. Entitlement Management\n- Create EntitlementService for checking subscription status\n- Cache entitlements locally with secure storage\n- Sync entitlements on app launch and purchase events\n- Handle offline entitlement checking gracefully\n- Implement feature flags based on subscription tier\n\n### 8. Database Schema (Prisma)\n```prisma\nmodel Subscription {\n  id                    String   @id @default(cuid())\n  userId                String   @unique\n  user                  User     @relation(fields: [userId], references: [id])\n  productId             String   // e.g., \"com.nutri.pro.monthly\"\n  originalTransactionId String   @unique\n  status                SubscriptionStatus\n  expiresAt             DateTime\n  isTrialPeriod         Boolean  @default(false)\n  isIntroOfferPeriod    Boolean  @default(false)\n  autoRenewEnabled      Boolean  @default(true)\n  gracePeriodExpiresAt  DateTime?\n  billingRetryPeriod    Boolean  @default(false)\n  priceLocale           String?\n  priceCurrency         String?\n  priceAmount           Decimal?\n  environment           String   // \"sandbox\" or \"production\"\n  createdAt             DateTime @default(now())\n  updatedAt             DateTime @updatedAt\n  \n  @@index([userId])\n  @@index([originalTransactionId])\n  @@index([status, expiresAt])\n}\n\nenum SubscriptionStatus {\n  ACTIVE\n  EXPIRED\n  IN_GRACE_PERIOD\n  IN_BILLING_RETRY\n  REVOKED\n  REFUNDED\n}\n\nmodel SubscriptionEvent {\n  id                    String   @id @default(cuid())\n  subscriptionId        String\n  subscription          Subscription @relation(fields: [subscriptionId], references: [id])\n  notificationType      String   // App Store notification type\n  subtype               String?\n  transactionId         String\n  originalTransactionId String\n  eventData             Json\n  processedAt           DateTime @default(now())\n  \n  @@index([subscriptionId])\n  @@index([originalTransactionId])\n}\n```\n\n### 9. UI Components\n- **PaywallScreen**: Full-screen subscription offering\n  - Feature comparison between tiers\n  - Localized pricing with trial info\n  - Terms of Service and Privacy Policy links (required)\n  - Restore Purchases button\n- **SubscriptionBadge**: Show Pro status in UI\n- **UpgradePrompt**: Contextual upgrade nudges\n- **ManageSubscriptionScreen**: View current plan, link to App Store management\n\n### 10. Security Considerations\n- Never trust client-side purchase verification alone\n- Always validate with App Store Server API\n- Use App Store Server Notifications for authoritative status\n- Protect webhook endpoint with signature verification\n- Store sensitive data (transaction IDs) securely\n- Implement proper error handling to prevent purchase fraud\n\n### 11. Testing Strategy\n- Use StoreKit Testing in Xcode for local testing\n- Test in App Store Sandbox environment\n- Test all subscription states:\n  - New subscription, renewal, expiration\n  - Trial to paid conversion\n  - Upgrade/downgrade between tiers\n  - Cancellation and re-subscription\n  - Billing issues and grace period\n  - Refunds and revocation\n- Test Family Sharing scenarios if enabled\n- Test Ask to Buy (deferred transactions)\n- Test restore purchases flow\n\n### 12. Analytics & Monitoring\n- Track subscription events for analytics\n- Monitor conversion rates (trial to paid)\n- Track churn and retention metrics\n- Alert on unusual refund patterns\n- Log all webhook processing for debugging\n\n### 13. App Store Review Compliance\n- Include \"Restore Purchases\" functionality\n- Display subscription terms clearly\n- Link to Terms of Service and Privacy Policy\n- Handle all edge cases gracefully\n- Proper error messages for users\n\n## Dependencies\n- Requires App Store Connect configuration\n- Requires Apple Developer Program membership\n- Backend webhook endpoint for notifications\n- Expo development build (not Expo Go) for IAP testing\n\n## Files to Create/Modify\n\n### Mobile (React Native/Expo)\n- `lib/services/purchases/` - Purchase service directory\n  - `index.ts` - Main purchase service\n  - `types.ts` - TypeScript interfaces\n  - `products.ts` - Product ID constants\n  - `entitlements.ts` - Entitlement checking\n- `lib/context/SubscriptionContext.tsx` - React context for subscription state\n- `lib/hooks/useSubscription.ts` - Hook for subscription status\n- `lib/hooks/usePurchases.ts` - Hook for purchase operations\n- `app/paywall.tsx` - Paywall screen\n- `app/subscription.tsx` - Manage subscription screen\n- `lib/components/PaywallCard.tsx` - Subscription option card\n- `lib/components/SubscriptionBadge.tsx` - Pro badge component\n- `lib/components/UpgradePrompt.tsx` - Upgrade nudge component\n- `lib/api/subscriptions.ts` - API client for subscription endpoints\n\n### Backend (Express/Node.js)\n- `server/src/services/subscriptionService.ts` - Subscription business logic\n- `server/src/services/appStoreService.ts` - App Store Server API integration\n- `server/src/controllers/subscriptionController.ts` - Subscription endpoints\n- `server/src/controllers/webhookController.ts` - App Store webhook handler\n- `server/src/routes/subscription.ts` - Subscription routes\n- `server/src/routes/webhook.ts` - Webhook routes\n- `server/src/validation/subscriptionSchemas.ts` - Zod schemas\n- `server/src/middleware/webhookAuth.ts` - Webhook signature verification\n- `server/prisma/schema.prisma` - Add Subscription models\n\n### Configuration\n- `app.json` - Add In-App Purchase capability\n- App Store Connect - Configure products and subscriptions\n\n## Acceptance Criteria\n1. Users can view and purchase subscription plans\n2. Trial periods work correctly with proper eligibility checking\n3. Subscriptions renew automatically and status syncs to backend\n4. Users can restore purchases on new devices\n5. Server validates all purchases before granting access\n6. Webhook processes all App Store notifications correctly\n7. Entitlements are checked efficiently with proper caching\n8. All edge cases (expiration, billing issues, refunds) handled\n9. Analytics track key subscription metrics\n10. Passes App Store review guidelines",
        "testStrategy": "## Testing Strategy\n\n### Unit Tests\n1. **Purchase Service Tests**\n   - Product fetching and caching\n   - Purchase flow state management\n   - Entitlement calculation logic\n   - Error handling for various failure modes\n\n2. **Backend Service Tests**\n   - App Store Server API mock responses\n   - Subscription status transitions\n   - Webhook payload parsing and validation\n   - JWT token generation and validation\n\n3. **Entitlement Logic Tests**\n   - Active subscription detection\n   - Grace period handling\n   - Trial eligibility checking\n   - Feature flag resolution\n\n### Integration Tests\n1. **StoreKit Sandbox Testing**\n   - Full purchase flow with sandbox accounts\n   - Subscription renewal simulation\n   - Trial to paid conversion\n   - Restore purchases flow\n\n2. **Backend Webhook Tests**\n   - Webhook signature verification\n   - Event processing for all notification types\n   - Database state updates\n   - Error recovery scenarios\n\n3. **End-to-End Tests**\n   - Complete purchase journey\n   - Entitlement sync after purchase\n   - Cross-device restore\n   - Subscription management\n\n### Manual Testing Checklist\n- [ ] Purchase monthly subscription\n- [ ] Purchase yearly subscription\n- [ ] Start free trial\n- [ ] Trial expires and converts to paid\n- [ ] Cancel subscription\n- [ ] Re-subscribe after cancellation\n- [ ] Restore purchases on new device\n- [ ] Upgrade from monthly to yearly\n- [ ] Downgrade from yearly to monthly\n- [ ] Handle billing issue/grace period\n- [ ] Refund processing\n- [ ] Family Sharing (if enabled)\n- [ ] Ask to Buy deferred purchase\n- [ ] Offer code redemption\n- [ ] Promotional offer application",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up react-native-iap library and StoreKit 2 integration",
            "description": "Install react-native-iap v12+ with StoreKit 2 support, configure Expo development build for IAP capabilities, implement transaction listener with async/await patterns, and set up proper transaction finishing to prevent duplicate charges.",
            "dependencies": [],
            "details": "Install react-native-iap (v12+) which supports StoreKit 2. Configure app.json with In-App Purchase capability and create Expo development build (IAP doesn't work in Expo Go). Create lib/services/purchases/index.ts with purchase service initialization, implement Transaction.updates listener for real-time updates, set up proper transaction finishing logic, configure sandbox vs production environment detection, and implement error handling for common StoreKit errors (user canceled, network issues, etc.).",
            "status": "pending",
            "testStrategy": "Unit test purchase service initialization, test transaction listener with mock transactions, verify transaction finishing prevents duplicates, test environment detection (sandbox vs production), test error handling for all StoreKit error codes."
          },
          {
            "id": 2,
            "title": "Configure App Store Connect products and subscription groups",
            "description": "Set up subscription products in App Store Connect (com.nutri.pro.monthly, com.nutri.pro.yearly), configure subscription groups for upgrade/downgrade paths, and set up introductory offers and promotional offers.",
            "dependencies": [
              1
            ],
            "details": "In App Store Connect, create subscription group 'Nutri Pro Subscriptions'. Add two products: com.nutri.pro.monthly ($9.99/month) and com.nutri.pro.yearly ($99.99/year, ~17% discount). Configure subscription group settings to allow upgrades/downgrades. Set up introductory offer: 7-day free trial for both tiers. Create promotional offers for win-back campaigns (e.g., 50% off for 3 months). Generate offer codes for marketing. Document all product IDs in lib/services/purchases/products.ts as constants.",
            "status": "pending",
            "testStrategy": "Manual verification in App Store Connect dashboard, test product fetching via StoreKit in sandbox environment, verify subscription group hierarchy allows proper upgrade/downgrade, test introductory offer eligibility detection."
          },
          {
            "id": 3,
            "title": "Implement trial period and promotional offer eligibility checking",
            "description": "Implement logic to check user eligibility for free trials and promotional offers using StoreKit 2's Product.subscription.isEligibleForIntroOffer API, and display trial information in UI.",
            "dependencies": [
              2
            ],
            "details": "Create lib/services/purchases/entitlements.ts with functions to check trial eligibility using Product.subscription.isEligibleForIntroOffer (StoreKit 2 API). Implement promotional offer eligibility checking based on subscription history. Create TrialEligibilityChecker class that caches eligibility status to avoid repeated API calls. Add logic to display trial information ('7-day free trial, then $9.99/month') when eligible, or regular pricing when not eligible. Handle edge cases: never subscribed (eligible), currently subscribed (not eligible), lapsed subscriber (promotional offer eligible).",
            "status": "pending",
            "testStrategy": "Unit test eligibility checking logic with various user states (new, active, lapsed), test caching mechanism, integration test with sandbox accounts (new account should be trial eligible, account with previous subscription should not be), verify correct messaging displayed in UI."
          },
          {
            "id": 4,
            "title": "Build PaywallScreen with subscription options and purchase flow",
            "description": "Create full-screen paywall UI (app/paywall.tsx) displaying subscription tiers with localized pricing, trial eligibility, feature comparison, and purchase buttons with biometric authentication.",
            "dependencies": [
              3
            ],
            "details": "Create app/paywall.tsx as modal screen with: (1) Feature comparison table (Free vs Pro), (2) Subscription option cards showing Product.displayPrice (localized), trial information if eligible, and prominent CTA button, (3) Legal footer with Terms of Service and Privacy Policy links (required by App Store), (4) Restore Purchases button, (5) Loading states during purchase, (6) Success/error handling with user-friendly messages. Implement purchase flow: user taps Subscribe → StoreKit shows confirmation with biometric/password → process transaction → grant entitlements → dismiss paywall. Handle deferred transactions (Ask to Buy). Create lib/components/PaywallCard.tsx for reusable subscription option cards.",
            "status": "pending",
            "testStrategy": "Component test for PaywallScreen rendering, test purchase flow with sandbox account, test biometric authentication prompt, test deferred transaction handling, test error states (user canceled, network error), verify Terms/Privacy links work, test Restore Purchases button."
          },
          {
            "id": 5,
            "title": "Build ManageSubscriptionScreen and SubscriptionBadge component",
            "description": "Create subscription management screen (app/subscription.tsx) showing current plan details and App Store management link, plus a Pro badge component for displaying subscription status throughout the app.",
            "dependencies": [
              4
            ],
            "details": "Create app/subscription.tsx displaying: current subscription tier, renewal date, pricing, trial/intro offer status, auto-renew toggle status (read-only, managed via App Store), 'Manage Subscription' button linking to App Store subscription management (using Linking.openURL with App Store subscription URL), and Restore Purchases button. Create lib/components/SubscriptionBadge.tsx showing 'Pro' badge with styling when user has active subscription. Create lib/components/UpgradePrompt.tsx for contextual upgrade nudges when free users try premium features. Add subscription status to profile screen.",
            "status": "pending",
            "testStrategy": "Component test for ManageSubscriptionScreen, verify App Store link opens correctly, test SubscriptionBadge displays only for Pro users, test UpgradePrompt triggers on premium feature access, integration test with various subscription states."
          },
          {
            "id": 6,
            "title": "Implement Restore Purchases functionality",
            "description": "Implement restore purchases flow using Transaction.currentEntitlements to sync past purchases and validate them with backend, required by App Store Review Guidelines.",
            "dependencies": [
              5
            ],
            "details": "In lib/services/purchases/index.ts, implement restorePurchases() function using Transaction.currentEntitlements (StoreKit 2 efficient API, not deprecated restoreCompletedTransactions). Flow: (1) User taps 'Restore Purchases', (2) Show loading indicator, (3) Fetch currentEntitlements from StoreKit, (4) For each entitlement, extract originalTransactionId and send to backend for validation, (5) Backend validates with App Store Server API, (6) Sync subscription status to local state, (7) Show success message ('Subscription restored') or info message ('No purchases to restore'). Handle errors gracefully. Add 'Restore Purchases' button to both PaywallScreen and ManageSubscriptionScreen.",
            "status": "pending",
            "testStrategy": "Unit test restore logic with mock transactions, integration test with sandbox account (purchase on device A, restore on device B), test no purchases case, test error handling (network failure), verify backend validation called for each transaction."
          },
          {
            "id": 7,
            "title": "Implement backend App Store Server API integration for receipt validation",
            "description": "Create backend service to validate purchases using the modern App Store Server API (NOT deprecated verifyReceipt), implement JWT-based authentication, and query transaction/subscription status.",
            "dependencies": [
              6
            ],
            "details": "Create server/src/services/appStoreService.ts implementing: (1) App Store Server API client using JWT authentication (generate JWT with private key from App Store Connect), (2) getTransactionInfo(transactionId) endpoint to fetch transaction details, (3) getSubscriptionStatus(originalTransactionId) endpoint to check current subscription state, (4) validateTransaction() function that checks signature and decodes JWSTransaction, (5) Environment detection (sandbox vs production, use different API endpoints). Store App Store Connect API credentials in environment variables (KEY_ID, ISSUER_ID, PRIVATE_KEY). Never use deprecated verifyReceipt endpoint. Create server/src/validation/subscriptionSchemas.ts with Zod schemas for validation.",
            "status": "pending",
            "testStrategy": "Unit test JWT generation, test API client with sandbox environment, test transaction validation with test transaction data, test getSubscriptionStatus with various states (active, expired, grace period), verify proper error handling for invalid transactions."
          },
          {
            "id": 8,
            "title": "Implement App Store Server Notifications V2 webhook with signature verification",
            "description": "Create webhook endpoint to receive real-time subscription status updates from Apple (renewals, cancellations, refunds), implement cryptographic signature verification, and process all notification types.",
            "dependencies": [
              7
            ],
            "details": "Create server/src/controllers/webhookController.ts with POST /api/webhooks/app-store endpoint. Implement signature verification using Apple's public key (fetch from Apple's JWKS endpoint, verify JWS signature). Parse notification payload (signedPayload is JWS, decode and verify). Handle all App Store Server Notifications V2 types: SUBSCRIBED, DID_RENEW, DID_CHANGE_RENEWAL_STATUS, DID_FAIL_TO_RENEW, EXPIRED, REFUND, REVOKE, OFFER_REDEEMED, GRACE_PERIOD_EXPIRED. For each notification, update Subscription record in database and create SubscriptionEvent audit log. Implement idempotency using notification UUID. Create server/src/middleware/webhookAuth.ts for signature verification middleware. Configure webhook URL in App Store Connect.",
            "status": "pending",
            "testStrategy": "Unit test signature verification with test JWS tokens, test notification parsing for all types, test idempotency (duplicate notifications), integration test with App Store Connect sandbox (trigger renewals/cancellations), verify SubscriptionEvent audit trail created, test error handling for invalid signatures."
          },
          {
            "id": 9,
            "title": "Create Prisma database schema for Subscription and SubscriptionEvent models",
            "description": "Add Subscription and SubscriptionEvent models to Prisma schema with all required fields for subscription lifecycle tracking, run migrations, and generate Prisma client.",
            "dependencies": [
              8
            ],
            "details": "Add to server/prisma/schema.prisma: (1) Subscription model with fields: id, userId (unique), productId, originalTransactionId (unique), status (enum: ACTIVE, EXPIRED, IN_GRACE_PERIOD, IN_BILLING_RETRY, REVOKED, REFUNDED), expiresAt, isTrialPeriod, isIntroOfferPeriod, autoRenewEnabled, gracePeriodExpiresAt, billingRetryPeriod, priceLocale, priceCurrency, priceAmount (Decimal), environment (sandbox/production), timestamps. (2) SubscriptionEvent model with fields: id, subscriptionId, notificationType, subtype, transactionId, originalTransactionId, eventData (Json), processedAt. Add indexes for efficient queries: userId, originalTransactionId, status+expiresAt composite. Add relation to User model (user.subscription). Run 'npm run db:generate' and 'npm run db:migrate' to apply schema changes.",
            "status": "pending",
            "testStrategy": "Verify Prisma schema compiles without errors, test migration runs successfully, verify indexes created in PostgreSQL, unit test Subscription model CRUD operations, test foreign key constraints (userId references User)."
          },
          {
            "id": 10,
            "title": "Implement entitlement service with secure caching and offline support",
            "description": "Create EntitlementService to check subscription status efficiently with secure local caching, sync entitlements on app launch and purchase events, handle offline scenarios, and implement feature flags based on subscription tier.",
            "dependencies": [
              9
            ],
            "details": "Create lib/services/purchases/entitlements.ts with EntitlementService class: (1) checkEntitlement(feature) returns boolean based on subscription status, (2) syncEntitlements() fetches current status from backend and caches in Expo SecureStore, (3) getCachedEntitlements() reads from SecureStore for offline support (with staleness check), (4) Feature flags: UNLIMITED_HISTORY, ML_INSIGHTS, ADVANCED_ANALYTICS mapped to subscription tiers. Create lib/context/SubscriptionContext.tsx providing subscription status to entire app. Create lib/hooks/useSubscription.ts hook returning { isPro, isTrial, expiresAt, syncEntitlements }. Call syncEntitlements() on app launch, after successful purchase, and on restore. Implement cache expiration (refresh every 24 hours). Handle offline mode: use cached entitlements if < 7 days old.",
            "status": "pending",
            "testStrategy": "Unit test entitlement checking logic for each feature flag, test caching mechanism (save/load from SecureStore), test staleness check, test offline behavior with old/recent cache, integration test: purchase → syncEntitlements → verify Pro features unlocked, test subscription expiration handling."
          },
          {
            "id": 11,
            "title": "Set up StoreKit Testing configuration and Sandbox testing flows",
            "description": "Configure StoreKit Testing in Xcode for local testing without App Store, create Sandbox test accounts, and test all subscription lifecycle scenarios (new subscription, renewal, trial conversion, upgrade/downgrade, cancellation, billing issues, refunds).",
            "dependencies": [
              10
            ],
            "details": "Configure StoreKit Configuration file (.storekit) in Xcode with test products matching App Store Connect (com.nutri.pro.monthly, com.nutri.pro.yearly). Set up subscription durations (monthly = 5 minutes, yearly = 1 hour for faster testing). Create Sandbox test accounts in App Store Connect with different regions (US, UK, EU for currency testing). Test scenarios: (1) New subscription with trial → conversion to paid, (2) Successful renewal, (3) Expiration after cancellation, (4) Upgrade from monthly to yearly, (5) Downgrade from yearly to monthly, (6) Billing failure → grace period → billing retry, (7) Refund processing, (8) Family Sharing if enabled, (9) Ask to Buy (deferred transactions), (10) Restore purchases on new device. Document testing checklist in ml-service or server README.",
            "status": "pending",
            "testStrategy": "Manual testing checklist for all scenarios, verify StoreKit Testing transactions appear in Transaction Manager, test with multiple sandbox accounts, test currency localization, verify webhook notifications received for each state change, test grace period and billing retry states."
          },
          {
            "id": 12,
            "title": "Integrate subscription analytics and ensure App Store Review compliance",
            "description": "Add analytics tracking for subscription events (impressions, conversions, churn), implement monitoring for refund patterns, and ensure full compliance with App Store Review Guidelines (restore purchases, terms display, error handling).",
            "dependencies": [
              11
            ],
            "details": "Analytics integration: Track events (paywall_viewed, subscription_started, trial_started, trial_converted, subscription_renewed, subscription_canceled, subscription_expired, purchase_restored, upgrade_completed, downgrade_completed) using existing analytics service or add new one (e.g., Mixpanel, Amplitude). Create server/src/services/subscriptionAnalyticsService.ts to calculate metrics: conversion rate (trial → paid), churn rate, MRR (Monthly Recurring Revenue), LTV (Lifetime Value). Set up alerts for unusual refund patterns (>5% refund rate). App Store compliance checklist: (1) Restore Purchases button visible and functional, (2) Terms of Service and Privacy Policy links on paywall, (3) Clear subscription terms display (price, duration, auto-renewal), (4) Graceful error handling with user-friendly messages, (5) No misleading marketing claims. Add subscription_tier field to User model for feature flag checks.",
            "status": "pending",
            "testStrategy": "Verify analytics events fire correctly for each user action, test metrics calculation with test data, verify refund alert triggers at threshold, compliance checklist review with screenshots, test all required UI elements present (Restore button, legal links, pricing display), verify error messages are user-friendly."
          }
        ]
      },
      {
        "id": 39,
        "title": "Build Admin Panel for User and Subscription Management",
        "description": "Build a secure, internal admin panel for managing users, subscriptions, webhook events, and analytics. Essential for customer support, subscription troubleshooting, GDPR compliance, and business analytics. Phased approach starting with MVP features critical for subscription launch.",
        "details": "## Overview\nBuild a production-ready admin panel for the Nutri app to handle customer support, subscription management, compliance requirements, and business analytics. The admin panel is essential for operating a subscription-based app - while Apple handles billing, customer experience and support require internal tooling.\n\n## Why This Is Needed (Not Optional)\n\n1. **Customer Support**: Users will report \"I paid but don't have access\" - need instant lookup\n2. **Subscription Troubleshooting**: Debug webhook failures, verify transaction status\n3. **GDPR Compliance**: Legal requirement for data export/deletion tooling\n4. **Business Analytics**: MRR, churn, conversion - not available at this granularity in App Store Connect\n5. **Operational Efficiency**: Feature flags, food database management, ML monitoring\n\n## Technology Stack (2025 Best Practices)\n\n### Recommended: Next.js 14+ Admin App\n- **Framework**: Next.js 14+ with App Router (React Server Components)\n- **UI Library**: shadcn/ui + Tailwind CSS (consistent with React Native web styling)\n- **Charts/Dashboards**: Tremor (built on shadcn/ui) or Recharts\n- **Tables**: TanStack Table v8 with server-side pagination\n- **Forms**: React Hook Form + Zod (same as backend validation)\n- **Authentication**: NextAuth.js v5 with separate admin user table\n- **Deployment**: Vercel (or same infrastructure as backend)\n\n### Alternative Options Considered:\n- **Retool/Appsmith**: Faster to build but vendor lock-in, less customizable\n- **React Admin**: Mature but opinionated, less modern DX\n- **Refine**: Good option, similar to our recommendation\n\n### Why Next.js:\n- Same React skills as mobile app\n- Server Components for secure data fetching\n- API routes can proxy to main backend\n- Excellent DX with hot reload\n- Easy deployment\n\n## Architecture\n\n```\nadmin-panel/                    # Separate Next.js app\n├── app/\n│   ├── (auth)/\n│   │   ├── login/page.tsx     # Admin login\n│   │   └── layout.tsx\n│   ├── (dashboard)/\n│   │   ├── layout.tsx         # Dashboard layout with sidebar\n│   │   ├── page.tsx           # Overview dashboard\n│   │   ├── users/\n│   │   │   ├── page.tsx       # User list with search/filter\n│   │   │   └── [id]/page.tsx  # User detail view\n│   │   ├── subscriptions/\n│   │   │   ├── page.tsx       # Subscription list\n│   │   │   └── [id]/page.tsx  # Subscription detail\n│   │   ├── webhooks/\n│   │   │   └── page.tsx       # Webhook event logs\n│   │   ├── analytics/\n│   │   │   └── page.tsx       # Subscription analytics\n│   │   └── settings/\n│   │       ├── feature-flags/page.tsx\n│   │       └── team/page.tsx  # Admin user management\n│   └── api/                   # Proxy to main backend\n├── components/\n│   ├── ui/                    # shadcn/ui components\n│   ├── dashboard/             # Dashboard-specific components\n│   └── data-table/            # Reusable table components\n├── lib/\n│   ├── api.ts                 # Backend API client\n│   ├── auth.ts                # NextAuth configuration\n│   └── utils.ts\n└── middleware.ts              # Auth middleware\n```\n\n## Phase 1: MVP (Launch Critical) - 2-3 weeks\n\n### 1.1 Admin Authentication & Security\n- Separate AdminUser table in database (NOT shared with app users)\n- Email/password with mandatory MFA (TOTP)\n- Role-based access: SUPER_ADMIN, SUPPORT, VIEWER\n- Session-based auth with secure cookies\n- Audit logging for ALL admin actions\n- IP allowlisting option for production\n- Rate limiting on login endpoint\n\n### 1.2 User Management\n- **User Search**: By email, ID, name (instant search with debounce)\n- **User List**: Paginated, sortable, filterable table\n- **User Detail View**:\n  - Profile information (email, name, created date)\n  - Subscription status (tier, expires, trial info)\n  - Recent activity (meals logged, health metrics)\n  - Account actions: Reset password link, disable account\n- **GDPR Actions**:\n  - Export user data (JSON download)\n  - Delete user account (with confirmation, cascades to all data)\n\n### 1.3 Subscription Management\n- **Subscription List**: All subscriptions with status filter\n- **Subscription Detail**:\n  - Current status, product, expiration\n  - Transaction history (all originalTransactionIds)\n  - Webhook events related to this subscription\n  - Auto-renew status\n- **Manual Actions** (SUPER_ADMIN only):\n  - Grant Pro access (specify duration, reason logged)\n  - Extend subscription (specify days, reason logged)\n  - Revoke access (with reason)\n- **Subscription Lookup**: By originalTransactionId for support tickets\n\n### 1.4 Webhook Event Viewer\n- **Event List**: All App Store webhook events, newest first\n- **Filters**: By notification type, date range, subscription ID\n- **Event Detail**: Full JSON payload, processing status, errors\n- **Retry Failed**: Button to reprocess failed webhooks\n- **Search**: By originalTransactionId for debugging\n\n### 1.5 Basic Analytics Dashboard\n- **Subscription Metrics** (real-time):\n  - Total active subscribers (by tier)\n  - MRR (Monthly Recurring Revenue)\n  - New subscriptions today/this week/this month\n  - Cancellations/churn today/this week/this month\n- **Trial Metrics**:\n  - Active trials\n  - Trial conversion rate (7-day, 30-day)\n- **Charts**:\n  - Subscribers over time (line chart)\n  - Revenue over time (line chart)\n  - Subscription status distribution (pie chart)\n\n## Phase 2: Growth Features - 2-3 weeks\n\n### 2.1 Advanced Analytics\n- Cohort retention analysis\n- LTV (Lifetime Value) calculation\n- Churn prediction indicators\n- Geographic distribution\n- Revenue by product (monthly vs yearly)\n- Refund rate monitoring with alerts\n\n### 2.2 Feature Flags Management\n- Create/edit/delete feature flags\n- Target by: user ID, subscription tier, percentage rollout\n- Flag types: boolean, string, number, JSON\n- Instant propagation to app (webhook or polling)\n- Audit log of flag changes\n\n### 2.3 Food Database Management\n- View/search food database entries\n- Edit nutritional information\n- Add new foods\n- Flag/review user-submitted entries\n- Bulk import from CSV\n\n### 2.4 Push Notification Management\n- Send push notifications to segments\n- Segments: all users, Pro users, trial users, inactive users\n- Schedule notifications\n- View delivery stats\n\n## Phase 3: Scale Features - 2+ weeks\n\n### 3.1 Team Management\n- Invite admin users\n- Role assignment (SUPER_ADMIN, SUPPORT, VIEWER, ANALYST)\n- Permission matrix by role\n- Activity log per admin user\n- Disable/remove admin users\n\n### 3.2 A/B Testing Dashboard\n- View active experiments\n- Create new experiments (paywall variants, onboarding flows)\n- View results with statistical significance\n- Declare winners and roll out\n\n### 3.3 ML Model Monitoring\n- Model performance metrics over time\n- Inference latency tracking\n- Error rate monitoring\n- Model version management\n- A/B test model versions\n\n### 3.4 Advanced Security\n- Login anomaly detection\n- Suspicious activity alerts\n- API key management for integrations\n- Webhook secret rotation\n\n## Database Schema Additions\n\n```prisma\n// Add to server/prisma/schema.prisma\n\nmodel AdminUser {\n  id            String   @id @default(cuid())\n  email         String   @unique\n  passwordHash  String\n  name          String\n  role          AdminRole @default(SUPPORT)\n  mfaSecret     String?  // TOTP secret\n  mfaEnabled    Boolean  @default(false)\n  isActive      Boolean  @default(true)\n  lastLoginAt   DateTime?\n  lastLoginIp   String?\n  createdAt     DateTime @default(now())\n  updatedAt     DateTime @updatedAt\n  \n  auditLogs     AdminAuditLog[]\n  \n  @@index([email])\n}\n\nenum AdminRole {\n  SUPER_ADMIN  // Full access, can manage other admins\n  SUPPORT      // User/subscription management, no settings\n  ANALYST      // Read-only analytics access\n  VIEWER       // Read-only all access\n}\n\nmodel AdminAuditLog {\n  id          String   @id @default(cuid())\n  adminUserId String\n  adminUser   AdminUser @relation(fields: [adminUserId], references: [id])\n  action      String   // e.g., \"USER_LOOKUP\", \"SUBSCRIPTION_GRANT\", \"USER_DELETE\"\n  targetType  String?  // e.g., \"User\", \"Subscription\"\n  targetId    String?  // ID of affected record\n  details     Json?    // Additional context\n  ipAddress   String\n  userAgent   String?\n  createdAt   DateTime @default(now())\n  \n  @@index([adminUserId])\n  @@index([action])\n  @@index([targetType, targetId])\n  @@index([createdAt])\n}\n\nmodel FeatureFlag {\n  id          String   @id @default(cuid())\n  key         String   @unique  // e.g., \"new_paywall_design\"\n  name        String\n  description String?\n  type        FeatureFlagType @default(BOOLEAN)\n  value       Json     // Default value\n  isEnabled   Boolean  @default(false)\n  targeting   Json?    // Rules for user targeting\n  createdAt   DateTime @default(now())\n  updatedAt   DateTime @updatedAt\n  \n  @@index([key])\n}\n\nenum FeatureFlagType {\n  BOOLEAN\n  STRING\n  NUMBER\n  JSON\n}\n```\n\n## Backend API Additions\n\nCreate admin-specific endpoints in server/src/routes/admin/:\n\n```typescript\n// Admin authentication\nPOST   /api/admin/auth/login\nPOST   /api/admin/auth/logout\nPOST   /api/admin/auth/mfa/setup\nPOST   /api/admin/auth/mfa/verify\nGET    /api/admin/auth/me\n\n// User management\nGET    /api/admin/users              // List with pagination, search, filters\nGET    /api/admin/users/:id          // User detail with subscription\nPOST   /api/admin/users/:id/export   // GDPR data export\nDELETE /api/admin/users/:id          // GDPR deletion\nPOST   /api/admin/users/:id/disable  // Disable account\n\n// Subscription management\nGET    /api/admin/subscriptions      // List with filters\nGET    /api/admin/subscriptions/:id  // Detail with events\nPOST   /api/admin/subscriptions/:id/grant   // Manual grant\nPOST   /api/admin/subscriptions/:id/extend  // Extend duration\nPOST   /api/admin/subscriptions/:id/revoke  // Revoke access\nGET    /api/admin/subscriptions/lookup?txn=XXX  // By transaction ID\n\n// Webhook events\nGET    /api/admin/webhooks           // List with filters\nGET    /api/admin/webhooks/:id       // Event detail\nPOST   /api/admin/webhooks/:id/retry // Retry processing\n\n// Analytics\nGET    /api/admin/analytics/subscriptions  // Subscription metrics\nGET    /api/admin/analytics/revenue        // Revenue metrics\nGET    /api/admin/analytics/trials         // Trial metrics\n\n// Feature flags\nGET    /api/admin/feature-flags\nPOST   /api/admin/feature-flags\nPUT    /api/admin/feature-flags/:id\nDELETE /api/admin/feature-flags/:id\n\n// Audit logs\nGET    /api/admin/audit-logs         // List with filters\n```\n\n## Security Requirements (Non-Negotiable)\n\n1. **Authentication**\n   - Separate admin user table (NEVER share with app users)\n   - Mandatory MFA for all admin accounts\n   - Session expiration: 8 hours inactive, 24 hours max\n   - Secure cookie flags: HttpOnly, Secure, SameSite=Strict\n\n2. **Authorization**\n   - RBAC enforced on every endpoint\n   - Principle of least privilege\n   - SUPER_ADMIN required for: user deletion, subscription grants, admin management\n\n3. **Audit Logging**\n   - Log ALL admin actions with: who, what, when, IP, user agent\n   - Immutable audit trail (no deletes/updates)\n   - Retention: 2 years minimum\n\n4. **Network Security**\n   - Separate subdomain: admin.nutri.app\n   - HTTPS only (HSTS enabled)\n   - Consider IP allowlisting for production\n   - Rate limiting on all endpoints\n\n5. **Data Protection**\n   - PII access logged\n   - Data exports encrypted\n   - No PII in URL parameters\n   - Mask sensitive data in logs\n\n## Files to Create\n\n### Admin Panel (New Next.js App)\n```\nadmin-panel/\n├── package.json\n├── next.config.js\n├── tailwind.config.js\n├── tsconfig.json\n├── .env.local\n├── middleware.ts\n├── app/\n│   ├── layout.tsx\n│   ├── (auth)/login/page.tsx\n│   ├── (dashboard)/\n│   │   ├── layout.tsx\n│   │   ├── page.tsx                 # Dashboard overview\n│   │   ├── users/page.tsx\n│   │   ├── users/[id]/page.tsx\n│   │   ├── subscriptions/page.tsx\n│   │   ├── subscriptions/[id]/page.tsx\n│   │   ├── webhooks/page.tsx\n│   │   └── analytics/page.tsx\n├── components/\n│   ├── ui/                          # shadcn/ui\n│   ├── layout/sidebar.tsx\n│   ├── layout/header.tsx\n│   ├── users/user-table.tsx\n│   ├── users/user-detail.tsx\n│   ├── subscriptions/subscription-table.tsx\n│   ├── subscriptions/grant-modal.tsx\n│   ├── webhooks/event-table.tsx\n│   ├── analytics/metrics-cards.tsx\n│   └── analytics/charts.tsx\n└── lib/\n    ├── api.ts\n    ├── auth.ts\n    └── utils.ts\n```\n\n### Backend Additions\n```\nserver/src/\n├── controllers/adminController.ts\n├── services/adminService.ts\n├── services/adminAuthService.ts\n├── services/adminAnalyticsService.ts\n├── routes/admin.ts\n├── middleware/adminAuth.ts\n├── middleware/adminAudit.ts\n└── validation/adminSchemas.ts\n```\n\n## Acceptance Criteria\n\n### Phase 1 (MVP)\n1. Admin can log in with MFA\n2. Admin can search and view user details\n3. Admin can view subscription status for any user\n4. Admin can manually grant/extend/revoke subscriptions\n5. Admin can view webhook event history\n6. Admin can export user data (GDPR)\n7. Admin can delete user account (GDPR)\n8. All actions are audit logged\n9. Role-based access control enforced\n10. Dashboard shows key subscription metrics\n\n### Phase 2\n11. Feature flags can be managed via UI\n12. Advanced analytics dashboards functional\n13. Food database can be edited\n14. Push notifications can be sent\n\n### Phase 3\n15. Multiple admin users with role management\n16. A/B testing dashboard functional\n17. ML model monitoring integrated",
        "testStrategy": "## Testing Strategy\n\n### Unit Tests\n1. **Admin Authentication**\n   - Password hashing and verification\n   - MFA token generation and validation\n   - Session management\n   - Role-based permission checks\n\n2. **Admin Services**\n   - User search and filtering logic\n   - Subscription grant/extend/revoke logic\n   - Analytics calculations (MRR, churn rate)\n   - Data export generation\n\n3. **Audit Logging**\n   - All actions create audit log entries\n   - Correct data captured (IP, user agent, details)\n\n### Integration Tests\n1. **Admin API Endpoints**\n   - Authentication flow with MFA\n   - CRUD operations for all resources\n   - Permission enforcement by role\n   - Pagination and filtering\n\n2. **Subscription Management**\n   - Manual grant creates correct database records\n   - Extension updates expiration correctly\n   - Revocation removes entitlements\n\n3. **Webhook Retry**\n   - Failed webhooks can be reprocessed\n   - Correct handling of retry results\n\n### E2E Tests\n1. **Admin Login Flow**\n   - Login with valid credentials + MFA\n   - Session persistence\n   - Logout clears session\n\n2. **User Management Flow**\n   - Search for user by email\n   - View user details\n   - Export user data\n   - Delete user (with cascading)\n\n3. **Subscription Flow**\n   - Find user with subscription issue\n   - View webhook history\n   - Grant extended access\n   - Verify audit log created\n\n### Security Tests\n1. **Authentication**\n   - Brute force protection (rate limiting)\n   - Invalid MFA rejected\n   - Session fixation prevention\n\n2. **Authorization**\n   - VIEWER cannot perform write operations\n   - SUPPORT cannot access admin management\n   - Only SUPER_ADMIN can delete users\n\n3. **Audit Trail**\n   - Cannot delete or modify audit logs\n   - All sensitive actions logged\n\n### Manual Testing Checklist\n- [ ] Admin login with MFA\n- [ ] User search by email\n- [ ] User detail view shows subscription\n- [ ] Grant Pro access to free user\n- [ ] Extend subscription by 30 days\n- [ ] View webhook event history\n- [ ] Retry failed webhook\n- [ ] Export user data\n- [ ] Delete user account\n- [ ] Verify audit log entries\n- [ ] Test role-based restrictions\n- [ ] Dashboard metrics accurate\n- [ ] Charts render correctly",
        "status": "pending",
        "dependencies": [
          38
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Next.js 14+ admin app with App Router and shadcn/ui",
            "description": "Create a new Next.js 14+ application in admin-panel/ directory with TypeScript, Tailwind CSS, App Router, and shadcn/ui component library. Configure project structure following the architecture specified in task details.",
            "dependencies": [],
            "details": "1. Run `npx create-next-app@latest admin-panel` with TypeScript, Tailwind, App Router\n2. Install shadcn/ui: `npx shadcn-ui@latest init`\n3. Install dependencies: `npm install @tanstack/react-table tremor recharts react-hook-form zod axios next-auth@beta`\n4. Create folder structure: app/(auth), app/(dashboard), components/ui, components/layout, lib/\n5. Configure tailwind.config.js to match React Native web styling tokens from lib/theme/colors.ts\n6. Create .env.local with NEXTAUTH_SECRET, NEXTAUTH_URL, API_URL (pointing to http://localhost:3000)\n7. Set up tsconfig.json with strict mode (zero 'any' types policy)\n8. Create lib/api.ts with axios client configured to proxy to backend API\n9. Add middleware.ts placeholder for auth protection\n10. Verify build: `npm run build` succeeds",
            "status": "pending",
            "testStrategy": "Run `npm run dev` and verify Next.js app loads at localhost:3001. Verify TypeScript compilation with `npx tsc --noEmit`. Test that shadcn/ui components can be imported. Verify Tailwind CSS classes render correctly."
          },
          {
            "id": 2,
            "title": "Extend Prisma schema with AdminUser and AdminAuditLog models",
            "description": "Add AdminUser, AdminAuditLog, and FeatureFlag models to server/prisma/schema.prisma with proper indexes, relations, and enum types. Follow existing schema conventions.",
            "dependencies": [],
            "details": "1. Add AdminRole enum (SUPER_ADMIN, SUPPORT, ANALYST, VIEWER)\n2. Add FeatureFlagType enum (BOOLEAN, STRING, NUMBER, JSON)\n3. Create AdminUser model with fields: id (cuid), email (unique), passwordHash, name, role, mfaSecret, mfaEnabled, isActive, lastLoginAt, lastLoginIp, createdAt, updatedAt\n4. Add indexes: @@index([email]) on AdminUser\n5. Create AdminAuditLog model with fields: id (cuid), adminUserId, action, targetType, targetId, details (Json), ipAddress, userAgent, createdAt\n6. Add relation: AdminUser.auditLogs (one-to-many)\n7. Add indexes on AdminAuditLog: @@index([adminUserId]), @@index([action]), @@index([targetType, targetId]), @@index([createdAt])\n8. Create FeatureFlag model with fields: id (cuid), key (unique), name, description, type, value (Json), isEnabled, targeting (Json), createdAt, updatedAt\n9. Add @@index([key]) on FeatureFlag\n10. Run `npm run db:generate` to generate Prisma client\n11. Run `npm run db:push` (dev) or create migration with `npm run db:migrate`",
            "status": "pending",
            "testStrategy": "Verify Prisma client generates without errors. Run `npx prisma studio` and confirm new models appear. Create a test AdminUser record via Prisma Studio and verify all fields save correctly. Check indexes are created in PostgreSQL."
          },
          {
            "id": 3,
            "title": "Implement admin authentication backend with MFA (TOTP) support",
            "description": "Create admin authentication service with bcrypt password hashing, JWT session tokens, and TOTP-based MFA. Build endpoints for login, logout, MFA setup, and MFA verification.",
            "dependencies": [
              2
            ],
            "details": "1. Install dependencies in server/: `npm install speakeasy qrcode @types/speakeasy @types/qrcode`\n2. Create server/src/services/adminAuthService.ts with functions: loginAdmin(email, password), verifyMFA(adminUserId, token), setupMFA(adminUserId), generateSessionToken(adminUser)\n3. Use bcryptjs (already in deps) for password verification\n4. Use speakeasy.generateSecret() for MFA setup, return QR code data URL using qrcode library\n5. Session tokens: JWT with payload { adminUserId, role, sessionId } signed with JWT_SECRET, 8-hour expiration\n6. Create server/src/controllers/adminAuthController.ts with handlers: login (POST), logout (POST), setupMFA (POST), verifyMFA (POST), getMe (GET)\n7. Create server/src/validation/adminSchemas.ts with Zod schemas: adminLoginSchema, adminMFASetupSchema, adminMFAVerifySchema\n8. Add constants to server/src/config/constants.ts: ADMIN_SESSION_EXPIRY = '8h', ADMIN_SESSION_MAX = '24h'\n9. Store session metadata (sessionId, expiresAt) in AdminUser or separate AdminSession table\n10. Return { token, requiresMFA, qrCode? } from login endpoint",
            "status": "pending",
            "testStrategy": "Unit tests in server/src/__tests__/adminAuth.test.ts: test password verification, MFA secret generation, token verification, session expiration. Integration test: POST /api/admin/auth/login with correct/incorrect credentials. Test MFA flow end-to-end with speakeasy.totp.verify()."
          },
          {
            "id": 4,
            "title": "Configure NextAuth.js v5 with credentials provider for admin authentication",
            "description": "Set up NextAuth.js v5 in admin panel with custom credentials provider that calls backend admin auth API, handles MFA flow, and manages admin sessions with proper security.",
            "dependencies": [
              1,
              3
            ],
            "details": "1. Create admin-panel/lib/auth.ts with NextAuth configuration\n2. Configure credentials provider to call backend POST /api/admin/auth/login\n3. Handle MFA flow: if requiresMFA=true in response, store pendingMfaToken in session and redirect to MFA page\n4. On MFA verification, call POST /api/admin/auth/mfa/verify and complete sign-in\n5. Store admin JWT token in session callbacks (jwt, session)\n6. Configure session strategy: 'jwt', maxAge: 8 hours\n7. Set cookies: { secure: true, httpOnly: true, sameSite: 'strict' }\n8. Create admin-panel/middleware.ts to protect /dashboard routes with NextAuth middleware\n9. Implement role-based access control in middleware (check session.user.role)\n10. Create admin-panel/app/api/auth/[...nextauth]/route.ts with NextAuth handler\n11. Add NEXTAUTH_SECRET to .env.local (generate with `openssl rand -base64 32`)\n12. Create types in admin-panel/lib/types.ts for AdminUser (id, email, name, role)",
            "status": "pending",
            "testStrategy": "Test login flow: navigate to /login, submit credentials, verify session cookie is set. Test protected routes redirect to /login when unauthenticated. Test MFA flow if enabled. Verify session expires after 8 hours. Test role-based access (VIEWER cannot access SUPER_ADMIN routes)."
          },
          {
            "id": 5,
            "title": "Build admin login UI with MFA flow and session management",
            "description": "Create login page with email/password form, MFA verification page, and session management UI using shadcn/ui components and react-hook-form with Zod validation.",
            "dependencies": [
              4
            ],
            "details": "1. Create admin-panel/app/(auth)/login/page.tsx with email/password form\n2. Use shadcn/ui components: Card, Input, Button, Label from `components/ui/`\n3. Install react-hook-form and integrate with Zod: `@hookform/resolvers/zod`\n4. Create login form schema matching backend adminLoginSchema\n5. On submit, call signIn('credentials', { email, password })\n6. Handle MFA required response: redirect to /auth/mfa page\n7. Create admin-panel/app/(auth)/mfa/page.tsx for MFA token input (6-digit code)\n8. Display QR code if setupMFA=true (first-time setup)\n9. Use speakeasy-compatible TOTP input (6 digits, numeric only)\n10. Show error states with shadcn/ui Alert component (invalid credentials, MFA failed)\n11. Create admin-panel/app/(auth)/layout.tsx with centered auth card design\n12. Add loading states with shadcn/ui Spinner during API calls\n13. Redirect to /dashboard on successful authentication",
            "status": "pending",
            "testStrategy": "Manual testing: test login with valid/invalid credentials, verify error messages display. Test MFA setup flow with QR code scan using Google Authenticator app. Test MFA verification with correct/incorrect codes. Verify redirect to /dashboard on success. Test form validation (email format, required fields)."
          },
          {
            "id": 6,
            "title": "Create RBAC middleware and audit logging middleware for admin API",
            "description": "Implement role-based access control middleware and comprehensive audit logging middleware for all admin endpoints. Every admin action must be logged with who, what, when, IP, and user agent.",
            "dependencies": [
              3
            ],
            "details": "1. Create server/src/middleware/adminAuth.ts with requireAdmin(roles?: AdminRole[]) middleware\n2. Verify JWT token from Authorization header, decode and validate\n3. Load AdminUser from database, check isActive=true\n4. If roles specified, verify adminUser.role is in allowed roles\n5. Attach req.adminUser to request object (extend Express Request type)\n6. Return 401 if token invalid/expired, 403 if role not allowed\n7. Create server/src/middleware/adminAudit.ts with auditLog(action: string) middleware factory\n8. Capture: adminUserId, action, targetType (from route), targetId (from req.params), details (from req.body), ipAddress (req.ip), userAgent (req.headers['user-agent'])\n9. Insert AdminAuditLog record asynchronously (don't block response)\n10. Use res.on('finish') to log after response sent\n11. Create server/src/types/express.d.ts to extend Express Request with adminUser property\n12. Add audit logging to ALL admin endpoints (chain auditLog middleware after requireAdmin)",
            "status": "pending",
            "testStrategy": "Unit tests: verify requireAdmin allows valid JWT, rejects expired token, enforces role restrictions. Test auditLog creates database records with correct fields. Integration test: call admin endpoint, verify AdminAuditLog entry created. Test IP/user-agent capture. Test async logging doesn't delay response."
          },
          {
            "id": 7,
            "title": "Build user search and list backend API with pagination and filters",
            "description": "Create GET /api/admin/users endpoint with search (email, name), pagination, sorting, and filtering (subscription status, account status). Return user list with subscription info.",
            "dependencies": [
              6
            ],
            "details": "1. Create server/src/controllers/adminUserController.ts with listUsers handler\n2. Create server/src/services/adminUserService.ts with getUserList(params) function\n3. Query parameters: search (string), page (number), limit (number, max 100, default 20), sortBy (createdAt, email, name), sortOrder (asc, desc), status (active, disabled), subscriptionStatus (active, trial, expired, none)\n4. Use Prisma where clause with OR for search: { OR: [{ email: { contains: search, mode: 'insensitive' } }, { name: { contains: search } }] }\n5. Include subscription data with include: { subscriptions: { where: { status: 'ACTIVE' }, take: 1 } }\n6. Calculate total count for pagination metadata\n7. Return { users: [], pagination: { page, limit, total, totalPages } }\n8. Create Zod schema in server/src/validation/adminSchemas.ts: listUsersQuerySchema\n9. Apply RBAC: requireAdmin() (all roles can list users)\n10. Apply audit logging: auditLog('USER_LIST')\n11. Add route in server/src/routes/admin.ts: GET /api/admin/users",
            "status": "pending",
            "testStrategy": "Unit test adminUserService.getUserList with mocked Prisma client. Test search filters (email, name), pagination (page 1, page 2, limit). Integration test GET /api/admin/users with query params. Verify audit log created. Test performance with 10,000 users (should return < 500ms)."
          },
          {
            "id": 8,
            "title": "Build user detail API with GDPR export and delete endpoints",
            "description": "Create GET /api/admin/users/:id for detailed user view, POST /api/admin/users/:id/export for GDPR data export (JSON), and DELETE /api/admin/users/:id for GDPR-compliant account deletion with cascades.",
            "dependencies": [
              6
            ],
            "details": "1. Create getUserDetail(userId) in adminUserService: return user with all relations (subscriptions, meals, healthMetrics, activities count)\n2. GET /api/admin/users/:id handler in adminUserController, verify user exists (404 if not)\n3. Create exportUserData(userId) in adminUserService: fetch ALL user data (profile, meals, health metrics, activities, subscriptions, webhook events)\n4. Return as downloadable JSON with structure: { user: {}, meals: [], healthMetrics: [], activities: [], subscriptions: [] }\n5. POST /api/admin/users/:id/export handler: set Content-Type: application/json, Content-Disposition: attachment; filename=user-{userId}-export.json\n6. Create deleteUserAccount(userId, adminUserId, reason) in adminUserService\n7. Validation: require SUPER_ADMIN role for deletion (403 for other roles)\n8. Use Prisma transaction to delete in order: AppStoreWebhookEvent, Subscription, Activity, HealthMetric, Meal, User (cascade)\n9. Log deletion in AdminAuditLog with reason in details field\n10. DELETE /api/admin/users/:id handler: require { reason } in request body (Zod schema)\n11. Add routes: GET /users/:id, POST /users/:id/export, DELETE /users/:id\n12. Apply RBAC: requireAdmin(['SUPER_ADMIN']) for DELETE, requireAdmin() for GET/export",
            "status": "pending",
            "testStrategy": "Test GET /users/:id returns full user detail with counts. Test POST /users/:id/export downloads valid JSON with all user data. Test DELETE requires SUPER_ADMIN role (403 for SUPPORT). Test deletion cascades all related records. Verify audit log includes deletion reason. Test 404 for non-existent userId."
          },
          {
            "id": 9,
            "title": "Build user management UI with search, list, and detail views",
            "description": "Create admin panel pages for user search, paginated user list with TanStack Table, and detailed user view. Implement instant search with debouncing and client-side filtering.",
            "dependencies": [
              1,
              7
            ],
            "details": "1. Create admin-panel/app/(dashboard)/users/page.tsx for user list\n2. Implement search input with debounced onChange (300ms delay using useDebouncedValue hook)\n3. Use TanStack Table v8 with server-side pagination, sorting\n4. Define columns: email, name, subscription status (badge), created date, actions (View button)\n5. Fetch data from GET /api/admin/users with useQuery (use @tanstack/react-query)\n6. Add filters dropdown: subscription status (active, trial, expired, none), account status (active, disabled)\n7. Create admin-panel/components/users/user-table.tsx reusable component\n8. Implement pagination controls (Previous, Next, page numbers) using shadcn/ui Pagination\n9. Create admin-panel/app/(dashboard)/users/[id]/page.tsx for user detail view\n10. Display user profile card: email, name, created date, current weight, goal weight\n11. Show subscription status card: tier, expiration, trial info, auto-renew status\n12. Show recent activity stats: meals logged (last 7 days), health metrics synced (last 7 days)\n13. Add action buttons: Export Data, Delete Account (with role check)\n14. Use shadcn/ui Card, Badge, Tabs components for layout",
            "status": "pending",
            "testStrategy": "Manual testing: verify search updates results after 300ms debounce. Test pagination (next/previous, jump to page). Test sorting by clicking column headers. Verify user detail page loads with correct data. Test filters apply correctly. Verify action buttons appear only for authorized roles."
          },
          {
            "id": 10,
            "title": "Implement GDPR export and delete UI with confirmation modals",
            "description": "Create UI components for GDPR data export (download JSON) and account deletion with multi-step confirmation flow, reason input, and role-based access control.",
            "dependencies": [
              8,
              9
            ],
            "details": "1. Create admin-panel/components/users/export-data-button.tsx\n2. On click, call POST /api/admin/users/:id/export, trigger browser download of JSON file\n3. Show loading spinner during export (can take 5-10 seconds for large datasets)\n4. Display success toast notification using shadcn/ui Toast\n5. Create admin-panel/components/users/delete-user-modal.tsx with shadcn/ui AlertDialog\n6. Require SUPER_ADMIN role to show Delete Account button (check session.user.role)\n7. First confirmation: \"Are you sure you want to delete this account? This cannot be undone.\"\n8. Second step: require typing user's email to confirm (text input must match exactly)\n9. Third step: require deletion reason (textarea, minimum 10 characters)\n10. On confirm, call DELETE /api/admin/users/:id with { reason } body\n11. Show loading state during deletion\n12. On success, redirect to user list with success toast\n13. On error, display error message in modal (e.g., 403 Forbidden if not SUPER_ADMIN)",
            "status": "pending",
            "testStrategy": "Test export downloads JSON file with correct filename. Test delete modal shows only for SUPER_ADMIN role. Test multi-step confirmation flow (cannot proceed without typing email). Test reason validation (minimum 10 chars). Test deletion success redirects to user list. Test error handling for 403/500 responses."
          },
          {
            "id": 11,
            "title": "Build subscription management backend API with manual operations",
            "description": "Create backend APIs for subscription list, detail view, and manual operations (grant Pro access, extend subscription, revoke access). All operations require SUPER_ADMIN role and are audit logged.",
            "dependencies": [
              6
            ],
            "details": "1. Create server/src/controllers/adminSubscriptionController.ts and server/src/services/adminSubscriptionService.ts\n2. GET /api/admin/subscriptions: list all subscriptions with filters (status, productId, userId), pagination, include user info\n3. GET /api/admin/subscriptions/:id: return subscription with full transaction history, related webhook events\n4. GET /api/admin/subscriptions/lookup?txn={originalTransactionId}: lookup by Apple transaction ID for support tickets\n5. POST /api/admin/subscriptions/:id/grant: manually grant Pro access (require { duration, reason } in body)\n6. Implementation: create or update Subscription record with status=ACTIVE, expiresAt=now + duration, source=MANUAL_GRANT\n7. POST /api/admin/subscriptions/:id/extend: extend expiration (require { days, reason })\n8. Implementation: update expiresAt = expiresAt + days, log in metadata\n9. POST /api/admin/subscriptions/:id/revoke: revoke access (require { reason })\n10. Implementation: set status=CANCELLED, expiresAt=now\n11. All operations require requireAdmin(['SUPER_ADMIN']) middleware\n12. All operations use auditLog with action (SUBSCRIPTION_GRANT, SUBSCRIPTION_EXTEND, SUBSCRIPTION_REVOKE)\n13. Create Zod schemas: grantSubscriptionSchema, extendSubscriptionSchema, revokeSubscriptionSchema\n14. Add routes in server/src/routes/admin.ts",
            "status": "pending",
            "testStrategy": "Unit test subscription service functions with mocked Prisma. Test RBAC enforcement (403 for non-SUPER_ADMIN). Test grant creates new subscription with correct expiresAt. Test extend updates expiresAt correctly. Test revoke sets status=CANCELLED. Verify audit logs created with reason field. Test lookup by originalTransactionId finds correct subscription."
          },
          {
            "id": 12,
            "title": "Build subscription management UI with manual operation modals",
            "description": "Create subscription list page, detail view, and modals for manual grant/extend/revoke operations. Include originalTransactionId search for support workflows.",
            "dependencies": [
              1,
              11
            ],
            "details": "1. Create admin-panel/app/(dashboard)/subscriptions/page.tsx with TanStack Table\n2. Columns: user email, product (Pro Monthly/Yearly), status (badge: active=green, expired=red, trial=blue), expires at, actions\n3. Add search by originalTransactionId input (for support tickets like \"customer says they paid but no access\")\n4. Create admin-panel/app/(dashboard)/subscriptions/[id]/page.tsx for detail view\n5. Show subscription info card: user, product, status, created/expires dates, auto-renew status\n6. Show transaction history table: all originalTransactionIds, purchase dates, amounts (from webhook metadata)\n7. Show related webhook events table: notification type, received at, processing status\n8. Create admin-panel/components/subscriptions/grant-modal.tsx for manual grant\n9. Require SUPER_ADMIN role to show manual operation buttons\n10. Grant modal fields: duration (select: 7 days, 30 days, 90 days, 1 year), reason (textarea, required)\n11. Create extend-modal.tsx with days input (number, 1-365) and reason\n12. Create revoke-modal.tsx with reason input only\n13. All modals use shadcn/ui Dialog component with confirmation step\n14. On success, refetch subscription data and show toast notification",
            "status": "pending",
            "testStrategy": "Test subscription list loads with correct data and filters. Test search by originalTransactionId finds subscription. Test detail view shows transaction history. Test manual operation modals appear only for SUPER_ADMIN. Test grant modal creates subscription with correct expiration. Test extend modal updates expiresAt. Test revoke modal cancels subscription. Verify success toasts display."
          },
          {
            "id": 13,
            "title": "Build webhook event viewer backend API with filtering and retry",
            "description": "Create API endpoints for listing App Store webhook events with filtering (notification type, date range, subscription ID), event detail view, and retry failed webhook processing.",
            "dependencies": [
              6
            ],
            "details": "1. Assumption: AppStoreWebhookEvent model exists from Task 38 (add to schema if missing)\n2. Create server/src/controllers/adminWebhookController.ts and server/src/services/adminWebhookService.ts\n3. GET /api/admin/webhooks: list webhook events with pagination (newest first)\n4. Query params: notificationType (SUBSCRIBED, DID_RENEW, etc.), status (success, failed), startDate, endDate, subscriptionId, originalTransactionId\n5. Use Prisma where clause with filters, order by createdAt DESC\n6. Include related subscription and user data\n7. GET /api/admin/webhooks/:id: return full webhook event with complete JSON payload, processing status, error message if failed\n8. POST /api/admin/webhooks/:id/retry: retry processing failed webhook\n9. Implementation: call webhook processing service (from Task 38) with stored payload, update status based on result\n10. Require SUPER_ADMIN role for retry endpoint\n11. Apply audit logging: auditLog('WEBHOOK_RETRY')\n12. Create Zod schemas: listWebhooksQuerySchema\n13. Add routes in server/src/routes/admin.ts",
            "status": "pending",
            "testStrategy": "Test GET /api/admin/webhooks returns paginated events. Test filters (notification type, date range, status). Test detail view returns full payload. Test retry endpoint processes webhook (mock processing service). Verify RBAC for retry (403 for non-SUPER_ADMIN). Verify audit log created on retry."
          },
          {
            "id": 14,
            "title": "Build webhook event viewer UI with filtering and retry functionality",
            "description": "Create webhook event list page with advanced filtering, event detail view showing full JSON payload, and retry button for failed events with real-time status updates.",
            "dependencies": [
              1,
              13
            ],
            "details": "1. Create admin-panel/app/(dashboard)/webhooks/page.tsx with TanStack Table\n2. Columns: notification type, subscription ID (link to subscription detail), received at, status (badge: success=green, failed=red, pending=yellow), actions\n3. Add filter controls: notification type dropdown (all types from Task 38), status dropdown (success, failed, pending), date range picker (shadcn/ui Calendar)\n4. Implement client-side filtering with URL params (persist filters on page reload)\n5. Click row to expand and show full JSON payload using shadcn/ui Collapsible or Accordion\n6. Use JSON syntax highlighting library (react-json-view or similar) for payload display\n7. For failed events, show error message prominently\n8. Add Retry button for failed events (require SUPER_ADMIN role)\n9. On retry click, call POST /api/admin/webhooks/:id/retry\n10. Show loading spinner on retry button during processing\n11. Poll event status after retry (useQuery with refetchInterval) or use optimistic update\n12. Display success/failure toast after retry completes\n13. Add search input for originalTransactionId (quick lookup from support tickets)",
            "status": "pending",
            "testStrategy": "Test webhook list loads with correct data. Test filters (notification type, status, date range) apply correctly. Test JSON payload displays with syntax highlighting. Test retry button appears only for failed events and SUPER_ADMIN role. Test retry updates event status. Test search by originalTransactionId filters events. Test pagination works with filters applied."
          },
          {
            "id": 15,
            "title": "Build analytics calculations backend for MRR, churn, and trial conversion",
            "description": "Create backend API endpoint and service layer for calculating key subscription analytics: MRR, churn rate, trial conversion rate, new subscriptions, and active subscribers by tier.",
            "dependencies": [
              6
            ],
            "details": "1. Create server/src/services/adminAnalyticsService.ts with analytics calculation functions\n2. getSubscriptionMetrics(): calculate active subscribers by tier (Pro Monthly, Pro Yearly, Trial)\n3. MRR calculation: sum of (monthly subscription price) for active subscriptions + (yearly price / 12) for yearly subscriptions\n4. Use Prisma aggregation: Subscription.count({ where: { status: 'ACTIVE', productId: 'pro_monthly' } })\n5. getNewSubscriptions(period): count subscriptions created in period (today, this week, this month)\n6. Use Prisma where: { createdAt: { gte: startDate } }\n7. getChurnMetrics(period): calculate cancellations in period, churn rate = cancellations / active_at_start\n8. Use Subscription status transitions (need to track cancelledAt timestamp - add to schema if missing)\n9. getTrialMetrics(): active trials count, trial conversion rate = (trials converted to paid) / (trials started in last 30 days)\n10. Query: Subscription.count({ where: { status: 'ACTIVE', AND: [{ createdAt: { lte: 30 days ago } }, { originalTransactionId: { not: null } }] } })\n11. Create GET /api/admin/analytics/overview endpoint in adminAnalyticsController\n12. Return JSON: { mrr, activeSubscribers: { total, proMonthly, proYearly, trial }, newSubscriptions: { today, week, month }, churn: { rate, count }, trials: { active, conversionRate } }\n13. Add route in server/src/routes/admin.ts\n14. Apply RBAC: requireAdmin() (all roles can view analytics)",
            "status": "pending",
            "testStrategy": "Unit test analytics calculations with mock data. Test MRR calculation (10 monthly at $9.99 + 5 yearly at $79.99 = $132.90). Test churn rate calculation. Test trial conversion rate with sample data. Integration test GET /api/admin/analytics/overview returns correct structure. Test performance with 10,000 subscriptions (should return < 2 seconds)."
          },
          {
            "id": 16,
            "title": "Build analytics dashboard UI with Tremor charts and metric cards",
            "description": "Create dashboard overview page with real-time subscription metrics, MRR/revenue charts, subscriber count over time, and trial conversion metrics using Tremor chart library.",
            "dependencies": [
              1,
              15
            ],
            "details": "1. Create admin-panel/app/(dashboard)/page.tsx for dashboard overview\n2. Fetch analytics data from GET /api/admin/analytics/overview using useQuery\n3. Create metric cards grid (4 cards across) using Tremor Card component:\n   - Total Active Subscribers (with count by tier in subtitle)\n   - Monthly Recurring Revenue (MRR in USD, format with currency)\n   - New Subscriptions This Month (with week/today in subtitle)\n   - Churn Rate (percentage with count in subtitle)\n4. Use Tremor Metric and Text components for card content\n5. Add color coding: green for positive metrics, red for churn\n6. Create admin-panel/components/analytics/metrics-cards.tsx reusable component\n7. Fetch time-series data for charts: GET /api/admin/analytics/subscribers-over-time and GET /api/admin/analytics/revenue-over-time (create these endpoints)\n8. Create Subscribers Over Time line chart using Tremor LineChart component (last 30 days, daily data points)\n9. Create Revenue Over Time area chart using Tremor AreaChart (last 12 months, monthly MRR)\n10. Create Subscription Status pie chart using Tremor DonutChart (active, trial, expired distribution)\n11. Add Trial Conversion Funnel using Tremor BarList (trials started → active → converted)\n12. Implement auto-refresh every 60 seconds using useQuery refetchInterval\n13. Add date range selector for charts (last 7 days, 30 days, 90 days, 1 year)",
            "status": "pending",
            "testStrategy": "Test dashboard loads with correct metric values. Verify charts render with sample data. Test auto-refresh updates data every 60 seconds. Test date range selector updates chart data. Test metric cards show color coding correctly. Verify currency formatting for MRR. Test responsive layout on different screen sizes."
          },
          {
            "id": 17,
            "title": "Implement security hardening: rate limiting, session security, IP allowlist",
            "description": "Add production-grade security hardening to admin panel: strict rate limiting on admin endpoints, secure session configuration, optional IP allowlisting, and enhanced security headers.",
            "dependencies": [
              6
            ],
            "details": "1. Create server/src/middleware/adminRateLimiter.ts using express-rate-limit\n2. Admin login endpoint: 5 requests per 15 minutes per IP (more strict than app endpoints)\n3. Admin API endpoints: 100 requests per 15 minutes per admin user (track by adminUserId from JWT)\n4. Use Redis store for rate limiting if available (fallback to memory store)\n5. Return 429 Too Many Requests with Retry-After header\n6. Create server/src/middleware/ipAllowlist.ts for optional IP restriction\n7. Read ADMIN_IP_ALLOWLIST from environment (comma-separated IPs), skip if not set\n8. Check req.ip against allowlist, return 403 if not allowed\n9. Log blocked IPs in AdminAuditLog with action='IP_BLOCKED'\n10. Update helmet configuration in server/src/middleware/security.ts for admin routes:\n    - Stricter CSP: no inline scripts, only same-origin\n    - X-Frame-Options: DENY (prevent iframe embedding)\n    - X-Content-Type-Options: nosniff\n11. Configure CORS for admin API: only allow admin-panel origin (admin.nutri.app or localhost:3001)\n12. Add security headers to Next.js admin panel in next.config.js\n13. Apply adminRateLimiter and ipAllowlist middleware to all /api/admin/* routes\n14. Document in .env.example: ADMIN_IP_ALLOWLIST (optional)",
            "status": "pending",
            "testStrategy": "Test rate limiting: make 6 login requests in 15 minutes, verify 6th returns 429. Test IP allowlist blocks non-allowed IPs (set ADMIN_IP_ALLOWLIST=127.0.0.1 and test from different IP). Test CORS rejects requests from unauthorized origins. Test security headers present in responses using securityheaders.com. Verify audit log records blocked IPs."
          },
          {
            "id": 18,
            "title": "Write comprehensive test suite and deployment configuration",
            "description": "Create unit tests, integration tests, E2E tests for admin panel MVP. Set up deployment configuration for Next.js app (Vercel) and document deployment process with environment variables.",
            "dependencies": [
              2,
              3,
              6,
              7,
              8,
              11,
              13,
              15,
              17
            ],
            "details": "1. Backend unit tests in server/src/__tests__/admin/:\n   - adminAuthService.test.ts: password verification, MFA setup, session tokens\n   - adminUserService.test.ts: user list, search, export, delete\n   - adminSubscriptionService.test.ts: grant, extend, revoke\n   - adminAnalyticsService.test.ts: MRR, churn, trial conversion calculations\n2. Backend integration tests in server/src/__tests__/admin/integration/:\n   - adminAuth.integration.test.ts: login flow, MFA flow, session management\n   - adminUsers.integration.test.ts: GET /users, GET /users/:id, DELETE /users/:id\n   - adminSubscriptions.integration.test.ts: manual operations\n   - Test RBAC enforcement (403 for unauthorized roles)\n   - Test audit logging (verify records created)\n3. Frontend tests in admin-panel/:\n   - Install @testing-library/react, @testing-library/jest-dom\n   - Test components: user-table.test.tsx, grant-modal.test.tsx, metrics-cards.test.tsx\n   - Test hooks: useResponsive (if created), useDebounce\n   - Mock API calls with MSW (Mock Service Worker)\n4. Create admin-panel/vercel.json for Vercel deployment configuration\n5. Set environment variables in Vercel dashboard: NEXTAUTH_SECRET, NEXTAUTH_URL, API_URL (backend URL)\n6. Create deployment guide in admin-panel/README.md:\n   - Prerequisites (Node.js 18+, npm)\n   - Environment variables required\n   - Build command: npm run build\n   - Start command: npm start\n   - Database migrations (run before deployment)\n7. Create seed script for AdminUser in server/prisma/seed.ts (create first SUPER_ADMIN account)\n8. Document admin user creation: npm run seed:admin (create initial admin with email/password)\n9. Add npm scripts to admin-panel/package.json: test, test:watch, test:coverage\n10. Achieve > 80% coverage for critical paths (auth, RBAC, audit logging)",
            "status": "pending",
            "testStrategy": "Run all tests: `npm test` in server/ and admin-panel/. Verify coverage reports meet 80% threshold. Test deployment to Vercel staging environment. Verify environment variables loaded correctly. Test admin login works in production. Test RBAC and audit logging in staging. Run security scan with `npm audit` and fix high/critical vulnerabilities."
          }
        ]
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-12-18T09:07:38.857Z",
      "taskCount": 36,
      "completedCount": 15,
      "tags": [
        "master"
      ],
      "created": "2025-12-23T11:23:13.933Z",
      "description": "Tasks for master context",
      "updated": "2025-12-23T11:31:50.082Z"
    }
  }
}