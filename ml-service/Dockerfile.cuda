# =============================================================================
# Nutri ML Service - Production Dockerfile with NVIDIA CUDA Support
# For deployment on GPU-enabled cloud instances (AWS g4dn, g5, p3, etc.)
# =============================================================================

# -----------------------------------------------------------------------------
# Stage 1: Builder
# Install dependencies with CUDA support
# -----------------------------------------------------------------------------
FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04 AS builder

# Install Python and build dependencies
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3.11-dev \
    python3.11-venv \
    python3-pip \
    gcc \
    g++ \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.11 as default
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1 && \
    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1

WORKDIR /build

# Copy requirements and install dependencies
COPY requirements.txt .

# Install PyTorch with CUDA support first (override CPU version in requirements.txt)
RUN pip install --no-cache-dir --user \
    torch==2.4.1+cu121 \
    torchvision==0.19.1+cu121 \
    torchaudio==2.4.1+cu121 \
    --index-url https://download.pytorch.org/whl/cu121

# Install remaining dependencies
RUN pip install --no-cache-dir --user -r requirements.txt

# -----------------------------------------------------------------------------
# Stage 2: Runtime
# Minimal production image with CUDA runtime
# -----------------------------------------------------------------------------
FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04

WORKDIR /app

# Install Python runtime and dependencies
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3.11-venv \
    libpq5 \
    curl \
    && rm -rf /var/lib/apt/lists/* \
    && update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1 \
    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1

# Create non-root user for security
RUN groupadd --system --gid 1001 mlservice && \
    useradd --system --uid 1001 --gid mlservice --home /home/mlservice --create-home mlservice

# Copy Python dependencies from builder
COPY --from=builder --chown=mlservice:mlservice /root/.local /home/mlservice/.local

# Update PATH for non-root user
ENV PATH=/home/mlservice/.local/bin:$PATH

# Python and CUDA optimizations
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    # CUDA settings
    NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility \
    # Force CUDA device
    CUDA_VISIBLE_DEVICES=0

# Copy application code
COPY --chown=mlservice:mlservice ./app /app/app

# Create directories for ML models and HuggingFace cache
RUN mkdir -p /app/app/ml_models /home/mlservice/.cache/huggingface && \
    chown -R mlservice:mlservice /app /home/mlservice/.cache

# Set HuggingFace cache directory
ENV HF_HOME=/home/mlservice/.cache/huggingface \
    TRANSFORMERS_CACHE=/home/mlservice/.cache/huggingface

# Switch to non-root user
USER mlservice

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
    CMD curl -f http://localhost:8000/health/live || exit 1

# Run with optimized settings for GPU inference
# - workers=1: GPU memory is shared, multiple workers can cause OOM
# - timeout=300: Allow time for model loading on first request
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1", "--timeout-keep-alive", "300"]
